Contradicting behaviour in variations of tf.cond usage with tf.nn.static_state_saving_rnn

Model.txt
Training.txt
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.2/1.3/1.4 (tested on all)
Bazel Version : N/A
Python version: 2.7
CUDA/cuDNN version: Cuda 8, CuDNN 6
GPU model and memory: GeForce GTX 1080 , 12 GB
Exact command to reproduce: python Training.py

Problem
I am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, I am using tf.contrib.training.batch_sequences_with_states API with tf.nn.static_state_saving_rnn API to transfer RNN state information to subsequent segments of the same sequence. I am using tf.RandomShuffleQueue() to store my data and to decouple the I/O from training I am running the enqueue operations asynchronously in a different thread.
To facilitate a testing run after each training epoch I am using two separate tf.RandomShuffleQueue() structures and hence two different tf.contrib.training.batch_sequences_with_states() and tf.nn.static_state_saving_rnn() instances for train/test data correspondingly. Just the RNN cell which is passed to tf.nn.static_state_saving_rnn instances remains the same, so that the modified set of weights are used at test time.
Moreover, I use a placeholder which is a boolean flag using which the appropriate nodes in the computation graph are switched at train/test time. This switching is done using tf.cond() operation.
Situation 1
The problem is that of a deadlock situation at a specific stage between the enqueue operations and training operations, both running in separate threads. The enqueue operation timeouts mostly because the queue has reached the maximum capacity and for some reason training operation never returns and is waiting to get some more data and hence no dequeue operation is called.
Situation 2
In file Model.py, if I uncomment the lines from 97-101 and comment line 104, then there is no such deadlock situation. The only difference is in the way that specific tf.cond() operation is written. One is in a declarative form(working code) and other is in an inline form(broken/deadlock code).
Situation 3
In file - Training.py, dummy data is generated by the gen_data() procedure(lines - 43-48) and called on line 61. The second parameter to this function is the number of time steps for each sequence. If this number is fixed to a value which is less than the unroll length parameter of tf.contrib.training.batch_sequences_with_states() instances(i.e. each sequence can very well fit in one batch itself), this deadlock does not occur irrespective of situation 1 or situation 2 described above.
Hence, we suspect there is some minute intricacy in tf.cond() and tf.nn.static_state_saving_rnn() which gives rise to such a deadlock.
Source code / logs
Two files are attached(.txt files since the interface does not allow attaching files with extension .py) -

Model.txt - Contains the Model class and the inference() method where the majority of the computation graph is built.
Training.txt - Contains the client code which generates dummy data and calls to sess.run()

The current code is according to Situation 1 as described above and the behaviour can be seen by running the command - python Training.py
Regards,
Daksh