Running ops with restored model gives FailedPreconditionError

I'm trying running some ops with a pre-trained model and the model was restored successfully. When running some ops that depend on one node in the graph, TF returns me
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_213
	 [[Node: Variable_213/read = Identity[T=DT_FLOAT, _class=["loc:@Variable_213"], _device="/job:localhost/replica:0/task:0/cpu:0"](Variable_213)]]

It might be that I missed something , but this didn't show up before I updated my Tensorflow to 1.2. I'm running TF on MacOS X, Python 2.7. Below is the source code:
Source code / logs
import tensorflow as tf
import csv
import os
import glob
from PIL import Image
from datetime import datetime

def read_image_file(filename):
    image = tf.image.decode_png(tf.read_file(filename), channels=3)
    image = tf.cast(image, tf.float32)
    image = tf.reshape(image, [224, 112, 3])
    return image

def labelFiles(filename):
    label_list = []
    for name in filename:
        label_list.append(getLabel(name))
    return label_list

def readCSV():
    with open('label.csv', 'r') as infile:
        reader = csv.reader(infile)
        label = {rows[0]:rows[1] for rows in reader}
        return label

def getWord(filename):
    word = ''
    (name, extension) = os.path.splitext(filename)
    (path, name) = os.path.split(name)
    for i in name:
        if i.isalpha():
            start = name.index(i)
            word = word + name[start:]
            #print(word)
            break
    return word

def getLabel(filename):
    label = [0]*len(labels)
    index =  int(labels.get(getWord(str(filename)), 0))
    label[index] = 1
    label = tf.constant(label, dtype = tf.float32)
    tf.reshape(label, [-1])
    print(label)
    return label

def weight(shape):
    initial = tf.truncated_normal(shape, stddev=0.01)
    return tf.Variable(initial)

def bias_0(shape):
    initial = tf.constant(0.0, shape=shape)
    return tf.Variable(initial)

def conv2d(image, weight, stride):
    return tf.nn.conv2d(image, weight, strides=[1,stride,stride,1], padding='SAME')

def max_pool_3x3(image, stride):
    return tf.nn.max_pool(image, ksize=[1, 3, 3, 1], strides=[1, stride, stride, 1], padding='SAME')

def batch_norm(image, out_size):
    mean, var = tf.nn.moments(image, [0, 1, 2])
    scale = tf.Variable(tf.ones([out_size]))
    beta = tf.Variable(tf.zeros([out_size]))
    epsilon = 0.001
    bn = tf.nn.batch_normalization(image,mean,var,beta,scale,epsilon)
    return bn

def inception(image, conv1_size, conv2_1_size, conv2_2_size, conv3_1_size, conv3_2_size, conv4_2_size):
    conv1_weight = weight(conv1_size)
    conv1_bias = bias_0([conv1_size[3]])
    conv1 = tf.nn.relu(batch_norm(conv2d(image, conv1_weight, 1) + conv1_bias, conv1_size[3]))

    conv2_1_weight = weight(conv2_1_size)
    conv2_1_bias = bias_0([conv2_1_size[3]])
    conv2_1 = tf.nn.relu(batch_norm(conv2d(image, conv2_1_weight, 1) + conv2_1_bias, conv2_1_size[3]))
    conv2_2_weight = weight(conv2_2_size)
    conv2_2_bias = bias_0([conv2_2_size[3]])
    conv2_2 = tf.nn.relu(batch_norm(conv2d(conv2_1, conv2_2_weight, 1) + conv2_2_bias, conv2_2_size[3]))

    conv3_1_weight = weight(conv3_1_size)
    conv3_1_bias = bias_0([conv3_1_size[3]])
    conv3_1 = tf.nn.relu(batch_norm(conv2d(image, conv3_1_weight, 1) + conv3_1_bias, conv3_1_size[3]))
    conv3_2_weight = weight(conv3_2_size)
    conv3_2_bias = bias_0([conv3_2_size[3]])
    conv3_2 = tf.nn.relu(batch_norm(conv2d(conv3_1, conv3_2_weight, 1) + conv3_2_bias, conv3_2_size[3]))

    pool4_1 = max_pool_3x3(image, 1)
    conv4_2_weight = weight(conv4_2_size)
    conv4_2_bias = bias_0([conv4_2_size[3]])
    conv4_2 = tf.nn.relu(batch_norm(conv2d(pool4_1, conv4_2_weight, 1) + conv4_2_bias, conv4_2_size[3]))
    return tf.concat([conv1, conv2_2, conv3_2, conv4_2], 3)


now = datetime.utcnow().strftime("%Y%m%d%H%M%S")

root_logdir = "/home/tensorflow/tf_logs"
logdir = "{}/run-{}/".format(root_logdir, now)
savedir = "/home/tensorflow/saves"

labels = readCSV()
num_labels = len(labels)
trainpath = '/home/resized/trainset/'
testpath = '/home/resized/testset/'

trainnames = glob.glob(trainpath + '*.png')
testnames = glob.glob(testpath + '*.png')


train_label = labelFiles(trainnames)
test_label = labelFiles(testnames)

batch_size = 256
epochs = 50
num_batch = int(len(trainnames)/batch_size)
initial_learning_rate = 0.001
decay_steps = num_batch*8
decay_rate = 0.96
global_step = tf.Variable(0, trainable = False)
learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase=True)


# TRAIN QUEUE
train_queue = tf.RandomShuffleQueue(len(trainnames)*1.5, 0, [tf.string, tf.float32], shapes=[[],[len(labels),]])

enqueue_train = train_queue.enqueue_many([trainnames, train_label])

train_image, train_image_label = train_queue.dequeue()

train_image = read_image_file(train_image)

train_batch, train_label_batch = tf.train.batch(
    [train_image, train_image_label],
    batch_size=batch_size,
    num_threads=1,
    capacity=10*batch_size,
    enqueue_many=False,
    shapes=[[224,112,3], [len(labels),]],
    allow_smaller_final_batch=True
)

train_close = train_queue.close()

# TEST QUEUE

test_queue = tf.FIFOQueue(len(testnames)*1.5, [tf.string, tf.float32], shapes=[[],[len(labels),]])

enqueue_test = test_queue.enqueue_many([testnames, test_label])

test_image, test_image_label = test_queue.dequeue()

test_image = tf.expand_dims(read_image_file(test_image), 0)

test_close = test_queue.close()

# MODEL

keep_prob = tf.placeholder(tf.float32)

weights_conv1 = weight([7, 7, 3, 64])

bias_conv1 = bias_0([64])

weights_conv2 = weight([1, 1, 64, 64])

bias_conv2 = bias_0([64])

weights_conv3 = weight([3, 3, 64, 192])

bias_conv3 = bias_0([192])

weights_fc13 = weight([1024, num_labels])

bias_fc13 = bias_0([num_labels])


def GoogleNet(data):

    # First Conv. Layer
    conv1 = tf.nn.relu(batch_norm(conv2d(data, weights_conv1, 2) + bias_conv1, 64))

    pool1 = max_pool_3x3(conv1, 2)

    lrn1 = tf.nn.local_response_normalization(pool1, 2, 1, 0.00002, 0.75)

    # Second Conv. Layer
    conv2 = tf.nn.relu(batch_norm(conv2d(lrn1, weights_conv2, 1) + bias_conv2, 64))

    conv3 = tf.nn.relu(batch_norm(conv2d(conv2, weights_conv3, 1) + bias_conv3, 192))

    lrn2 = tf.nn.local_response_normalization(conv3, 2, 1, 0.00002, 0.75)

    pool2 = max_pool_3x3(lrn2, 2)

    # First Inception Layer
    incep4 = inception(
        pool2,
        [1, 1, 192, 64],
        [1, 1, 192, 96],
        [3, 3, 96, 128],
        [1, 1, 192, 16],
        [5, 5, 16, 32],
        [1, 1, 192, 32]
    )

    incep5 = inception(
        incep4,
        [1, 1, 256, 128],
        [1, 1, 256, 128],
        [3, 3, 128, 192],
        [1, 1, 256, 32],
        [5, 5, 32, 96],
        [1, 1, 256, 64]
    )

    pool3 = max_pool_3x3(incep5, 2)

    # Second Inception Layer
    incep6 = inception(
        pool3,
        [1, 1, 480, 192],
        [1, 1, 480, 96],
        [3, 3, 96, 208],
        [1, 1, 480, 16],
        [5, 5, 16, 48],
        [1, 1, 480, 64]
    )

    incep7 = inception(
        incep6,
        [1, 1, 512, 160],
        [1, 1, 512, 112],
        [3, 3, 112, 224],
        [1, 1, 512, 24],
        [5, 5, 24, 64],
        [1, 1, 512, 64]
    )

    incep8 = inception(
        incep7,
        [1, 1, 512, 128],
        [1, 1, 512, 128],
        [3, 3, 128, 256],
        [1, 1, 512, 24],
        [5, 5, 24, 64],
        [1, 1, 512, 64]
    )

    incep9 = inception(
        incep8,
        [1, 1, 512, 112],
        [1, 1, 512, 144],
        [3, 3, 144, 288],
        [1, 1, 512, 32],
        [5, 5, 32, 64],
        [1, 1, 512, 64]
    )

    incep10 = inception(
        incep9,
        [1, 1, 528, 256],
        [1, 1, 528, 160],
        [3, 3, 160, 320],
        [1, 1, 528, 32],
        [5, 5, 32, 128],
        [1, 1, 528, 128]
    )

    pool4 = max_pool_3x3(incep10, 2)

    # Third Inception Layer
    incep11 = inception(
        pool4,
        [1, 1, 832, 256],
        [1, 1, 832, 160],
        [3, 3, 160, 320],
        [1, 1, 832, 32],
        [5, 5, 32, 128],
        [1, 1, 832, 128]
    )

    incep12 = inception(
        incep11,
        [1, 1, 832, 384],
        [1, 1, 832, 192],
        [3, 3, 192, 384],
        [1, 1, 832, 48],
        [5, 5, 48, 128],
        [1, 1, 832, 128]
    )

    pool5 = tf.nn.avg_pool(incep12, ksize = [1, 7, 4, 1], strides = [1, 1, 1, 1], padding = 'VALID')

    pool5_flat = tf.reshape(pool5, [-1, 1024])

    # FC Layers
    fc13_drop = tf.nn.dropout(pool5_flat, keep_prob)

    fc13 = tf.matmul(fc13_drop, weights_fc13) + bias_fc13

    return fc13

#Training
with tf.name_scope("cost_function") as scope:
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_label_batch, logits=GoogleNet(train_batch)))
    train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost, global_step=global_step)


#Accuracy
with tf.name_scope("accuracy") as scope:
    correct_prediction = tf.equal(tf.argmax(GoogleNet(test_image), 1), tf.argmax(test_image_label, 0))
    accuracy = tf.cast(correct_prediction, tf.float32)

cost_summary = tf.summary.scalar("cost_function", cost)
file_writer = tf.summary.FileWriter(logdir)

#Session
with tf.Session() as sess:
    saver = tf.train.Saver()
    saver.restore(sess, "/Users/aleex/Dropbox/TensorFlow/GoogleNet/saves/GoogleNet.ckpt")
    print("Model restored...")
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)

    sess.run(enqueue_test)
    accuracy_vector = []
   
    print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))
    
    for num in range(len(testnames)):
        accuracy_vector.append(sess.run(accuracy, feed_dict={keep_prob: 1.0}))
    mean_accuracy = sess.run(tf.divide(tf.add_n(accuracy_vector), len(testnames)))

    print("test accuracy %g"%mean_accuracy)
    sess.run(test_close)

    coord.request_stop()
    coord.join(threads)

    file_writer.close()

It was the line print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0})) that messed this up.
A side question here: the model was trained well, but it gives me a poor test accuracy of 0.0004, which basically shows that the predictions with the test samples are all wrong. Is this a bug?