Data Parallel, multi GPU (minimal) example?

I am trying to get a simple example of tensorflow that parallelises the data over multiple GPUs to train. I have looked at cifar10_multi_gpu.py and its associated input files etc. but it seems to be a bit beyond me at this stage. Especially since I'm not sure why multiple GPUs would speed up the process in that example. If I understand correctly in that example all GPUs get the same copy of the data and the model. The only advantage that I see from this is that you can search a more spread out weight/parameter space.
What I expected was a line that did something similar to:
with tf.Session() as sess:
    for i in range(4): #for 4 gpus
        with tf.device('/gpu:%d' % i):
            tf.feed_dict({x: data[i*step:(i+1)*step], y: labels[i*step:(i+1)*step])

with the emphasis of the feed dict feeding different data to the four different gpus. I haven't however thought how I would go about getting the gradients and averaging them yet.
Any code examples would be highly appreciated. If not thoughts or any direction on how to proceed is also welcome.