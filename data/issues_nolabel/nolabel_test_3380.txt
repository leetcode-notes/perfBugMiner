Multi-GPU Example only using a single GPU

When running the CIFAR-10 example, it appears like TensorFlow is only utilizing one card - despite the fact that I have 4 cards installed in this machine (it is an NVIDIA DIGITS box). The script runs fine otherwise (can complete computation on a single card), but simply refuses to use more than one card. When attempting to use the script here: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py it appears as if Tensorflow allocates memory to one card, runs the computation, then allocates memory to a second card (in reverse numerical order, no less), and then runs the computation again, on that single card. Oddly, TensorFlow seems to see my cards in reverse order - when using "/gpu:0", it allocates work to the 4th card in the box. This is probably unrelated, but I mention it just in case.
Operating System: Ubuntu 14.04
uname -a output: Linux nvidia-1 3.16.0-34-generic #47~14.04.1-Ubuntu SMP Fri Apr 10 17:49:16 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
CIFAR example invocation: cifar10_multi_gpu_train.py --num-gpus=2 - results are the same if I set that number between 2 and 4, inclusive.
nvidia-smi output, before running the CIFAR example:
| NVIDIA-SMI 352.93     Driver Version: 352.93         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:05:00.0      On |                  N/A |
| 22%   36C    P8    17W / 250W |    256MiB / 12284MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |
| 22%   38C    P8    16W / 250W |     23MiB / 12287MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |
| 22%   37C    P8    17W / 250W |     23MiB / 12287MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |
| 22%   35C    P8    15W / 250W |     23MiB / 12287MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1697    G   /usr/bin/X                                     174MiB |
|    0      2983    G   compiz                                          55MiB |
+-----------------------------------------------------------------------------+

NVIDIA-SMI under load:
+------------------------------------------------------+
| NVIDIA-SMI 352.93     Driver Version: 352.93         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:05:00.0      On |                  N/A |
| 22%   38C    P8    16W / 250W |    364MiB / 12284MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |
| 22%   41C    P8    16W / 250W |    138MiB / 12287MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |
| 22%   39C    P8    17W / 250W |    138MiB / 12287MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |
| 22%   45C    P2    88W / 250W |  11767MiB / 12287MiB |     90%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1697    G   /usr/bin/X                                     174MiB |
|    0      2983    G   compiz                                          49MiB |
|    0     11514    C   python                                         111MiB |
|    1     11514    C   python                                         111MiB |
|    2     11514    C   python                                         111MiB |
|    3     11514    C   python                                       11740MiB |
+-----------------------------------------------------------------------------+

I installed TensorFlow from source.
python -c "import tensorflow; print(tensorflow.__version__)" I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally 0.9.0
This is the Git commit hash that I got when pulling down TensorFlow:
a3f61c1d5c76339e6c9655dac426bb3822659772