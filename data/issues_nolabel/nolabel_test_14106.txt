Batch normalization for LSTM

A recurrent batch normalization is implemented based on https://arxiv.org/pdf/1603.09025.pdf. The new functionality allows for setting normalization for each of the inputs, the actual input or the state, and the cell prior to the activation function. To avoid a double implementation, tensorflow.python.layers.normalization.BatchNormalization is called to perform the normalization, and it can be controlled fully, but if no extra configuration is provided a good default configuraiton is used. By default no normalization is done. The only aspect mentioned in the paper that was not implemented, was that it was recommended there that the statistics be taken on a time step basis, rather than having all time steps share the same statistics. This would have necessitated a lot of tinkering in BatchNormalization, to the point that it would be more economical if that behaviour was desired, to simply port the parts needed into LSTM and adjust them as necessary, but that should not be part of the main branch.