Potential bug: Gradients currently flow through variables that are fed

x = tf.Variable(1.0)
y = 2*x
z = 3*y
dzdx, = tf.gradients(z, x)

sess.run(tf.initialize_all_variables())
print(sess.run([z, dzdx], feed_dict={y: 10.0}))

# Result: z = 30.0, dzdx = 6.0

To me this seems like a bug: the only reasonable behavior here seems to be to treat y as constant, so that dzdx is 0.0. This would let us feed parts of models when needed, in turn automatically shutting off all gradient updates with respect to the parameters that would have been responsible for generating these fed values.
This would be a nice thing to have in my current use case. If we want to perform iterative optimization with respect to two parameter sets, we would like to feed results from set 1 and optimize set 2; feed results from set 2 and optimize set 1; etc.
Is the current behavior intended and/or useful in some use cases? If not, can we fix this?