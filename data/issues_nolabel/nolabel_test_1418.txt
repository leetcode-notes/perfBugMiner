Using 2 computers - Distributed Tensorflow

Copied from: http://stackoverflow.com/questions/35824742/tensorflow-distributed-assing-devices
I recently installed the version of tensorflow for distributed computers. From the trend: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime , i tried to implemented multiples gpus at multiples computers, and also looking at "https://stackoverflow.com/questions/34439045/tensorflow-setup-for-distributed-computing" found a white paper for some additional specifications. I can run the server and a worker on 2 different computers with 2 and 1 gpu, and using the session gprc, allocate and run the program on remote or local mode. i ran locally tensorflow in the remote computer with:
bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \
--cluster_spec='local|localhost:2500' --job_name=local --task_id=0 &

and for using on the server
bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \
--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=worker --task_id=0 \
--job_name=prs --task_id=0 &

However, when i try to specify the device for running on 2 computers at the same time the python show me the error:
Could not satisfy explicit device specification '/job:worker/task:0'
when i use
with tf.device("/job:prs/task:0/device:gpu:0"):
  x = tf.placeholder(tf.float32, [None, 784], name='x-input')
  W = tf.Variable(tf.zeros([784, 10]), name='weights')
with tf.device("/job:prs/task:0/device:gpu:1"):
  b = tf.Variable(tf.zeros([10], name='bias'))
# Use a name scope to organize nodes in the graph visualizer
with tf.device("/job:worker/task:0/device:gpu:0"):
  with tf.name_scope('Wx_b'):
    y = tf.nn.softmax(tf.matmul(x, W) + b)

or even changing the name of job. So I am wondering if it is required to Add a New Device (https://stackoverflow.com/questions/35213172/add-a-new-device-in-tensorflow) or probably i am doing something wrong with the initialization of the cluster.
Best Regards