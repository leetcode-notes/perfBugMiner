Running LinearClassifier.fit with SparseTensor

I'm trying to create a LinearClassifer with a sparse binary numpy coo matrix (reports) using a SparseTensor.  This is with TensorFlow 0.9.0
I do this as follows:
reports_indices = list()
rows,cols = reports.nonzero()
for row,col in zip(rows,cols):
   reports_indices.append([row,col])

x_sparsetensor = tf.SparseTensor(
  indices=reports_indices,
  values=[1] * len(reports_indices),
  shape=[reports.shape[0],reports.shape[1]])

The dimensions of reports is 10K by 1.5K.
I then setup the LinearClassifier as follows:
m = tf.contrib.learn.LinearClassifier()

m.fit(x=drug_cols,y=(1*y_vec).todense(),input_fn=None)

This results in the following error:
TypeError: object of type 'Tensor' has no len()
Is my construction incorrect for some reason?  Thanks in advance for any help.