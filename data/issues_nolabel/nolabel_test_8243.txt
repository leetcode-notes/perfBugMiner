[Time Consuming] TF C++ Session->Run - Images for Real-time Inference

[Tensorflow (TF) on CPU]
I am using the skeleton code provided for C++ TF inference [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc] in order to run a frozen model I have created in Python. This model is an FC NN with two hidden layers.
In my current project's code, I run the inference for each single image (8x8 pixels). For each sample, each Session->Run call takes about 0.02 seconds, which is expensive in my application. However, when I send a batch of 1560 samples, the Session->Run call takes about 0.03 seconds.
Are these time measurements normal for the Session->Run Call? From the C++ end, should I send my frozen model batches of images in one Tensor?  From the Python end, are there optimisation tricks to alleviate that bottleneck? Is there a way to concurrently do Session-Run calls in C++?
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#1439
http://stackoverflow.com/questions/39070708/why-sometimes-tensorflow-runs-slower-and-slower-with-the-process-of-training
Environment info
Operating System: Linux
Installed version of CUDA and cuDNN: N/A
What other attempted solutions have you tried?

I installed TF using the optimised instruction set for the CPU, but it does not seem to give me the huge time saving mentioned here [http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions]
Unified the session for the Graph I created.

Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).