Fix LSTM Layer Normalization implementation to match original paper

The Layer Norm paper (https://arxiv.org/abs/1607.06450) applies layer norm to LSTMs by separately normalizing the linear transformation of the inputs and the linear transformation of the recurrent cell state. See equation (20) in the Supplementary Material (page 13).
The current contrib LSTM layer norm implementations apply layer norm to LSTMs by separately normalizing each of the gate/cell update preactivations (separate columns of the sum of the linear transformations of the inputs and recurrent cell state). This is very different from the paper (different tensors are being normalized).
This PR changes the two implementations to match the paper. There is one slight difference with the paper: I omit two of the layer norm offset Î² parameters because there is a bias term already.