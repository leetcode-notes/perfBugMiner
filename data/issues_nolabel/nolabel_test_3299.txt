Frequent dynamic memory allocation in tensorflow/core/kernels/conv_ops.cc?

I am looking at the code in tensorflow/core/kernels/conv_ops.cc, Compute(OpKernelContext* context)  function:
// Output tensor is of the following dimensions: // [ in_batch, out_rows, out_cols, out_depth ] Tensor* output = nullptr; OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));
Does it mean each CONV OP will always dynamic allocate output tensor (including the buffer as the size of in_batch x out_rows x out_cols x out_depth) even if I load the graph once but iterate multiple times to inference multiple images? Where the memory was freed?
Thanks.