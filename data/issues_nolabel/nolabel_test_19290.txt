Multiple TITAN V introduce new error: CUB segmented reduce error

System information

Keras code (with tf backend) that works perfectly on multiples of Titan Xp or 1080 Ti and tested on multiple TF versions both on Windows and Ubuntu
Windows 10
TensorFlow  1.8.0 (b'v1.8.0-0-g93bc2e2072' 1.8.0)
Python 3.6.5
CUDA 9.0/ cuDNN 7.0
Two TITAN V

Describe the problem
We upgraded our hardware from two Titan Xp to two Titan V. Running the exact same training script that was running perfectly on two GPUs now produces the error:
tensorflow.python.framework.errors_impl.InternalError: CUB segmented reduce errorinvalid configuration argument

If we run the same script on a single GPU it runs fine. If we run on two GPUs with batch size '1' it also works fine. This could be a Keras issue, but I don't find anything in the Trackback to indicate that. The training starts but at Epoch 2 it crashes after going over some batches.
Also, a minor note, the word 'errorinvalid' has no space in between.
Source code / logs
Full trackback:
Traceback (most recent call last):
  File "C:\Users\User\AppData\Local\conda\conda\envs\py3tensorflow\lib\site-packages\tensorflow\python\client\session.py", line 1322, in _do_call
    return fn(*args)
  File "C:\Users\User\AppData\Local\conda\conda\envs\py3tensorflow\lib\site-packages\tensorflow\python\client\session.py", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "C:\Users\User\AppData\Local\conda\conda\envs\py3tensorflow\lib\site-packages\tensorflow\python\client\session.py", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: CUB segmented reduce errorinvalid configuration argument
         [[Node: loss/DEC_CONV1C_loss/Mean_1 = Mean[T=DT_FLOAT, Tidx=DT_INT32, _class=["loc:@training/Adam/gradients/loss/DEC_CONV1C_loss/Mean_1_grad/truediv"], keep_dims=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](loss/DEC_CONV1C_loss/Mean, training/Adam/gradients/loss/DEC_CONV1C_loss/Mean_1_grad/mod)]]
         [[Node: training/Adam/gradients/DEC_CONV1C_1/concat_grad/Slice_1/_429 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:GPU:1", send_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device_incarnation=1, tensor_name="edge_1072_training/Adam/gradients/DEC_CONV1C_1/concat_grad/Slice_1", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:1"]()]]

The error prints this twice.