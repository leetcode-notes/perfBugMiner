Kernel/Bias created for only one LSTM when using BasicLSTMCell with MultiRNNCell

I am using the example code ptb_word_lm.py that creates 2 layers of lstm's. The code clearly creates 2 (c,h) tuples, each tuple corresponding to one LSTM. Each c and h should depend on matrices i,j,f,o (input_gate, new_input, foget_gate, outpu_gate) that are found in BasicLSTMCell  in rnn_cell_impl.py. With tf.global_variables() you should be able to get these matrices. In fact, it returns a kernel matrix and a bias matrix. The kernel matrix is of size 400x800 which corresponds to only one LSTM parameters. (the i,j,f,o matrices are concatenated horizontally. The matrices for the input and h_t-1 are concatenated vertically). However, whether I am using 1 layer LSTM or 2 layer LSTM, the number of parameters stays the same (400x800), when it should be 400x1600 or a tuple of some sort or an additional kernel and bias. It seems that tf.global_variables() does not expose all trainable parameters in this case. Is that true or am I missing something?
EDIT:
I am now fairly certain that this is a problem, because when I use the "block" option in the script, multirnncell works correctly and generates 2 kernel matrices and 2 bias matrices. with BasicLSTMCell, it only generates one kernel matrix and 1 bias matrix.