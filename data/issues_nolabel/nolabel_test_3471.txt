C++ api runs much slower than Python API (compile flags)

My graph run in Python only takes 6 seconds for one batch, but when I run the identical batch on the same graph (graph_freeze) in the C++ Api, the time is 80 seconds. I'm guessing this 13x slowdown is probably from using the wrong C flags during compilation. This is all running on CPU only.
I'm loading the graphs using the same way as in the label_images example.
I took a look at: #2721, and added the -mavx C flag, which increased it by about double, but still 13x slower than the python.
The graph is a mostly a large multi layered regular RNN but with some feedforward as well.
Any ideas on how to get it to the same speed as python? Is there somewhere I can see what flags tensorflow installed from source is compiled with?
Environment info
Operating System: Linux ubuntu 64 bit 14.04
Installed version of CUDA and cuDNN:  None (CPU Only)
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

Which pip package you installed.
Linux 64 Bit CPU Python 3.5
The output from python -c "import tensorflow; print(tensorflow.__version__)".
0.9.0

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

Steps to reproduce

Create graph in python
freeze_graph.py
Load graph in C++

What have you tried?

adding -mavx C flag

Logs or other output that would be helpful
(If logs are large, please upload as attachment).