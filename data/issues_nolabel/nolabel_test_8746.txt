Gradients are not averaged when using GradientDescentOptimizer

When using the GradientDescentOptimizer the weights are updated using var.device(d) -= grad * lr();. The gradients are not averaged as we could expect. That why my model was working fine with a mini-batch gradient descent but not with the batch gradient descent.
The trick is to use GradientDescentOptimizer(1/m * 0.01) where m is the size of the batch.
The corresponding StackOverflow issue is here: http://stackoverflow.com/questions/43027109/tensorflow-weights-increasing-when-using-the-full-dataset-for-the-gradient-desce