why training and validation loss are always zero during the training phase of deep neural network?

I'm new to machine learning and I'm trying to learn more about it. I have been facing many problems doing my project as DEEP NEURAL NETWORK Classifier (classes 0,1).
I'm using windows 8.1, 4GB ram, python 3.6.4(pip installation), tensorflow cpu version v1.4 (pip installation).
All other packages required for this code also installed through pip installation and have been imported.
During training, Train loss and validation loss are always zero. I don't know why?
If anyone know the problem, please let me know asap!! My project deadline is very near!!
Things I've tried:

different values for hidden_units, no. of hidden layers, batch_size, learning_rate
Equally distributed my sample inputs according to the class labels.
I gave the same data to LDA (Linear Discriminant Analysis) and it's able to classify at the accuracy of 99.6%
I gave the same data to bernoulliRBM and it classifies all samples as class '0'

here is my CODE
def build_neural_network(hidden_units_1=8, hidden_units_2=16):

    tf.reset_default_graph()
    inputs = tf.placeholder(tf.float64, shape=[None, train_x.shape[1]])
    labels = tf.placeholder(tf.float64, shape=[None, 1])
    learning_rate = tf.placeholder(tf.float64)
    is_training=tf.Variable(True,dtype=tf.bool)
    initializer = tf.contrib.layers.xavier_initializer()
    fc = tf.layers.dense(inputs, hidden_units_1, activation=None,kernel_initializer=initializer)
    fc=tf.layers.batch_normalization(fc, training=is_training)
    fc=tf.nn.relu(fc)
    fc = tf.layers.dense(fc, hidden_units_2, activation=None,kernel_initializer=initializer)
    fc=tf.layers.batch_normalization(fc, training=is_training)
    fc=tf.nn.relu(fc)
    logits = tf.layers.dense(fc, 1, activation=None)
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
    cost = tf.reduce_mean(cross_entropy)
   
    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
    
    predicted = tf.nn.softmax(logits)
    correct_pred = tf.equal(tf.round(predicted), labels)
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))

    export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',
                'cost', 'optimizer', 'predicted', 'accuracy']
    Graph = namedtuple('Graph', export_nodes)
    local_dict = locals()
    graph = Graph(*[local_dict[each] for each in export_nodes])
    
    return graph

model = build_neural_network()

epochs = 10
train_collect = 10
train_print=train_collect*2
learning_rate_value = 0.01 
batch_size=100
x_collect = []
train_loss_collect = []
train_acc_collect = []
valid_loss_collect = []
valid_acc_collect = []

saver = tf.train.Saver()
with tf.Session() as sess:
    saver.restore(sess, "./save.ckpt")
    iteration=0
    for e in range(epochs):
          for batch_x,batch_y in get_batch(train_x,train_y,batch_size):
                iteration+=1
                feed = {model.inputs: train_x,
                            model.labels: train_y,
                            model.learning_rate: learning_rate_value,
                            model.is_training:True,
                            }
                train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], 
                        feed_dict=feed)
                
                if iteration % train_collect == 0:
                    x_collect.append(e)
                    train_loss_collect.append(train_loss)
                    train_acc_collect.append(train_acc)
                
                    if iteration % train_print==0:
                        print("Epoch: {}/{}".format(e + 1, epochs),
                        "Train Loss: {:.4f}".format(train_loss),
                        "Train Acc: {:.4f}".format(train_acc))
                    
                    feed = {model.inputs: valid_x,
                                model.labels: valid_y,
                                model.is_training:False
                                }
                    val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)
                    valid_loss_collect.append(val_loss)
                    valid_acc_collect.append(val_acc)
                                   
                    if iteration % train_print==0:
                        print("Epoch: {}/{}".format(e + 1, epochs),
                       "Validation Loss: {:.4f}".format(val_loss),
                       "Validation Acc: {:.4f}".format(val_acc))
            

saver.save(sess, "./save.ckpt")

#here is the OUTPUT of training:
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949

 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949
 Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013
 ...

(I'm getting the same result as above till epoch 10/10)