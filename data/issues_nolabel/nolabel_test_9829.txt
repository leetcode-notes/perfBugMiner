Kernel Restarting The kernel appears to have died. It will restart automatically (Jupyter-Tensorflow)

I am facing a huge problem where the jupyter kernel keeps dying. This is my first experience of kernel ending on Jupyter. I'm using Tensorflow to fit a CNN model (the total input datasize is only 9.8MB)
I did not have this issue in my previous run on the same code (however I did have errors where it said "dst tensor is not initialized". That was when I made the dataset really small to attempt to fit it. Could someone please kindly help ?
This is the code:
import tensorflow as tf
import numpy as np
IMG_PX_SIZE = 50
HM_SLICES = 20
n_classes = 2
x = tf.placeholder('float')
y = tf.placeholder('float')
keep_rate = 0.8
keep_prob = tf.placeholder(tf.float32)
def conv3d(x, W):
return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')
def maxpool3d(x):
return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')
def convolutional_neural_network(x):
weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),
'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),
'W_fc':tf.Variable(tf.random_normal([62720  ,1024])),
'out':tf.Variable(tf.random_normal([1024, n_classes]))}
biases = {'b_conv1':tf.Variable(tf.random_normal([32])),
           'b_conv2':tf.Variable(tf.random_normal([64])),
           'b_fc':tf.Variable(tf.random_normal([1024])),
           'out':tf.Variable(tf.random_normal([n_classes]))}

x = tf.reshape(x, shape=[-1, IMG_PX_SIZE, IMG_PX_SIZE, HM_SLICES, 1])

conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])
conv1 = maxpool3d(conv1)

conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])
conv2 = maxpool3d(conv2)

fc = tf.reshape(conv2,[-1, 62720  ])
fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])
fc = tf.nn.dropout(fc, keep_rate)

output = tf.matmul(fc, weights['out']) + biases['out']

return output

def train_neural_network(x):
much_data = np.load('muchdata_sampled-50-50-20.npy')
train_data = much_data[:100]
validation_data = much_data[-100:]

prediction = convolutional_neural_network(x)
cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )
optimizer = tf.train.AdamOptimizer().minimize(cost)

hm_epochs = 3
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for epoch in range(hm_epochs):
        epoch_loss = 0
        for data in train_data:
            X = data[0]
            Y = data[1]
            _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})
            epoch_loss += c

        print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)

    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
    print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))

train_neural_network(x)
(My System information)
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes

OS Platform and Distribution:

OS = Windows 10

TensorFlow installed from (source or binary):
*Installed from Source
TensorFlow version (use command below):
*When I ran => python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
*I get only this result => b'unknown' 1.0.0
CUDA/cuDNN version:
*cuda_8.0.61_win10
*cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0
GPU model and memory:
*GeForce GTX 1050 graphics card
*RAM 32GB