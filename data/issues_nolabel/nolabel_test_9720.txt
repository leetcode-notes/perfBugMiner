tf.while_loop swap_memory=True when not running train_op, does it always swap?

From the documentation of tf.while_loop:

For training, TensorFlow remembers the tensors that are produced in the forward inference but needed in back propagation. These tensors can be a main source of memory consumption and often cause OOM problems when training on GPUs. When the flag swap_memory is true, we swap out these tensors from GPU to CPU. This for example allows us to train RNN models with very long sequences and large batches.

But after having trained, is TensorFlow's engine smart enough to avoid swapping even if swap_memory=True or should optimize_for_inference.py flip swap_memory (i.e. make the equivalent modifications to the graph)?
Naturally this only applies to running inference on a GPU.
@yuanbyu