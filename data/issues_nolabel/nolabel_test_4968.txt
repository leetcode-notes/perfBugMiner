TensorFlow produces arbitrary results without error when using large amounts of GPU memory

Environment info
Operating System: Docker container based on nvidia/cuda:7.5-cudnn5-devel running on CentOS
If installed from binary pip package, provide:

A link to the pip package you installed: pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".

I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0rc0

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import numpy as np
import tensorflow as tf

mb = 100
sz = int(mb * 1e6 / 4)
total_gb = 11
n = int(total_gb / mb * 1000)
x = np.array(np.random.randn(sz), dtype=np.float32)

with tf.Graph().as_default():
    place = tf.placeholder(tf.float32, shape=[sz])
    with tf.device('/gpu:0'):
        x_gpu = [tf.Variable(place, dtype=tf.float32) for i in range(n)]
    init = tf.initialize_all_variables()
    with tf.Session() as sess:
        sess.run(init, feed_dict={place: x})
        # I know it's not optimal, but to make it more comparable to the Theano code below
        x_out = [sess.run(v) for v in x_gpu]

delta = [np.sum(np.abs(xi - x)) for xi in x_out]
print(delta)
Executed using python3 memcheck_tensorflow.py
Expected output:
some tensorflow output ...

[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Actual output:
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: Tesla K40m
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:81:00.0
Total memory: 11.25GiB
Free memory: 11.15GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:81:00.0)
[28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28212806.0, 28209880.0, 28209880.0, 28208760.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28206538.0, 28206538.0, 28209880.0, 28208760.0, 28212806.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28207726.0, 28208760.0, 28206538.0, 28209880.0, 28206538.0, 28210806.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209880.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28206602.0, 28209472.0, 28209472.0, 28208760.0, 28208760.0, 28209880.0, 28208760.0, 28207726.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28206602.0, 28209880.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28208760.0, 28206538.0, 28209880.0, 28209880.0, 28208760.0, 28208760.0, 28206602.0, 28207726.0, 28209880.0, 28207726.0, 28209880.0, 28209880.0, 28209880.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28206538.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28210806.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28209880.0, 28206602.0, 28208760.0, 28208760.0]

What other attempted solutions have you tried?
I've verified that this problem exists on at least one other GPU (I've seen it on Nvidia GPUs K40m and K80).
I've run similar code in Theano without any problems (same GPU, same Docker container):
import numpy as np
import theano

mb = 100
sz = int(mb * 1e6 / 4)
total_gb = 11
n = int(total_gb / mb * 1000)
x = np.array(np.random.randn(sz), dtype=np.float32)

x_gpu = [theano.shared(x) for i in range(n)]
x_out = [v.get_value() for v in x_gpu]

delta = [np.sum(np.abs(xi - x)) for xi in x_out]
print(delta)
Executed using THEANO_FLAGS=device=gpu,floatX=float32 python3 memcheck_theano.py
Output
Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5103)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]