Regularization loss will duplicated when reusing PartitionedVariable in tf.layers

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version: 2.7.11
Bazel version (if compiling from source): 0.10.0
GCC/Compiler version (if compiling from source): 4.8.5
CUDA/cuDNN version: CUDA 8.0/ cuDNN 7
GPU model and memory: N/A
Exact command to reproduce: see below

Describe the problem
When reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers. I have found that there is no special treatment for reusing regularization loss of PartitionedVariables in tf.layers .
Source code / logs
Here is the small script can reproduce the result.
import tensorflow as tf
partitioner = tf.fixed_size_partitioner(3)
l2_regularizer = tf.contrib.layers.l2_regularizer(0.001)
for i in xrange(2):
  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):
    inputs_tensor = tf.constant(1.0, shape=[100, 100])
    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name="fc", kernel_regularizer=l2_regularizer)
print (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))

This short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6.
A pull request has been submitted here to fix this bug: #19053