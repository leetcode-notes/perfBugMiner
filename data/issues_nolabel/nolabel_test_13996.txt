Proper way to handle csv input for cpu training?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
('v1.3.0-rc2-20-g0787eee', '1.3.0')
Python version:
Python 2.7.12 from Anaconda

Describe the problem
I am training some classification model with an 32-cpu Ubuntu machine and one of the problem is to feed data fast enough to the training process.
I am trying to read data from some csv file but the default tf.csv or tf.data module seems to be slow.
A speed test for reading 1000000 row * 17 column csv file shows a speed like :

tf.decode_csv with queue and theads  :   ~192 seconds
tf.data :  ~164 seconds
hand write cpp reading op :  ~25 seconds
pure python code with help from pandas : ~23 seconds

It is fast enough to use pandas for one single file, but it might face the GIL problem if try to speed up with more threads.
Codes can be found below. I am not sure if I use it the right way, is there any official benchmarks or guidelines for this?
Source code / logs
https://github.com/littleDing/mini_csv_reader
The speed test is run through speed_test.py