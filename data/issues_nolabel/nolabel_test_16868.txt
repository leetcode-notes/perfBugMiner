Backpropagation/weight update issue with custom layer

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win 10
TensorFlow installed from (source or binary): From pip (binary)
TensorFlow version (use command below): 1.5
Python version:  3.5
Bazel version (if compiling from source):---
GCC/Compiler version (if compiling from source):----
CUDA/cuDNN version:None
GPU model and memory:None
Exact command to reproduce:See source code

Describe the problem
I am attempting to implement a custom layer. The layer uses the Image to Patch function and simple tensorflow operator. The layer is implemented in keras to simplify the model building and training but the backend is in tensorflow.
I am using a simple cnn as a benchmark, whenever I implement my custom layer ( even only as the first layer to 'encode' the data) backpropagation seems to break as no weights get updated in the entirety of the model.
From my understanding the all the operations used (mult, div, add, minus) are differentiable and things such as reshape, transpose and extract_image_patches should not prevent backpropagation and weight updates.
I tried using the basic layer building method and inheriting from the convolution class (_Conv) and both cases prevent the weight update for the whole model, but such a thing shouldn't be the case.
Source code / logs
Prototype layer: https://github.com/roya0045/cvar2/blob/master/tfvar.py
Model builder: https://github.com/roya0045/cvar2/blob/master/test2.py