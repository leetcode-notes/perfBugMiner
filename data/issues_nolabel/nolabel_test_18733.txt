How can we handle resource exhaust error coming while allocating a tensor of a particular size.

I am currently using single GPU : GEFORCE GTX1080 8GB and I am unable to handle an error coming while I am trying to train a network. Error is :
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[131072,2048]
[[Node: training/Adam/mul_3 = Mul[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](Adam/beta_1/read, training/Adam/Variable/read)]]
[[Node: loss/mul/_179 = _Recvclient_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_697_loss/mul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]]
However, I understand that I have limited memory in GPU which has to be utilized. I currently just wanting to know if there is any workaround of this by which I can train the network without reducing the dimensions of the network.
Model :
model = Sequential()
model.add(TimeDistributed(Flatten(), input_shape = (24,8,8,2048)))
model.add(Dropout(0.5))
model.add(LSTM(512, return_sequences=True, dropout=0.5))
model.add(LSTM(512, return_sequences=False, dropout=0.5))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(5, activation='softmax'))

Input to this network is actually image embedding I am getting from InceptionV3 without top.
Any help in this regard is appreciated.
Thanks in advance.