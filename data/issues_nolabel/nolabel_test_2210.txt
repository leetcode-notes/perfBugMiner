Sharing a GPU between tensorflow and another cuda code.

Hello,
I am trying to run tensorflow and another cuda code in the same python script. My script is coded such that it iteratively uses the other cuda code to generate bunch of samples and then it uses tensorflow to process them. The tensoflow session is always open while doing these iterations.
The problem is that in tensorflow 0.7, I get the following error when the session is closing (exiting python):
F tensorflow/stream_executor/cuda/cuda_driver.cc:302] current context was not created by the StreamExecutor cuda_driver API: 0x2eb5c50; a CUDA runtime call was likely performed without using a StreamExecutor context
And, in tensorflow 0.8, I get another error in middle of run:
*** Error in `python': munmap_chunk(): invalid pointer: 0x00007ffea63b7380 ***
Aborted (core dumped)
I have modified my other cuda code to use cuda's Stream Executor context but this did not solve any of these issues. I'm also pretty sure my cuda code does not have any memory leak. If I replace my cuda code with its cpu version or the tensorflow operations with numpy counterparts every thing works perfectly. I have also limited gpu memory for TF using:
tf.GPUOptions(per_process_gpu_memory_fraction=0.5)
I am wondering if we actually can share a single GPU between tenorflow and another cuda code in the same script? If so, is there any particular protocol that I should follow.