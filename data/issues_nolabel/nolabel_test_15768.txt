Training broke with ResourceExausted error

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.4.0
Python version: 3.5
CUDA/cuDNN version: 8
GPU model and memory: TITAN X, 12207MiB


Most Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.
https://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error
Here is the code of the model, https://paste.ubuntu.com/26298336/
A short description of the model would be,

Character level Embedding Vector -> Embedding lookup -> LSTM1
Word level Embedding Vector->Embedding lookup -> LSTM2
[LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer
[LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator

While running the code it produces the following error output at the epoch 32,
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device="/job:localhost/replica:0/task:0/device:GPU:0"](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack", tensor_type=DT_INT32, _device="/job:localhost/replica:0/task:0/device:CPU:0"]]
My question is if there is any error then it should occur in the first epoch why at 32 epoch?
I am using embedding_lookup in following way,
_word_embeddings = tf.Variable(
                        embeddings,
                        name="_word_embeddings",
                        dtype=tf.float32,
                        trainable=False)
            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name="word_embeddings")


Where embeddings is a (61698, 100) sized vector. which is only 24 MB. However in the error message, it showed the error with, (24760, 100) sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,

gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.