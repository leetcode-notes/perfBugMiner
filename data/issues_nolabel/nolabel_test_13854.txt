tf.train.SyncReplicasOptimizer training does not start

System information

**Have I written custom code **: Yes
**OS Platform and Distribution **: Linux Ubuntu 16.04
**TensorFlow installed from **: binary
TensorFlow version (use command below): tensorflow-gpu==1.3.0
Python version: Python 3.6.2
Bazel version (if compiling from source): not installed
CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61
GPU model and memory: NVIDIA Quadro P5000 16GB
Exact command to reproduce:
write actual IP addresses instead of  ip_address1 and ip_address2

( on machine 1 )
$  python trainer.py 
--replicas_num=1 
--ps_hosts=ip_address1:2222 
--worker_hosts=ip_address2:2223 
--job_name=ps --task_index=0
( on machine 2 )
$  python trainer.py 
--replicas_num=1 
--ps_hosts=ip_address1:2222 
--worker_hosts=ip_address2:2223 
--job_name=worker --task_index=0
Describe the problem
I'm trying to train an below model with distributed synchronized training.
I tried, 1 ps and 1 worker, 1 ps and 2 workers, 1 ps and 3 workers.
It is a sample code similar to https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program
If I comment out line 71 shown below training became asynchronized trainig and there is no problem.
opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)
But if I run it with line 71 not commented out training will not start.
(I followed https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#usage to make it synchronized.)
There is no error. But trainig will not continue. sess.run() (line 96) never ends.
Is there any suggestions what might be the problem ?
Source code / logs
Source code
import tensorflow as tf
from tensorflow.contrib import slim
from tensorflow.examples.tutorials.mnist import input_data

flags = tf.flags
flags.DEFINE_string("ps_hosts", "", "Comma-separated list of hostname:port pairs")
flags.DEFINE_string("worker_hosts", "", "Comma-separated list of hostname:port pairs")
flags.DEFINE_string("job_name", "", "One of 'ps', 'worker'")
flags.DEFINE_integer("task_index", 0, "Index of task within the job")
flags.DEFINE_integer("replicas_num", 3, "Number of replicas")
FLAGS = flags.FLAGS

def main(_):
    # config
    BATCH_SIZE = 10
    TRAINING_STEPS = 5000
    PRINT_EVERY = 100
    LOG_DIR = "/tmp/"

    ps_hosts = FLAGS.ps_hosts.split(",")
    worker_hosts = FLAGS.worker_hosts.split(",")

    # cluster specification
    cluster = tf.train.ClusterSpec({"ps": ps_hosts, "worker": worker_hosts})

    # start a server for a specific task
    server = tf.train.Server(cluster,
                             job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)

    def net(x):
        x_image = tf.reshape(x, [-1, 28, 28, 1])
        net = slim.layers.conv2d(x_image, 32, [5, 5], scope='conv1')
        net = slim.layers.max_pool2d(net, [2, 2], scope='pool1')
        net = slim.layers.conv2d(net, 64, [5, 5], scope='conv2')
        net = slim.layers.max_pool2d(net, [2, 2], scope='pool2')
        net = slim.layers.flatten(net, scope='flatten')
        net = slim.layers.fully_connected(net, 500, scope='fully_connected')
        net = slim.layers.fully_connected(net, 10, activation_fn=None, scope='pred')
        return net

    print("job_name     = %s" % FLAGS.job_name)
    print("task_index   = %d" % FLAGS.task_index)
    print("replicas_num = %d" % FLAGS.replicas_num)

    if FLAGS.job_name == "ps":
        server.join()
    elif FLAGS.job_name == "worker":
        # Between-graph replication
        with tf.device(tf.train.replica_device_setter(
                worker_device="/job:worker/task:%d" % FLAGS.task_index,
                cluster=cluster)):
            # count the number of updates
            global_step = tf.get_variable('global_step', [],
                                          initializer=tf.constant_initializer(0),
                                          trainable=False)

            x = tf.placeholder(tf.float32, shape=[None, 784], name="x-input")
            # target 10 output classes
            y_ = tf.placeholder(tf.float32, shape=[None, 10], name="y-input")
            y = net(x)

            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))
            #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

            opt = tf.train.AdamOptimizer(1e-4)

            # for synchronous training
            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)

            train_step = opt.minimize(cross_entropy, global_step=global_step)
            #sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0))

            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

            init_op = tf.global_variables_initializer()

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),
                                 logdir=LOG_DIR,
                                 global_step=global_step,
                                 init_op=init_op)

        with sv.managed_session(server.target) as sess:
            step = 0

            test = 0
            while not sv.should_stop() and step <= TRAINING_STEPS:
                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)

                print("--------- run_step: test %d ----------" % test)
                test += 1

                _, acc, step = sess.run([train_step, accuracy, global_step],
                                        feed_dict={x: batch_x, y_: batch_y})
                print("--------- run_step: test %d ----------" % test)
                test += 1

                if step % PRINT_EVERY == 0:
                    print("Worker : {}, Step: {}, Accuracy (batch): {}".\
                          format(FLAGS.task_index, step, acc))

            test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})
            print("Test-Accuracy: {}".format(test_acc))

        sv.stop()


if __name__ == "__main__":
    tf.app.run(main=main)

Logs
I got logs shown below from the parameter server and the worker
from the parameter server:
...
job_name     = ps
task_index   = 0
replicas_num = 1
from the worker:
...
job_name     = worker
task_index   = 0
replicas_num = 1
2017-10-20 17:54:43.891111: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 10bf51b4613ac80d with config:
--------- run_step: test 0 ----------