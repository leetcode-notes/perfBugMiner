NaNs during training with `tf.contrib.rnn.LayerNormBasicLSTMCell`

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.3
Python version:  3.5
CUDA/cuDNN version: CUDA 8.0 and CuDNN 6 (although I can replicate without a GPU)
GPU model and memory: Nvidia K80 (from Amazon P2 instance)
Exact command to reproduce:

Describe the problem
When training a model using tf.contrib.rnn.LayerNormBasicLSTMCell, sometimes my weights go to nan, even though the training data looks perfectly innocent.
I have not seen this with Tensorflow 1.2.1, which leads me to suspect that there's been a regression somewhere, but I could've just been luckier (üçÄ) with TF 1.2 (nevermind -- I have reproduced this with TF 1.2.1)
Source code / logs
I've created two examples of this in https://github.com/alanhdu/tensorflow-12512 (clone the repo, enter a folder, and run test.py or build and run the Dockerfile. The key line(s) there are:
print([np.isfinite(v).all() for v in sess.run(tf.trainable_variables())])
sess.run(train_step, feed_dict)
print([np.isfinite(v).all() for v in sess.run(tf.trainable_variables())])
The first print statement prints all Trues, which is good -- but after the training step, suddenly some of the weights have nans in them (and hence there's one False in the second print statement).