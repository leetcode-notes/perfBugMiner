Distributed TF managed session: inter_op_parallelism_threads and intra_op_parallelism_threads has no effect

We are using:
with sv.managed_session(
  server.target,
  config=tf.ConfigProto(log_device_placement=True,
                        inter_op_parallelism_threads=1,
                        intra_op_parallelism_threads=1),
) as sess:

but TensorFlow still launches a number of threads proportional to the number of cores on each process. I counted +50 threads used by each process on my 24 vcore machine.
What can we do to limit the number of threads in a managed session.
EDIT: @yaroslavvb identified the problem. See the end of the thread about Session vs Server options.