All weights become nan after training more than about 20000+10000 times

Operating System: CentOS 7
Installed version of CUDA and cuDNN: cuda 7.5.18, cudnn4
When I run my own project, I found that all weights would become nan after training more than 20000+10000 times (That means I first trained the model for 20000 times, and then I continued training the model for 10000 times). The accuracy would suddenly change from 0.8 to 0.5. I thought that might be my own problem of setting weights and they become divergent after a certain point.
But we I rerun the Tensorflow tutorial: "Deep MNIST for Experts", I got the same problem.
Follow the instruction, I run the following script:
for i in range(20000):
batch = mnist.train.next_batch(50)
if i%100 == 0:
train_accuracy = accuracy.eval(feed_dict={
x:batch[0], y_: batch[1], keep_prob: 1.0})
print("step %d, training accuracy %g"%(i, train_accuracy))
train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
Everything is perfect until now, and the training accuracy is as high as 0.99.
However, when I rerun the above script, something strange happened. The training accuracy suddenly become around 0.1 and all weights become nan. Like following:


To reproduce the problem, first train the model for 20000 times, and then continue training the module for 20000 times, using another for loop.