XLA: Won't converge and doesn't respect visible devices.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (writing a copy I can share)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Nvidia Optimized Container 17.12
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.4.1
Python version: IntelPython 3.5
Bazel version (if compiling from source): 0.5.4
GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
CUDA/cuDNN version: 9.0.176 with CUBLAS Basic Accelerated Linear Algebra 9.0.234 / 7
GPU model and memory: Pascal Titan 12Gb
Exact command to reproduce: Coming

Describe the problem
I've seen 2 problems with XLA:

XLA doesn't respect visible devices (neither CUDA Visible devices or tf config options)
XLA doesn't converge

1
XLA doesn't respect visible devices, I have seen this in both Horovod and when running on a single GPU (on a Multi-GPU desktop), N XLA instances are created despite only one GPU being used.
2
When I run my script with and without XLA, the XLA compiled version plateaus and doesn't converge, whilst the none-XLA version converges. Where XLA has a loss of a magnitude higher.
The benchmark is a VGG network (with BatchNorm) and achieves a 3x speed up with XLA, it just doesn't converge.
Is this a known issue? Or is it just a possibility of occurring?