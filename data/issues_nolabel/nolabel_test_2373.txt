Embedding lookup table doesn't mask padding value

Hi there,
I'm using an embedding_lookup operation in order to generate dense vector representations for each token in my document which are feed to a convolutional neural network (the network architecture is similar to the one in a WildML article).
Everything works correctly but when I pad my document inserting a padding value in it, the embedding lookup generates a vector for this token too. I think that this approach could alterate the results in the classification task. What I want to achieve is something similar to what Torch LookupTableMaskZero does.

Is correct what I want to do?
Is already implemented something like this?
If not, how can I mask the padding value in order to prevent the generation of the corresponding vector for it?

Thank you in advance,
Alessandro