tensorflow.python.debug.cli.offline_analyzer failed to read debug data from HDFS filesys

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.3.0 from master branch
Python version: Python 2.7.12 (default, Nov 19 2016, 06:48:10)
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: null
GPU model and memory: null
Exact command to reproduce: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>

Issue description
I saved debug data by DumpingDebugHook into hdfs filesys and then it failed to read the data by python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path>, but it works well with the local filesys by the same way.
Error info:
# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100
tfdbg offline: FLAGS.dump_dir = hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
hdfsExists: invokeMethod((Lorg/apache/hadoop/fs/Path;)Z) error:
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "G53.ad000000000427.et2/11.140.133.72"; destination host is: "ns1":8020;
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:454)
	at sun.nio.ch.Net.connect(Net.java:446)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py", line 78, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py", line 41, in main
    FLAGS.dump_dir, validate=FLAGS.validate_graph)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py", line 682, in __init__
    raise IOError("Dump root directory %s does not exist" % dump_root)
IOError: Dump root directory hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100 does not exist

Be sure that the above hdfs dir exists, which including debug data by DumpingDebugHook, as below:
# hdfs dfs -ls -d hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100
drwxr-xr-x   - root supergroup          0 2017-09-18 08:01 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100