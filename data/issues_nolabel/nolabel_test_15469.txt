Compute test accuracy in batches to avoid OOM on GPUs

Reported here: #136
Alternative to this for mnist_deep.py: #157
Note that some reports in #136 claim that BFC solves the problem and that it would be included in the next binary release, but as this writing is two years after those comments, it stands to reason that the example still doesn't work out of the box for "low" memory GPUs.
Also noting for completeness that I am running on a GeForce GTX 670 (2GB) and that avg(avg(x_i)) = avg(x_i) when |x_i| for all i is equal.