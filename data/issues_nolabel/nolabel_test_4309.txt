In-place operations (dropout, ReLU, etc..)

As far as I understand, tensorflow does not support in-place operation of operations such as ReLU, Dropout, etc..
If an operation outputs the same type and shape tensor and does not involve any branching in the graph then it should be possible to operate on the same chunk of memory.
As it stands, tensorflow doesn't support this which severely increases memory usage for large deep learning models.