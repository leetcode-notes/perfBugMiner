tf.set_random_seed does not reset random op state

TF Version: 1.1.0rc1 (installed from nightly: Apr 10, 2017 1:03 AM)
(run on CPU, Python 2)
import tensorflow as tf
import numpy as np

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

tf.set_random_seed(1)
a = tf.truncated_normal_initializer(seed=None)([1])
print(a.eval())

tf.set_random_seed(1)
b = tf.truncated_normal_initializer(seed=None)([1])
print(b.eval())

Output:
[ 1.05293429]
[-0.4487586]

Expected:
The same value, since...

If the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.

The values are identical in repeated runs of the whole script, but not after resetting the graph-level seed (as in the example above).
(possibly related to #9003)