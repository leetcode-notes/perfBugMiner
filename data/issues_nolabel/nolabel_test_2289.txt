Broadcasting ops on numpy arrays creates tons of ops

Hi,
I had a little toy project where I took in some data from numpy:
x_data = np.zeros([18145, 3], np.float32)
b_init = np.zeros([1, 3], np.float32)
b_init[0][0] = 16.0
b_init[0][1] = 128.0
b_init[0][2] = 128.0
W_init = np.zeros([3, 3], np.float32)

[fill in x_data here]

W = tf.Variable(W_init)
b = tf.Variable(b_init)
y = tf.matmul(x_data - b, W)

This took a lot of time and memory to set up (several minutes), and it was nowhere obvious why.
It eventually turned out that the broadcasting on the subtraction is what's causing the issue; it creates one op per line in x_data. I needed to write
y = tf.matmul(tf.constant(x_data) - b, W)

to make it fast.
Shouldn't this be the default behavior?