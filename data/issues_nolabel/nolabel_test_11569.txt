Why use memory_layer in all cases?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 14.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.2

In current AttentionWrapper, both BahdanauAttention and LuongAttention enforce to use a memory_layer. According to my understanding, it is needed only when the depth of memory is not matched with that of query_layer. Is it intended to be in this manner?
@ebrevdo , would you mind having a look at this?