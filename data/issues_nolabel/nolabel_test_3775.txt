reduce_mean is not numerically accurate on GPU

For smaller size matrix tf.reduce_mean works fine.
But for larger size matrix floating point results are not accurate.
I am running on CUDA 7.0, Titan X.
import tensorflow as tf

mat3 = tf.get_variable("mat3", [4500,4500], initializer=tf.constant_initializer(-1.))
reduce_mat3_ref = tf.reduce_mean(mat3)
with tf.Session('') as sess:
    tf.initialize_all_variables().run()
    reduce1 = sess.run(reduce_mat3_ref)
    print(reduce1) # on CPU this is -1.0, on GPU this produces -1.00006
    assert(reduce1 == -1.)