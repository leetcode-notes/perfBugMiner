Understanding cause of bad performance

I am training a convnet with multilple gpus and was using the cifar10 model as an example. It computes the gradients in every tower, stitches them and averages them. But that is mathematically equivalent to defining total_loss = tf.add_n(tower_loss_list) so I thought I would just do that.
But this implementation with 2 gpus runs at 1/2 the speed of the single gpu implementation. I am guessing the reason is that the gradient ops placement is very bad, and forces a lot data to be moved around.
Do you also think that is the reason?
If so, what could be done to improve the automatic device assignment?