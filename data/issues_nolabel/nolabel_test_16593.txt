contrib.tfgan: batch_norm is_training=True for both training and inferencing, non-slim version

Hi, I am exploring contrib.tfgan, such a great work @joel-shor .
batch_norm is_training=True for both training and inferencing
However, when I see the example in source code of both generator and discriminator of MNIST, as below.
https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb
with slim.arg_scope( [layers.fully_connected, layers.conv2d_transpose], activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm, weights_regularizer=layers.l2_regularizer(weight_decay)): net = layers.fully_connected(noise, 1024) net = layers.fully_connected(net, 7 * 7 * 256) net = tf.reshape(net, [-1, 7, 7, 256])
The default argument of layers.batch_norm is set to True, and this gen_fn and dis_fn are used for  both training phase and generating test images phase (inferencing).
Is it a bug or it is intended? If it is intended, can you explain why is that?
non-slim implementation
In addition, I don't really like slim, and I believe some people don't either. Can I use other model construction libraries like tf.layers or keras to build the network. Is tfslim a must?
Thank you,
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.5 and 1.4.1
Python version: 3.6
Bazel version (if compiling from source): 0.7
GCC/Compiler version (if compiling from source): 4.2
CUDA/cuDNN version:NA (CPU)
GPU model and memory:NA
Exact command to reproduce: