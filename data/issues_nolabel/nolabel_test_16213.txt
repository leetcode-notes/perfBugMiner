Non-chief replicas freeze after chief completes training

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): v1.4.0-19-ga52c8d9, 1.4.1
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 6.0
GPU model and memory: Titan X (Pascal), 12 GiB
Exact command to reproduce: Custom Script

Describe the problem
Non-chief replicas freeze after chief completes training, when in synchronous mode.
The problem appears to occur immediately after the chief shuts down.
See attached source code for a complete example of multi-GPU demonstrating the problem on a toy dataset. Modify the cluster variable and start the PS first, followed by workers then the chief node last. This is somewhat broken out in run_distributed.sh.
sv = tf.train.Supervisor( is_chief=(FLAGS.task_index == 0), global_step = global_step, init_op = init_op ) ... with sv.prepare_or_wait_for_session(server.target, config=config) as sess: # is chief if FLAGS.task_index == 0: sv.start_queue_runners(sess, [chief_queue_runner]) sess.run(init_token_op) ...
Source code / logs
rnn-multi-gpu.zip