Quantize Neural Networks with TensorFlow doesn't work on my models

I'm following this tutorial here.
And this code works fine.
curl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz
tar xzf /tmp/inceptionv3.tgz -C /tmp/
bazel build tensorflow/tools/quantization/tools:quantize_graph
bazel-bin/tensorflow/tools/quantization/tools/quantize_graph \
  --input=/tmp/classify_image_graph_def.pb \
  --output_node_names="softmax" --output=/tmp/quantized_graph.pb \
  --mode=eightbit

But when I try with another .pb it does not work, it tells me that the output node does not exist.
With non frozen .pb
Traceback (most recent call last):
  File "/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py", line 1304, in <module>
    app.run()
  File "/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py", line 1271, in main
    tf_graph.ParseFromString(data)
  File "/usr/local/lib/python2.7/dist-packages/google/protobuf/message.py", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File "/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py", line 1091, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File "/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py", line 1117, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File "/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/decoder.py", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File "/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/decoder.py", line 820, in _RaiseInvalidWireType
    raise _DecodeError('Tag had invalid wire type.')
google.protobuf.message.DecodeError: Tag had invalid wire type.


With frozen .pb
Traceback (most recent call last):
  File "/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py", line 1304, in <module>
    app.run()
  File "/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py", line 1295, in main
    output_graph = rewriter.rewrite(FLAGS.output_node_names.split(","))
  File "/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py", line 400, in rewrite
    for output_node_name in output_node_names
KeyError: 'my_output_node'

I'm sure the output_node_names is correct.
Is there any requirements on the .pb file so the tool can quantize it ?
nb: one should edit the doc because the tool is no longer in tensorflow/tools/quantization/tools:quantize_graph but tensorflow/tools/quantization:quantize_graph