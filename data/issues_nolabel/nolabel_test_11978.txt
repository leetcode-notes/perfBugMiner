tf.nn.weighted_cross_entropy_with_logits  is bad

tf.nn.weighted_cross_entropy_with_logits
(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))
where l = (1 + (q - 1) * z)
but tf.nn.sigmoid_cross_entropy_with_logits
max(x, 0) - x * z + log(1 + exp(-abs(x)))
if q=1 weighted_cross_entropy_with_logits is equvalent sigmoid_cross_entropy_with_logits
(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0)) {l=1}- (max(x, 0) - x * z + log(1 + exp(-abs(x)))) ==
x+max(-x, 0)-max(x, 0) <> 0
tf.nn.weighted_cross_entropy_with_logits must be
(- z * x) + l * (log(1 + exp(-abs(x))) + max(x, 0)