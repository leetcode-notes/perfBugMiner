Discrepancies between GPU and CPU in floating-point operations

bs = 32
dim = 1024

tf.reset_default_graph()
with tf.device("/cpu:0"):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))

tf.reset_default_graph()
with tf.device("/gpu:0"):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))

Running this graph on GPU results in positive infinities, whereas on CPU these tensor entries evaluate to ~88.72284. I could not quite figure out which operation is responsible for the difference. In both cases TensorFlow reports probs as float32. The difference does not occur when replacing probs with a tf.ones tensor in float32 format.