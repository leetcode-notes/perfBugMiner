How to quantize Mobilenet v2 ?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
TensorFlow installed from (source or binary):Source
TensorFlow version (use command below):1.8.0rc0
Python version: 2.7.12
Bazel version (if compiling from source): 0.12.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version:cuda-9.0/7.0
GPU model and memory:GeForce GTX 1080/8105MiB
Phone: xiaomi5 (Snapdragon 820)

Describe the problem
Using tf.contrib.quantize.create_training_graph() and tf.contrib.quantize.create_eval_graph(),  I
quantized training  Mobilenet v2,  and  export inference graph,  but  when toco pb to tflite,  I encountered the following error:
2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
This layer MobilenetV2/expanded_conv_2/add  cannot be quantized ?
Who can explain why ?
Source code / logs
bazel run --config=opt \

//tensorflow/contrib/lite/toco:toco -- 
--input_file=${TRAIN_DIR}/frozen_graph.pb 
--output_file=${TRAIN_DIR}/tflite_model.tflite 
--input_format=TENSORFLOW_GRAPHDEF 
--output_format=TFLITE 
--inference_type=QUANTIZED_UINT8 
--input_shape=1,224,224,3 
--input_array=input 
--output_array=MobilenetV2/Predictions/Reshape_1 
--std_value=127 
--mean_value=128 
--dump_graphviz=${TRAIN_DIR}

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/frozen_graph.pb' '--output_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/tflite_model.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=MobilenetV2/Predictions/Reshape_1' '--std_value=127' '--mean_value=128' '--dump_graphviz=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan'
2018-05-02 10:26:26.887728: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1207 operators, 1752 arrays (0 quantized)
2018-05-02 10:26:26.934465: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1207 operators, 1752 arrays (0 quantized)
2018-05-02 10:26:27.211718: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 120 operators, 228 arrays (1 quantized)
2018-05-02 10:26:27.214013: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 120 operators, 228 arrays (1 quantized)
2018-05-02 10:26:27.215023: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 67 operators, 175 arrays (1 quantized)
2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.