How to remove unwanted warning while using GPU with tensorflow

System information

I am using tensorflow version :
OS Platform and Distribution: Linux64-:
TensorFlow installed from pip version 1.4.1
Python 3.6.3 :: Anaconda custom (64-bit)
Cuda-8.0
GPU model and memory: nVIDIA K20 (Kepler)

Describe the problem
I am using Adam Optimizer with a normal sequential network created via keras using tensorflow as backend.
I get the following logs repeatedly for fitting, and creating the network. I also applied batch-normalization for the Dense Layer
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K20Xm, pci bus id: 0000:84:00.0, compute capability: 3.5
Adam_2/decay: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
IsVariableInitialized_14: (IsVariableInitialized): /job:localhost/replica:0/task:0/device:GPU:0
Adam_2/decay/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
Adam_2/decay/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
Adam_2/beta_2: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
IsVariableInitialized_13: (IsVariableInitialized): /job:localhost/replica:0/task:0/device:GPU:0
training/Adam/gradients/batch_normalization_3_1/batchnorm/mul_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
training/Adam/gradients/batch_normalization_3_1/batchnorm/mul_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
training/Adam/gradients/batch_normalization_3_1/moments/Squeeze_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
training/Adam/gradients/zeros_87/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
How to turn off these unwanted logs?
I have already applied the following solution for switching off the warning logs from the tensorflow.
https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information
Source code
from keras.layers.normalization import BatchNormalization
model = Sequential()
model.add(Dense(64, input_dim=14, init='relu'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(64, init='uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(1, init='uniform'))
model.add(BatchNormalization())
model.add(Activation('softmax'))
model.compile(loss='binary_crossentropy', optimizer=Adam())
model.fit(X_train, y_train)