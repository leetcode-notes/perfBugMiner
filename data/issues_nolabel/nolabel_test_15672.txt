matmul back-propagation quadratic memory consumption

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.3.0
Python version: 3.6.3
CUDA/cuDNN version: using CPU only

Describe the problem
tf.matmul in back-propagation has memory consumption quadratic in the number of records of data. Test case and discussion here:
https://stackoverflow.com/questions/47991747/back-propagation-exhibiting-quadratic-memory-consumption
One answer at time of writing suggests a workaround using mini-batches, appears to confirm quadratic memory consumption is not expected. I've also tried using tf.tensordot(a,b,1) which seems to be a synonym of tf.matmul(a,b) and does not use as much memory. This despite the existence of another discussion indicating tf.matmul should be more efficient:
https://stackoverflow.com/questions/43100679/tensorflow-einsum-vs-matmul-vs-tensordot