Can not import transformed and quantized model using tf.import_graph_def

Hello guys,
I am trying to quantized the pretrain SSD_mobilenet_v1_coco from the Tensorflow Object Detection API using the following command:
bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \ --in_graph=MobileNetSSD.pb \ --out_graph=optimized_SSD.pb \ --inputs='image_tensor' \ --outputs='detection_boxes,detection_scores,detection_classes,num_detections' \ --transforms=' add_default_attributes strip_unused_nodes(type=uint8, shape="1,300,300,3") fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'
The command run successfully and I had the optimized_SSD.pb with only 6MB. However, when I try to use this pb model in the provided IPython Notebook file title I received this error:
Traceback (most recent call last): File "/home/phong/PycharmProjects/ConvertDarknet2VOC/evaluation.py", line 52, in <module> tf.import_graph_def(od_graph_def, name='') File "/home/phong/.virtualenvs/tensorflow_pycharm/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py", line 283, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named QuantizedResizeBilinear in defined operations.
Is there any way to load the quantized model using tensorflow ?