Obtaining different results on declaring unused placeholders

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.7
Python version: 3.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0
GPU model and memory: GeForce GTX 1080 Ti - 12GB
Exact command to reproduce: Code link

Describe the problem
I am getting different results on declaring some unused placeholders in the code even with the same seed. After setting the same seed in both the files, the results obtained are different although the only difference in the files is an addition of some unused placeholders in placeholder_reproduce_unusedplc.py. I am unable to understand the cause of this difference because the input and the computational graph is same for both the files. Also, with different seeds, sometimes the code with additional placeholders gives worse results.
Difference between the two files (Line #52-#57 placeholder_reproduce_unusedplc.py):
X1 = tf.placeholder("float", [None, timesteps, num_input])
Y1 = tf.placeholder("float", [None, num_classes])
X2 = tf.placeholder("float", [None, timesteps, num_input])
Y2 = tf.placeholder("float", [None, num_classes])
X3 = tf.placeholder("float", [None, timesteps, num_input])
Y3 = tf.placeholder("float", [None, num_classes])
X4 = tf.placeholder("float", [None, timesteps, num_input])
Y4 = tf.placeholder("float", [None, num_classes])

Source code / logs
Running placeholder_reproduce.py gives (same everytime):

Step 1, Minibatch Loss= 2.5037, Training Accuracy= 0.164
Step 200, Minibatch Loss= 2.1293, Training Accuracy= 0.305
Step 400, Minibatch Loss= 1.8812, Training Accuracy= 0.453
Step 600, Minibatch Loss= 1.8287, Training Accuracy= 0.398
Step 800, Minibatch Loss= 1.7506, Training Accuracy= 0.430
Step 1000, Minibatch Loss= 1.6265, Training Accuracy= 0.508
Step 1200, Minibatch Loss= 1.4767, Training Accuracy= 0.516
Step 1400, Minibatch Loss= 1.4777, Training Accuracy= 0.547
Step 1600, Minibatch Loss= 1.3804, Training Accuracy= 0.586
Step 1800, Minibatch Loss= 1.2695, Training Accuracy= 0.672
Step 2000, Minibatch Loss= 1.2690, Training Accuracy= 0.594
Step 2200, Minibatch Loss= 1.2799, Training Accuracy= 0.570
Step 2400, Minibatch Loss= 1.1228, Training Accuracy= 0.680
Step 2600, Minibatch Loss= 1.0570, Training Accuracy= 0.695
Step 2800, Minibatch Loss= 1.0974, Training Accuracy= 0.625
Step 3000, Minibatch Loss= 1.1049, Training Accuracy= 0.641
Step 3200, Minibatch Loss= 1.0421, Training Accuracy= 0.688
Step 3400, Minibatch Loss= 1.0742, Training Accuracy= 0.672
Step 3600, Minibatch Loss= 1.0896, Training Accuracy= 0.648
Step 3800, Minibatch Loss= 0.8617, Training Accuracy= 0.734
Step 4000, Minibatch Loss= 0.9472, Training Accuracy= 0.719
Step 4200, Minibatch Loss= 0.7870, Training Accuracy= 0.773
Step 4400, Minibatch Loss= 0.9419, Training Accuracy= 0.656
Step 4600, Minibatch Loss= 0.7562, Training Accuracy= 0.750
Step 4800, Minibatch Loss= 0.8001, Training Accuracy= 0.758
Step 5000, Minibatch Loss= 0.9771, Training Accuracy= 0.711
Step 5200, Minibatch Loss= 0.7151, Training Accuracy= 0.805
Step 5400, Minibatch Loss= 0.7159, Training Accuracy= 0.789
Step 5600, Minibatch Loss= 0.7710, Training Accuracy= 0.797
Step 5800, Minibatch Loss= 0.7368, Training Accuracy= 0.766
Step 6000, Minibatch Loss= 0.7593, Training Accuracy= 0.758
Step 6200, Minibatch Loss= 0.6506, Training Accuracy= 0.773
Step 6400, Minibatch Loss= 0.8102, Training Accuracy= 0.766
Step 6600, Minibatch Loss= 0.5647, Training Accuracy= 0.820
Step 6800, Minibatch Loss= 0.6096, Training Accuracy= 0.828
Step 7000, Minibatch Loss= 0.6203, Training Accuracy= 0.844
Step 7200, Minibatch Loss= 0.5551, Training Accuracy= 0.820
Step 7400, Minibatch Loss= 0.5007, Training Accuracy= 0.836
Step 7600, Minibatch Loss= 0.5582, Training Accuracy= 0.844
Step 7800, Minibatch Loss= 0.6226, Training Accuracy= 0.789
Step 8000, Minibatch Loss= 0.5149, Training Accuracy= 0.812
Step 8200, Minibatch Loss= 0.5257, Training Accuracy= 0.844
Step 8400, Minibatch Loss= 0.4988, Training Accuracy= 0.844
Step 8600, Minibatch Loss= 0.5633, Training Accuracy= 0.805
Step 8800, Minibatch Loss= 0.4969, Training Accuracy= 0.797
Step 9000, Minibatch Loss= 0.4822, Training Accuracy= 0.844
Step 9200, Minibatch Loss= 0.4551, Training Accuracy= 0.844
Step 9400, Minibatch Loss= 0.4117, Training Accuracy= 0.906
Step 9600, Minibatch Loss= 0.4025, Training Accuracy= 0.883
Step 9800, Minibatch Loss= 0.5490, Training Accuracy= 0.812
Step 10000, Minibatch Loss= 0.4854, Training Accuracy= 0.836
Optimization Finished!
Testing Accuracy: 0.8515625

Running placeholder_reproduce_unusedplc.py gives (same everytime):

Step 1, Minibatch Loss= 3.9436, Training Accuracy= 0.055
Step 200, Minibatch Loss= 2.0941, Training Accuracy= 0.234
Step 400, Minibatch Loss= 1.8377, Training Accuracy= 0.422
Step 600, Minibatch Loss= 1.7426, Training Accuracy= 0.391
Step 800, Minibatch Loss= 1.7220, Training Accuracy= 0.383
Step 1000, Minibatch Loss= 1.6012, Training Accuracy= 0.469
Step 1200, Minibatch Loss= 1.4531, Training Accuracy= 0.531
Step 1400, Minibatch Loss= 1.4427, Training Accuracy= 0.516
Step 1600, Minibatch Loss= 1.2995, Training Accuracy= 0.594
Step 1800, Minibatch Loss= 1.2551, Training Accuracy= 0.664
Step 2000, Minibatch Loss= 1.2198, Training Accuracy= 0.648
Step 2200, Minibatch Loss= 1.2236, Training Accuracy= 0.609
Step 2400, Minibatch Loss= 1.0227, Training Accuracy= 0.680
Step 2600, Minibatch Loss= 0.9933, Training Accuracy= 0.695
Step 2800, Minibatch Loss= 0.9887, Training Accuracy= 0.656
Step 3000, Minibatch Loss= 1.0707, Training Accuracy= 0.578
Step 3200, Minibatch Loss= 0.9540, Training Accuracy= 0.680
Step 3400, Minibatch Loss= 0.9850, Training Accuracy= 0.695
Step 3600, Minibatch Loss= 1.0233, Training Accuracy= 0.664
Step 3800, Minibatch Loss= 0.8106, Training Accuracy= 0.727
Step 4000, Minibatch Loss= 0.8919, Training Accuracy= 0.719
Step 4200, Minibatch Loss= 0.7129, Training Accuracy= 0.773
Step 4400, Minibatch Loss= 0.8317, Training Accuracy= 0.734
Step 4600, Minibatch Loss= 0.6762, Training Accuracy= 0.789
Step 4800, Minibatch Loss= 0.6951, Training Accuracy= 0.773
Step 5000, Minibatch Loss= 0.8617, Training Accuracy= 0.727
Step 5200, Minibatch Loss= 0.6173, Training Accuracy= 0.828
Step 5400, Minibatch Loss= 0.6464, Training Accuracy= 0.797
Step 5600, Minibatch Loss= 0.6643, Training Accuracy= 0.828
Step 5800, Minibatch Loss= 0.6795, Training Accuracy= 0.758
Step 6000, Minibatch Loss= 0.6252, Training Accuracy= 0.812
Step 6200, Minibatch Loss= 0.5862, Training Accuracy= 0.812
Step 6400, Minibatch Loss= 0.7126, Training Accuracy= 0.812
Step 6600, Minibatch Loss= 0.4639, Training Accuracy= 0.891
Step 6800, Minibatch Loss= 0.5116, Training Accuracy= 0.859
Step 7000, Minibatch Loss= 0.5322, Training Accuracy= 0.852
Step 7200, Minibatch Loss= 0.4902, Training Accuracy= 0.844
Step 7400, Minibatch Loss= 0.4027, Training Accuracy= 0.867
Step 7600, Minibatch Loss= 0.4654, Training Accuracy= 0.875
Step 7800, Minibatch Loss= 0.5647, Training Accuracy= 0.812
Step 8000, Minibatch Loss= 0.4110, Training Accuracy= 0.852
Step 8200, Minibatch Loss= 0.3976, Training Accuracy= 0.844
Step 8400, Minibatch Loss= 0.3916, Training Accuracy= 0.867
Step 8600, Minibatch Loss= 0.5487, Training Accuracy= 0.797
Step 8800, Minibatch Loss= 0.4265, Training Accuracy= 0.875
Step 9000, Minibatch Loss= 0.4590, Training Accuracy= 0.859
Step 9200, Minibatch Loss= 0.4135, Training Accuracy= 0.906
Step 9400, Minibatch Loss= 0.3457, Training Accuracy= 0.914
Step 9600, Minibatch Loss= 0.4067, Training Accuracy= 0.844
Step 9800, Minibatch Loss= 0.4394, Training Accuracy= 0.859
Step 10000, Minibatch Loss= 0.3586, Training Accuracy= 0.875
Optimization Finished!
Testing Accuracy: 0.890625