HOW can I USE tf.exp(x) to back propagation Gradient?

@tf.RegisterGradient("FGGrad")
def grad_fg(op, x):
return (2*tf.exp(x))/((1+tf.exp(x))**2)
def fg(x):
with G.gradient_override_map({"Identity": "FGGrad"}):
return tf.identity(x)
The code is above .
But i made a mistake.
HOW can I USE  (2*tf.exp(x))/((1+tf.exp(x))**2) to back propagation Gradient?
(I can use tf.pow(x,2))to back propagation Gradient)