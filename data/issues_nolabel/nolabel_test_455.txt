Problem with L2_Loss on GPU

The following code works correctly when running on CPU but produces an error when run on GPU. This has been experienced with both a workstation with an Nvidia GTX 980 and a Nvidia Titan X. The error message indicates theres a problem on the CUDA Backend.

import tensorflow as tf
sess = tf.InteractiveSession()
input = tf.placeholder("float32", shape=[None, 2])
target = tf.placeholder("float32", shape=[None, 2])
w=tf.Variable(tf.zeros([2, 2]))
b=tf.Variable(tf.zeros([1]))
l=tf.nn.relu(tf.matmul(input, w) + b)
cost = tf.reduce_sum(tf.nn.l2_loss(l - target)) # this breaks
cost = tf.reduce_sum( tf.pow(l - target, 2)/2 )
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
sess.run(tf.initialize_all_variables())
dict={input: [[1,1],[1,1]], target: [[1,1],[1,1]]}
train_step.run(feed_dict=dict) # <---- this throws "F tensorflow/core/kernels/tile_ops.cc:116] TileOp: Invalid combination of Device, DT and NDIM: N5Eigen9GpuDeviceE, float, 0"