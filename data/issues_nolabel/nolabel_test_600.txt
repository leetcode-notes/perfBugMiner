Translate (Seq2Seq) Tutorial Expectations

The output I am getting from the translate.py tutorial looks corrupted.
hello -> G0
I followed the translate tutorial here:
https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html
I wanted to create a small test model, limiting to 1M records
python translate.py --data_dir data --train_dir train --en_vocab_size=40000 --fr_vocab_size=40000 --size=256 --num_layers=2 --steps_per_checkpoint=50 --max_train_data_size=1000000
No errors during the training.
After 15 hours, I stopped the training when the perplexity was below 10 points.
global step 19500 learning rate 0.2655 step-time 2.76 perplexity 8.63
eval: bucket 0 perplexity 13.15
eval: bucket 1 perplexity 10.71
eval: bucket 2 perplexity 12.78
eval: bucket 3 perplexity 14.38
After stopping the training I deleted the last corrupted training file.
rm train/translate.ckpt-19500
If I try to translate simple words I get junk.
python translate.py --decode --data_dir data --train_dir train
Created model with fresh parameters.
-> hello
G0 Processing Processing Processing Processing Processing Processing Processing Processing Processing
-> house
G0 G0 pâturage d’infrastructures d’infrastructures Twin Twin Twin Twin Twin
-> Who is the president of the United States?
expédiées expédiées expédiées m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0
Other translation technologies (es. Moses) will provide a decent translation with that training data.
If there a glitch somewhere or did I do something wrong?