Keras Dropout layer changes results with dropout=0.0

I am using the current version pypi 1.2.0, but also found this "problem" in the master I compiled about two weeks ago. I am running Gentoo linux and tensorflow is installed in an virtualenv.
Maybe I am misunderstanding the concept of a dropout layer, but when I add a Dropout-Layer with 0% dropout, it still alters my results:
from reuters classification example:
model = Sequential()
model.add(Dense(128, input_shape=(max_words,)))
model.add(Activation('relu'))
model.add(Dropout(0.0))
model.add(Dense(num_classes))
model.add(Activation('softmax'))

After 3 epochs I get:
Epoch 3/3
  32/8083 [..............................] - ETA: 0s - loss: 0.7321 - acc: 0.8125
 512/8083 [>.............................] - ETA: 0s - loss: 0.8597 - acc: 0.8145
 928/8083 [==>...........................] - ETA: 0s - loss: 0.8689 - acc: 0.8017
1344/8083 [===>..........................] - ETA: 0s - loss: 0.9041 - acc: 0.7954
1792/8083 [=====>........................] - ETA: 0s - loss: 0.8979 - acc: 0.7969
2304/8083 [=======>......................] - ETA: 0s - loss: 0.8896 - acc: 0.7969
2784/8083 [=========>....................] - ETA: 0s - loss: 0.8764 - acc: 0.7989
3168/8083 [==========>...................] - ETA: 0s - loss: 0.8678 - acc: 0.8018
3648/8083 [============>.................] - ETA: 0s - loss: 0.8666 - acc: 0.8021
4096/8083 [==============>...............] - ETA: 0s - loss: 0.8656 - acc: 0.8013
4576/8083 [===============>..............] - ETA: 0s - loss: 0.8536 - acc: 0.8018
5120/8083 [==================>...........] - ETA: 0s - loss: 0.8409 - acc: 0.8033
5664/8083 [====================>.........] - ETA: 0s - loss: 0.8369 - acc: 0.8054
6048/8083 [=====================>........] - ETA: 0s - loss: 0.8385 - acc: 0.8042
6592/8083 [=======================>......] - ETA: 0s - loss: 0.8431 - acc: 0.8025
7136/8083 [=========================>....] - ETA: 0s - loss: 0.8448 - acc: 0.8028
7680/8083 [===========================>..] - ETA: 0s - loss: 0.8490 - acc: 0.8025
8083/8083 [==============================] - 0s - loss: 0.8473 - acc: 0.8032 - val_loss: 0.9853 - val_acc: 0.7920
  32/2246 [..............................] - ETA: 0s
1568/2246 [===================>..........] - ETA: 0s
Test score: 0.934106262688
Test accuracy: 0.777382012467

and without a dropout layer:
Epoch 3/3
  32/8083 [..............................] - ETA: 0s - loss: 0.5419 - acc: 0.8750
 544/8083 [=>............................] - ETA: 0s - loss: 0.4974 - acc: 0.8842
1088/8083 [===>..........................] - ETA: 0s - loss: 0.5429 - acc: 0.8722
1664/8083 [=====>........................] - ETA: 0s - loss: 0.5568 - acc: 0.8762
2208/8083 [=======>......................] - ETA: 0s - loss: 0.5523 - acc: 0.8773
2752/8083 [=========>....................] - ETA: 0s - loss: 0.5494 - acc: 0.8790
3296/8083 [===========>..................] - ETA: 0s - loss: 0.5437 - acc: 0.8799
3808/8083 [=============>................] - ETA: 0s - loss: 0.5420 - acc: 0.8792
4352/8083 [===============>..............] - ETA: 0s - loss: 0.5446 - acc: 0.8750
4896/8083 [=================>............] - ETA: 0s - loss: 0.5405 - acc: 0.8754
5440/8083 [===================>..........] - ETA: 0s - loss: 0.5381 - acc: 0.8756
5984/8083 [=====================>........] - ETA: 0s - loss: 0.5392 - acc: 0.8755
6528/8083 [=======================>......] - ETA: 0s - loss: 0.5459 - acc: 0.8738
7104/8083 [=========================>....] - ETA: 0s - loss: 0.5482 - acc: 0.8740
7648/8083 [===========================>..] - ETA: 0s - loss: 0.5527 - acc: 0.8730
8083/8083 [==============================] - 0s - loss: 0.5525 - acc: 0.8727 - val_loss: 0.9100 - val_acc: 0.7898
  32/2246 [..............................] - ETA: 0s
1664/2246 [=====================>........] - ETA: 0s
Test score: 0.883166806993
Test accuracy: 0.792074799644

While the validation and test results are quite similar, the model without dropout overfits much more.
Also, if I set dropout to 1.0, the model should not be able to learn anything (as everything is dropped):
Epoch 3/3
  32/8083 [..............................] - ETA: 0s - loss: 0.8313 - acc: 0.7812
 576/8083 [=>............................] - ETA: 0s - loss: 0.7561 - acc: 0.8351
1120/8083 [===>..........................] - ETA: 0s - loss: 0.8669 - acc: 0.8000
1632/8083 [=====>........................] - ETA: 0s - loss: 0.8805 - acc: 0.7978
2176/8083 [=======>......................] - ETA: 0s - loss: 0.8780 - acc: 0.7973
2720/8083 [=========>....................] - ETA: 0s - loss: 0.8742 - acc: 0.7978
3264/8083 [===========>..................] - ETA: 0s - loss: 0.8693 - acc: 0.7990
3808/8083 [=============>................] - ETA: 0s - loss: 0.8616 - acc: 0.7996
4352/8083 [===============>..............] - ETA: 0s - loss: 0.8565 - acc: 0.8001
4896/8083 [=================>............] - ETA: 0s - loss: 0.8480 - acc: 0.8025
5440/8083 [===================>..........] - ETA: 0s - loss: 0.8499 - acc: 0.8007
5984/8083 [=====================>........] - ETA: 0s - loss: 0.8492 - acc: 0.8025
6560/8083 [=======================>......] - ETA: 0s - loss: 0.8498 - acc: 0.8023
7136/8083 [=========================>....] - ETA: 0s - loss: 0.8505 - acc: 0.8014
7648/8083 [===========================>..] - ETA: 0s - loss: 0.8511 - acc: 0.8010
8083/8083 [==============================] - 0s - loss: 0.8484 - acc: 0.8021 - val_loss: 0.9544 - val_acc: 0.7909
  32/2246 [..............................] - ETA: 0s
1536/2246 [===================>..........] - ETA: 0s
Test score: 0.92991881655
Test accuracy: 0.780498664292