possible bug in translate.py

Hi there,
I read translate.py (/tensorflow/models/rnn/translate/translate.py), and got confused on the implement of decoder:
step1: when decode a new  input sentence, it is ok to chose a proper bucketId to do decode
step2: tokenize the inputs(line244):
encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)
step3: model.step the same as training procedure did
My question:
The decoder_inputs are all zeros, except that the first one was assgined GO, Is this ok for a decoder? should't the LSTM's input be assgined with the previous LSTM's output, one by one?