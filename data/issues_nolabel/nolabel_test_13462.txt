Feature Request: recompute gradient with updated weights within a graph

Hi,
I wonder could there could be some new features to recompute gradients with updated weights within a graph or if there is any better way to do this. For example, for estimating hessian norm, we need to compute
delta ~ N(0, I)
hessian_norm = 1/M \sum_{1}^{M}  gradient(f(x+delta))- gradient(f(x-delta))/(2delta)
we need to gradient value on x+delta. Currently we will get None type if we use tf.gradient on var+delta directly.
Thank you very much.