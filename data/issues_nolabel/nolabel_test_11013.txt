Bug: experiment.py continuous_train_and_eval() leads to overfitting

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): v1.0.1-4-gbd6743e-dirty 1.0.1
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

Describe the problem
Our team has found what we believe is a critical bug in experiment.py continuous_train_and_eval() that pretty much guarantees any model trained with this method will lead to overfitting.
The problem is that every time it alternates from evaluating to training again, it calls self._call_train(), which eventually makes it to estimator.py _train_model().
This function first creates a brand new graph then sets up the input pipelines. This necessarily means that the input pipelines are reset from scratch on every training lap, and since a training lap is typically much shorter than an epoch, this means the model is only trained with a small portion of the whole dataset.
Originally we raised this as an issue in Google's tf-seq2seq, but as far as we can tell now the bug probably belongs into Tensorflow instead, as we can't see a simple way to modify the tf-seq2seq code to avoid this issue from arising.
Thank you for looking into this matter.
Source code / logs
N/A