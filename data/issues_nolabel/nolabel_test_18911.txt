Check failed: dtype() == expected_dtype (9 vs. 3)

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source (Today's version)
TensorFlow version (use command below): v1.8.0-rc1-1107-g8a428cd 1.8.0-rc1
Python version: Python3
Bazel version (if compiling from source): 0.12
GCC/Compiler version (if compiling from source): 5.4
CUDA/cuDNN version: 9.1/7.1.3
GPU model and memory: Geforce gtx Titan X
Exact command to reproduce:

 std::vector<tensorflow::Tensor> finalOutput;

  std::string InputName  = "inp";
  std::string OutputName = "out";
  tensorflow::Status run_status =
      session->Run({{InputName, input_tensor}}, {OutputName}, {}, &finalOutput);

  for (int y = 0; y < height; y++)
  {
    for (int x = 0; x < width; x++)
    {
     std::cout << finalOutput[0].tensor<int, 4>()(0, y, x, 0); // Error: Check failed: dtype() == expected_dtype (9 vs. 3)
    }
  }

Describe the problem
I have trained network in python. I froze the model from python and writing inference model in c++. I am using above code to run the inference on frozen graph. It works as I got the tensor output but I am unable to read this tensor file by using above method.
Based on types.proto, I am comparing DT_INT64(frozen model) with DT_INT32(c++ inference model).
Things I have tried:

Specify tf.int32 in argmax layer. (Last layer in frozen model) -> It works (But I don't want to modify the network architecture)
Instead of  finalOutput[0].tensor<int, 4>, I have tried  finalOutput[0].tensor<long int, 4>,  finalOutput[0].tensor<int64_t, 4> but compilation issue as eigen might not be supporting this. (Just a guess)

Source code / logs
Error: Check failed: dtype() == expected_dtype (9 vs. 3)