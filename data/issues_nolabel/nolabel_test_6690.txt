Calculation Delta, Memoization, Update Propagation and Laziness

If you just update a few element in the data structure then having to do calculations which are not affected by the change is a waste.  Also if you read only or consume un affected values then the update and calculations themselves are a waste. Also eagerly calculating this would be a waste since I might only read the result after few updates. In addition you can avoid calculating invariant items. Perhaps XLA can get Lightweight Modular Staging (LMS) functionality to do this type of analysis and optimisation.
Also see: #6690

For example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window.

Also other potential optimisations (will require some research):

If I have a matrix inverse and I update the original matrix, what would be the optimal calculation to arrive at an updated inverse matrix
Similarly for matrix multiplication and other operations.