About the implementation of recently added attentional seq2seq decoder function

Hello, I heard a good news that the attentional seq2seq decoder function is recently added in the master branch.
It is a very pleasing news to me, since nowadays I have been struggling to implement the feature.
However, while reviewing the detailed implementation, I found some unclear points.
Firstly, in line 395-397 of attention_decoder_fn.py,
attention_query and context are concatenated and passed to a fully connected layer to be projected to a vector whose size is num_units.
As far as I know, the current implementations of RNN cells already do this procedure.
For example, the current implementation of LSTMCell (line 337-338 in core_rnn_cell_impl.py) concatenates inputs and m_prev, and multiplies the concatenated vector by (input_dim + num_units, 4 * num_units) weight matrix.
Thus, to avoid multiplying inputs with weight matrices two times, I think line 396-397 should be removed.
Reading codes again, I found I understood them wrongly.
And, I also think concat_v2 function should be used to concatenate two vectors.
Secondly, in line 122-123 and 288-289, I see that cell_output is used as attention_query parameter of attention_construct_fn.
This is correct if we choose 'Luong-style' alignment calculation, where the computation path is "h_t -> a_t -> c_t -> tilde h_t", however this seems to be problematic if we choose 'Bahdanau-style' alignment calculation, where the computation path is "h_(t-1) -> a_t -> c_t -> h_t".
For now I can't think of a simple way to correct this behavior since the style of attention mechanism is unknown in decoder_fn's scope, but I think it should be dealt with anyway.
Please tell me if I read the codes wrongly! :)
I am a bit confused with the exact meaning of luong and bahdanau method. The both options use h_(t-1), unlike the referencing paper.
I close this, and will make a new issue after organizing my brain!