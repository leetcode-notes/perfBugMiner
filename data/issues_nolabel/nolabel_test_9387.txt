Wrong implementation of ResNet in tensorflow/tensorflow/contrib/slim/python/slim/nets/ ?

I'm a researcher working on models related to ResNet_v1. As I was trying to use the code in this reporsitory: tensorflow/tensorflow/contrib/slim/python/slim/nets/
I realized that the ResNet_v1 model is different from Kaiming He's implementation. Specifically, in Kaiming's implementation, he applied stride 2 at the START of each block and the stride is applied to the 1*1 conv layer. For example, in Kaiming's original caffe prototxt (https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-101-deploy.prototxt), this is the first layer of 'conv3' (named in his ResNet paper) or block 2 (named by your implementation):
layer {
	bottom: "res2c"
	top: "res3a_branch1"
	name: "res3a_branch1"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 1
		pad: 0
		stride: 2
		bias_term: false
	}
}

This layer is a 1*1 conv layer that mimics the 'skip-connection' because of dimension mismatch at the bottleneck between blocks. And so does 'res3a_branch2a', which is the start of the first residual module in 'conv3'.
Instead, in the tensorflow implementation, the stride 2 is applied to the END of each block(see function resnet_v1_block() in resnet_v1.py) and the stride is applied to the 3*3 conv layer of the last Residual module in each block(see function bottleneck() in resnet_v1.py).
It's obvious that these two are different. Can anybody explain if it is wrong or implemented as such for some reasons?