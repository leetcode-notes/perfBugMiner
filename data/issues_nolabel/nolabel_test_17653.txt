Tensorflow distributed training: CPU usage difference

Hi,
Good day. I have a question that I hope someone could help.
Please forgive my grammar errors and missing words.
I have tried to combine the Cifar10 training examples
https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10
and  example in Distributed TensorFlow website:
https://www.tensorflow.org/deploy/distributed
to do the cifar10 distributed training.
I am a little bit confused on the tf.train.MonitoredTrainingSession implementation.
Servers are ok to communicate with each other.
System information

Linux Ubuntu 16.04:
Python3
CPU

Describe the problem
I used two servers, each of them are used as 1 PS and 1 Worker and no other programs. So total 2 PS and 2 Workers are trained in this experiment. What I expected is 2 Workers have the same CPU percentage usage when the training is going on. But I found that only one of the server ( with PS 1 and Worker 1) was running with 100% CPU usage, the other server( with PS 2 and Worker 2) only used 3% CPU.
Source code / logs
The commands I used are
python3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=ps --task_index=0
python3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=ps --task_index=1
python3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=worker --task_index=0
python3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=worker --task_index=1
The f.train.MonitoredTrainingSession is set as:
with tf.train.MonitoredTrainingSession(master=server.target,
is_chief=(FLAGS.task_index == 0),
checkpoint_dir="/tmp/train_logs",
hooks=hooks) as mon_sess:
while not mon_sess.should_stop():
mon_sess.run(train_op)