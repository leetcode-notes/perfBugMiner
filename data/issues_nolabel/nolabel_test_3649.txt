Boolean operations on GPU are extremely slow

ArchLinux. Cuda7.5. NIghtly TF built for Python2.
import tensorflow as tf
import time

v = tf.get_variable('test', shape=[100, 100, 100])
vb = tf.get_variable('test2', shape=[100, 100, 100], dtype=tf.bool,
        initializer=tf.constant_initializer(False))

b1 = tf.reduce_sum(v)
b2 = tf.reduce_all(vb)
b3 = tf.reduce_all(tf.cast(v, tf.bool))

sess = tf.Session()
sess.run(tf.initialize_all_variables())
with sess.as_default():
    start = time.time()
    for k in range(100):
        sess.run(b1)
    print time.time() - start   # 0.02s

    start = time.time()
    for k in range(100):
        sess.run(b2)
    print time.time() - start   # 7s!

    start = time.time()
    for k in range(100):
        sess.run(b3)
    print time.time() - start   # 17s!
CPU version of the same operation is also much faster than this.