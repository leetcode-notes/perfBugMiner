pywrap_tensorflow.list_devices() allocates all available memory (on all GPU devices)

Environment info
Operating System:
Ubuntu 14.04.5 LTS
Python 3.4.3
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
libcudadevrt.a  libcudart.so.8.0     libcudart_static.a  libcudnn.so.5      libcudnn_static.a
libcudart.so    libcudart.so.8.0.61  libcudnn.so         libcudnn.so.5.1.5
If installed from binary pip package, provide:

A link to the pip package you installed:
pip install tensorflow-gpu
The output from python -c "import tensorflow; print(tensorflow.__version__)".

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
from tensorflow.python import pywrap_tensorflow
pywrap_tensorflow.list_devices()

I have tried this locally on a system with two Quadro K620 and another system with one TITAN X.