Tensorflow Conv model crashes on GPU with zero size batch

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, I have created two CNN models
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): I installed tensorflow via pip install (Python 3)
TensorFlow version (use command below): 1.4.0
Python version: 3
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 8.0/6
GPU model and memory: NVIDIA Tesla k80 totalMemory: 11.17GiB freeMemory: 11.09GiB
Exact command to reproduce: See below:

Below is the log:

keep_dims is deprecated, use keepdims instead

Layer (type)                 Output Shape              Param #
input_1 (InputLayer)         (None, 7, 264)            0

reshape_1 (Reshape)          (None, 7, 264, 1)         0

conv2d_1 (Conv2D)            (None, 7, 264, 64)        16960

max_pooling2d_1 (MaxPooling2 (None, 7, 132, 64)        0

flatten_1 (Flatten)          (None, 59136)             0

dense_1 (Dense)              (None, 1024)              60556288

dropout_1 (Dropout)          (None, 1024)              0

dense_2 (Dense)              (None, 512)               524800

dropout_2 (Dropout)          (None, 512)               0

dense_3 (Dense)              (None, 88)                45144
Total params: 61,143,192
Trainable params: 61,143,192
Non-trainable params: 0

/home/hpnhxxwn/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:2057: UserWarning: Using a generator with use_multiprocessing=True and multiple worker
s may duplicate your data. Please consider using thekeras.utils.Sequence class. UserWarning('Using a generator withuse_multiprocessing=True`'
ld: learning rate is now 0.01
2017-11-29 04:58:20.964015: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX A
VX2 FMA
2017-11-29 04:58:21.094051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUM
A node, so returning NUMA node zero
2017-11-29 04:58:21.094719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1062] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2017-11-29 04:58:21.094744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1152] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0
, compute capability: 3.7)
Epoch 1/1000
5180/6944 [=====================>........] - ETA: 3:43 - loss: 0.1382 - acc: 0.9637 - mean_absolute_error: 0.0715 - sparse_categorical_accuracy: 7.1640e-05switching to  ['AkPnStgb']
5212/6944 [=====================>........] - ETA: 3:39 - loss: 0.1384 - acc: 0.9636 - mean_absolute_error: 0.0715 - sparse_categorical_accuracy: 7.1200e-052017-11-29 05:09:22.210897: F
tensorflow/stream_executor/cuda/cuda_dnn.cc:444] could not convert BatchDescriptor {count: 0 feature_map_count: 64 spatial: 7 264  value_min: 0.000000 value_max: 0.000000 layout: Batc
hDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM

Below is the model.
def baseline_model():
    inputs = Input(shape=input_shape)
    reshape = Reshape(input_shape_channels)(inputs)
    #normal convnet layer (have to do one initially to get 64 channels)
    conv1 = Conv2D(50,(5,25),activation='tanh')(reshape)
    do1 = Dropout(0.5)(conv1)
    pool1 = MaxPooling2D(pool_size=(1,3))(do1)
    conv2 = Conv2D(50,(3,5),activation='tanh')(pool1)
    do2 = Dropout(0.5)(conv2)
    pool2 = MaxPooling2D(pool_size=(1,3))(do2)
    flattened = Flatten()(pool2)
    fc1 = Dense(1000, activation='sigmoid')(flattened)
    do3 = Dropout(0.5)(fc1)
    fc2 = Dense(200, activation='sigmoid')(do3)
    do4 = Dropout(0.5)(fc2)
    outputs = Dense(note_range, activation='sigmoid')(do4)
    model = Model(inputs=inputs, outputs=outputs)
    return model

I have found other github issue created for the same issue, not so far seems like there is no fix yet. I heard the workaround is to use tf.cond, can someone show me how to use it in such case.