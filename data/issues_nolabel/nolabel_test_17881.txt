TensortRT Invalid data type: 'int32' when converting TF Object Detection graph

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes see gist (change line 6 to object detection frozen inference graph path): https://gist.github.com/louisquinn/0c6729a32e87e899ece317de84d02acc
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.7
Python version: 3.5
Bazel version (if compiling from source): 0.11.1
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: CUDA 9.0 / cuDNN 7.1
GPU model and memory: GTX 1080 8GB
Exact command to reproduce: See gist (change line 6 to object detection frozen inference graph path): https://gist.github.com/louisquinn/0c6729a32e87e899ece317de84d02acc

Describe the problem
Getting the following error when calling trt.create_inference_graph (see line 18 of gist):
tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2503] Non-OK-status: ConvertDType(tf_dtype, &dtype) status: Invalid argument: Unsupported data type int32
Is int32 not supported by TensorRT, not yet implemented in the Tensorflow wrap, or am I doing something incorrect? This happens with all pre-trained TF Object Detection models.