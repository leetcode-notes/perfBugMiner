Graph optimization and other features

I've been using Theano for about 4 years now and love its flexibility due to the many available low-level ops that allow me to implement complicated and possibly non-standard models without needing to write C++/CUDA code (most of the time). In addition, I can focus almost exclusively on the model design and don't need to think much about numerical stability or suboptimal graph design leading to increased execution times because the graph optimization framework takes care of this. One common example is computing the log-loss of a categorical classifier which I can express na√Øvely in Theano, but in most (all?) other tools including TensorFlow I need to use an op like tf.nn.softmax_cross_entropy_with_logits. In my opinion, the fact that I need to call these kinds of specialized ops manually, i.e. I need to know about them and think about when and how to use them in all kinds of situations, takes away many of the advantages of TensorFlow. Similarly, implementing model optimizers in C++/CUDA directly introduces unnecessary implementation complexity and does not take advantage of the fact that (most likely) all mathematical ops needed to specify the update formulas are already available (CPU and GPU kernels).
I believe TensorFlow has a superior framework design in terms of keeping C++ and Python clearly separated and the inherent multi-device computing capabilities, but the aspects I described above make me very hesitant to use TensorFlow. Are there any plans to add a proper graph optimization framework similar to the one in Theano? And what are your reasons for implementing the model optimizers as individual kernels instead of reusing ops (like using the updates-dictionary in Theano)?