Distributed tensorflow hangs

I set up a distributed tensorflow according to the inception example(https://github.com/tensorflow/models/tree/master/inception/inception), with a very deep network.
During training(minutes or hours), it must run into hanging. No error occurs in ps or workers, but the cpu/gpu usage falls to zero and no any further steps are made.
If I replace the network with a much simpler one(much less variables), all things work fine.
Is there any limit on the amount of vars placed on ps? or any suggestions to deal with the problem?
Setup: tensorflow r0.10 build on cuda 7.5 and cudnn v5