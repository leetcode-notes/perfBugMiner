CUDNN rnn error -Failed to call ThenRnnForward

OS Platform and Distribution - Ubuntu 16.04
TensorFlow installed from TensorFlow version -1.4 from
Bazel Version 0.6.1
CUDA Version 9.0.176
Machine Type -n1-standard-32 (32 vCPUs, 120 GB memory)
GPU - 4 x NVIDIA Tesla P100
I am using a cudnnrnnrelu like this ::
with tf.variable_scope('cudnn_rnn_stack', reuse = reuse) as scope:
rnn = tf.contrib.cudnn_rnn.CudnnRNNRelu(5,n_hidden,"linear_input", "bidirectional")
output, _ = rnn(tf.transpose(layer_1,[1,0,2]), training=True)
output_rnn_stack = tf.concat(output, 2)
Initially the epochs were running fine. But I encountered this error after 2-3 epochs:
InternalError (see above for traceback): Failed to call ThenRnnForward
[[Node: tower_0/cudnn_rnn_stack/cudnn_rnn_relu/CudnnRNN = CudnnRNN[T=DT_FLOAT, direc
tion="bidirectional", dropout=0, input_mode="linear_input", is_training=true, rnn_mode="rnn_r
elu", seed=0, seed2=0, device="/job:localhost/replica:0/task:0/device:GPU:0"](tower_0/cudnn
rnn_stack/transpose, tower_0/cudnn_rnn_stack/cudnn_rnn_relu/zeros, tower_0/cudnn_rnn_stack/cu
dnn_rnn_relu/Const, cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/read)]]
[[Node: Adam/update_cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/ApplyAdam/_870 = _R
ecvclient_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send
_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_nam
e="edge_2200_Adam/update_cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/ApplyAdam", tensor_type
=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]]
Running it on a distributed gpu setting in gcp instances