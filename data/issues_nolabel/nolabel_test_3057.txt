Use gpu_memory_fraction in while using distributed tensorflow

While I'm implementing models in A3C, I tried to allocate a fraction of gpu to a process and processes that use multiple fractions of gpus update a parameters of a parameter server (in a single machine). For example, I want to create 12 workers with 3 gpus to update a master model.
I referenced https://www.tensorflow.org/versions/r0.9/how_tos/distributed/index.html and used tf.GPUOptions(per_process_gpu_memory_fraction=0.1) but it doesn't work when we pass it to sv.managed_session (just take all of the memory of the first visible gpu).
Also, I can't find how to give GPUOptions to parameter server that does not have session creation (I always pass GPUoptions to tf.Session which similar to sv.managed_session). How can I allocate a specific fraction of a gpu to each tasks including parameter server and workers?
Code can be found https://github.com/devsisters/DQN-tensorflow/blob/distributed/main.py#L78.