multiple GPU training out of memory

When I run the cifar10_mutil_gpu_train.py example with 2 gpus, I got error message like the below, does this mean there is out of memory problem? How to solve or work around this?
My GPU is K80 so there are two K40 actually.
PoolAllocator: After 2573 get requests, put_count=2268 evicted_count=1000 eviction_rate=0.440917 and unsatisfied allocation rate=0.546055
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110