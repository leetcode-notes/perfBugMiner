tf.Session.list_devices seems to return an (int64 *) for device.memory_limit_bytes and leaks

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

no

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Linux Ubuntu 16.04

TensorFlow installed from (source or binary):

binary 1.3.0 GPU

TensorFlow version (use command below):

('v1.3.0-rc2-20-g0787eee', '1.3.0')

Python version:

Python 2.7.12

Bazel version (if compiling from source):

N/A

CUDA/cuDNN version:

CUDA 8.0 + cuDNN 6.0

GPU model and memory:

GeForce GTX 960M

Exact command to reproduce:

import tensorflow as tf

with tf.Session() as S:
    for d in S.list_devices():
        print d.name, d.device_type, d.memory_limit_bytes
produces:
2017-10-04 10:31:04.370386: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370409: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370415: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370419: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.566504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-04 10:31:04.566814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 960M
major: 5 minor: 0 memoryClockRate (GHz) 1.176
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.92GiB
2017-10-04 10:31:04.566862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-10-04 10:31:04.566868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-10-04 10:31:04.566875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)
/job:localhost/replica:0/task:0/device:CPU:0 CPU <Swig Object of type 'int64_t *' at 0x7f5c22f61a50>
/job:localhost/replica:0/task:0/device:GPU:0 GPU <Swig Object of type 'int64_t *' at 0x7f5c22f613f0>
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
Describe the problem
The memory_limit_bytes attribute seems to be wrapping an int64 * pointer (as opposed to a int64) and swig seems to think it's leaking this pointer. Also, in the docs its referred to as memory_limit
Source code / logs
N/A