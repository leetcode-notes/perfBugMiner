A Strange Behavior in TensorFlow about Conv2d Creation and Weight Optimization

To simplify the tensorflow syntax, I created utility functions that would generate a conv2d layer more easily for me. However, when I use it in an architecture, the results are bizzare and different from the vanilla coding version. Even though both methods are returning the same tensor shape, tensorflow fails to optimize the weights in the second approach and I always get extremely low accuracies, vs the first version returns a perfect accuracy.
It seems like every time Tensorflow initializes the weights randomly instead of optimizing it. Is this a known bug?
Version 1
sigma = 0.1
x = tf.placeholder(tf.float32, (None, 32, 32, 3))
conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))
conv1_b = tf.Variable(tf.zeros(6))
conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b
Version 2
def weights(dims, mu = 0, sigma = 0.1):
    w = tf.Variable(tf.truncated_normal(shape = dims, mean = mu, stddev = sigma))
    return w
def bias(dims):
    b = tf.Variable(tf.zeros(dims))
    return b
def conv(layer, dims, stride = [1, 1, 1, 1], pad = 'VALID'):
    w = weights(dims)
    b = bias(dims[-1])
    conv2 = tf.nn.conv2d(layer, w, strides = stride, padding = pad) + b
    return conv2
this line is the only line that I put in my network architecture: so I created my conv2d layers like this:
conv2 = conv(x, [5, 5, 3, 6])