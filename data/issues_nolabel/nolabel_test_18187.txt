Adam optimizer with decaying learning rate

I tried to implement the Adam optimizer with different beta1 and beta2 to observe the decaying learning rate changes using:
optimizer_obj = tf.train.optimizer(learning_rate=0.001, beta1=0.3, beta2=0.7)

To track the changes in learning rate, I printed the _lr_t variable of the object in the session:
print(sess.run(optimizer_obj._lr_t))

But for all iterations, I get the same value (the initialized 0.001 value).
I couldn't find how the parameters are updated in this class.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.5
Python version: 3.5
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce:

optimizer_obj = tf.train.optimizer(learning_rate=0.001, beta1=0.3, beta2=0.7)
with tf.Session() as sess:
    print(sess.run(optimizer_obj._lr_t))