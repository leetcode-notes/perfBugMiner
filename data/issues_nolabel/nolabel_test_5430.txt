RNN AutoEncoder

Hello, I am writting implementation for RNN autoencoder. The decoder layer is trying to predict next word.
My code looks like this:
class RnnAutoencoder():
   def __init__(...):
   ...
   # Encoder
   self.z_codes, self.enc_state = tf.nn.dynamic_rnn(self._enc_cell, inputs, dtype=tf.float32)

   # Intermediate layer after encoder
   last_output = self.z_codes[-1]
   self.pred_m = tf.nn.relu(tf.matmul(last_output, self.weight_m) + self.bias_m)

   # Decoder
   dec_input = self.pred_m
   dec_state = self.enc_state

   dec_outputs = []
   for step in range(len(inputs)):
      if step>0: vs.reuse_variables()
      dec_input_, dec_state = self._dec_cell(dec_input_, dec_state)
      dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_
      dec_outputs.append(dec_input_)


inputs = tf.placeholder(tf.float32, [1, None, 300])


But I get error:
TypeError: object of type 'Tensor' has no len()
on line:     for step in range(len(inputs)):
I know that i cannot apply len on Tensor in tensorflow but how can I iterate(for variable time steps) and do it the way that next input of decoder is output from previous step(of the same decoder). Basic tf.nn.dynamic_rnn doesn't work with this situation.
I read many implementations of lstm, rnn, auencoders on github but it is always with fixed size sequences or the implementation of decoder which doesn't get input which is output from previous step. I read seq2seq tutorial but it doesn't use the intermediate layer for encoding sequences as I need in my model and also i am not sure if it can handle dynamic size of sequences.