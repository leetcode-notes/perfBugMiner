TensorFlow Lite for Inception v3 - Squeeze operator isn't supported

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No, I haven't used any custom code for this.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
r1.5 (6c5063a)
Python version:
3
Bazel version (if compiling from source):
0.9.0
GCC/Compiler version (if compiling from source):
gcc4.4
CUDA/cuDNN version:
9.0/7.0
GPU model and memory:
GTX 1080 8GB
Exact command to reproduce:

bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=inception-retrained-graph_freezed.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=inception-retrained-graph.tflite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --output_arrays=InceptionV3/Predictions/Reshape_1 --input_shapes=1,299,299,3

Describe the problem
I tried to use the above command to convert a retrained inception v3 model into the new tflite format. The input graph has already been freezed. The general process is described in this tutorial. However, I'm not able to finish the conversion as the tool complains that the Squeeze operator isn't supported. According to this guide the script should be able to remove the squeeze operation. This comment suggests that it may not always be possible to remove it. The inception v3 architecture is guaranteed to work though. Am I safe using the --allow_custom_ops flag?
Some final note:
I retrained the inception net using the slim model rather than just retraining the last layer using the freezed graph version. May this be the cause of the additional squeeze problem?
Source code / logs
This is the complete log as returned by the toco script:
W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
2018-02-07 01:23:50.754337: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1074 operators, 1657 arrays (0 quantized)
2018-02-07 01:23:50.826283: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 126 operators, 317 arrays (0 quantized)
2018-02-07 01:23:50.827683: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 126 operators, 317 arrays (0 quantized)
2018-02-07 01:23:50.828908: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 11139584 bytes, theoretical optimal value: 8297856 bytes.
2018-02-07 01:23:50.829206: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 11.4574 billion (note that a multiply-add is counted as 2 ops).
2018-02-07 01:23:50.829493: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Squeeze.
[1]    16187 abort (core dumped)  bazel-bin/tensorflow/contrib/lite/toco/toco   --output_format=TFLITE