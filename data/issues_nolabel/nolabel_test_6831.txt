running session multiple times with tf.random returns different values for conv2d

import tensorflow as tf 
import numpy as np 

x_tf = tf.placeholder('float',[None, 2, 5, 1])
x_np = np.random.noraml(0,1,[1,2,5,1])

# ======== filter option1 and option2 ===========
f_np = np.random.normal(0,1,[1,3,1,1])
f_tf = tf.constant(f_np,'float') # option 1
f_tf = tf.random_normal([1,3,1,1]) # option 2
# ===============================================

x_conv = tf.nn.conv2d(x_tf,f_tf,[1,1,1,1],'SAME')

with tf.Session() as sess:
     tf.gloval_variables_initializer().run()
     x_conv_np  = sess.run(x_conv, feed_dict={x_tf, x_np})
     x_conv_np2 = sess.run(x_conv, feed_dict={x_tf, x_np})

If I run the code above with option1, I get the same values for x_conv_np and x_conv_np2
However, when I run the above with option2, I get different values for x_conv_np and x_conv_np2.
I am guessing the tf.random_normal gets initialized every time the session is ran.
Is this meant to happen or is it a bug?
I found this out while I was trying to debug my code, and it took me a while to figure out the issue.
Doesn't this mean that when option2 is chosen, the result of the convolution changes for each loop that iterates over mini batches?