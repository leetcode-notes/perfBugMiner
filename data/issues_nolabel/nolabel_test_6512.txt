The meaning of batch_size in ptb_word_lm

I am new to tensorflow, i am now a little confused about the meaning of batch_size. As commonly known that the meaning of batch_size is the number of samples for each batch, but according to the code in ptb_word_lm, it seems not:
reader.py
data_len = tf.size(raw_data)  #the number of words in dataset
batch_len = data_len // batch_size   #what does batch_len mean? the number of batchs?does not make sense
ptb_word_lm.py
self.epoch_size = ((len(data) // batch_size) - 1) // num_steps   #what does epoch_size mean? the number of sequence in each batch?does not make sense
But if batch_size means the number of batches, then everything make sense. have i got something misunderstood?