Getting Attention Activations to Visualize Attention in Seq2Seq

All attention papers feature some visualization of the attention weights on some input. Has anyone been able to run a sample through the Seq2Seq Attention Decoder model in translate.py and get the attention activations to do such a visualization?