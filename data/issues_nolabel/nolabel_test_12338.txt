Tensorflow crashes when i try to quantize my output

BUG
(https://github.com/tensorflow/tensorboard/issues).
Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

$ lspci -vvv|grep -i nvi
00:1e.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1) (prog-if 00 [VGA controller])
        Subsystem: NVIDIA Corporation GM204GL [Tesla M60]
        Kernel driver in use: nvidia
        Kernel modules: nvidia_375_drm, nvidia_375
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the problem
When I tried to quantize my output


       result = SESS.run(layers[0], {'incept/DecodeJpeg/contents:0': f.read()})





       result = SESS.run(tf.quantize_v2(layers[0], min_range=0, max_range=10, T=tf.quint8), {'incept/DecodeJpeg/contents:0': f.read()})



I run out of memory on my GPU
2017-08-16 20:59:40.313241: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ***********************
*****************************************************************************
2017-08-16 20:59:40.313284: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when al
locating tensor with shape[720,1280,3]
Traceback (most recent call last):
  File "generate_hashes.py", line 89, in <module>
    for (fn, points) in map_results.result():
  File "/usr/local/lib/python2.7/dist-packages/pebble/pool/base_pool.py", line 208, in next
    raise result
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[720,
1280,3]
         [[Node: incept/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, _device="/job:unknown_job/replica:0/task
:0/gpu:0"](incept/DecodeJpeg_G511)]]
         [[Node: incept/pool_3_G513 = _Recv[client_terminated=false, recv_device="/job:unknown_job/replica
:0/task:0/cpu:0", send_device="/job:unknown_job/replica:0/task:0/gpu:0", send_device_incarnation=-69665892
98495454362, tensor_name="edge_1131_incept/pool_3", tensor_type=DT_FLOAT, _device="/job:unknown_job/replic
a:0/task:0/cpu:0"]()]]

Caused by op u'incept/Cast', defined at:
  File "generate_hashes.py", line 78, in <module>
    load_network(False)
  File "generate_hashes.py", line 40, in load_network
    return tf.import_graph_def(graph_def, name='incept')
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py", line 311, in impo
rt_graph_def
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2506, in create_o
p
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1269, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[720,1280,3]
         [[Node: incept/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, _device="/job:unknown_job/replica:0/task
:0/gpu:0"](incept/DecodeJpeg_G511)]]
         [[Node: incept/pool_3_G513 = _Recv[client_terminated=false, recv_device="/job:unknown_job/replica
:0/task:0/cpu:0", send_device="/job:unknown_job/replica:0/task:0/gpu:0", send_device_incarnation=-69665892
98495454362, tensor_name="edge_1131_incept/pool_3", tensor_type=DT_FLOAT, _device="/job:unknown_job/replic
a:0/task:0/cpu:0"]()]]