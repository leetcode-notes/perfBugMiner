add AdaMax optimizer

Fix #17104. The PR implements the AdaMax optimizer: https://arxiv.org/pdf/1412.6980v8.pdf
Please pay attention to three points when reviewing:

To avoid division by zero, we add epsilon for v_t, which is slightly different from original paper section 7.1. And we explained the change in the document.
Contrast to the sparse implementation of tf.train.AdamOptimizer, the AdaMaxOptimizer only updates variable slices and corresponding m_t, v_t terms when that part of the variable was used in the forward pass.
Because AdaMax is quite similar with Adam, we refactor the Adam code in c++ side to reuse its route. Not sure whether the solution sounds good.

The PR is still very rudimentary implementation, any feedback/review would be appreciated.