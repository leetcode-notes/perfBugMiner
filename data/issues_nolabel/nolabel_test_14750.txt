Test case with multiple threads do not work in TF-1.4

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
RedHat7.2
TensorFlow installed from (source or binary):
tensorflow_gpu binary
TensorFlow version (use command below):
r1.4
Python version:
python 2.7
CUDA/cuDNN version:
CUDA-8.0 CUDNN-6.0
GPU model and memory:
NVIDIA-K80
Exact command to reproduce:
python tensorflow/python/training/sync_replicas_optimizer_test.py

Describe the problem
When I start a test case for sync optimizer, it always shows that ps:0,ps:1,worker:1 are not ready.
It seems this only happens in TF-1.4
Source code / logs
Source Code:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer_test.py
Logs:
2017-11-21 18:37:58.627374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:                                                                                                                [6/1673]
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:06:00.0
totalMemory: 11.17GiB freeMemory: 11.11GiB
2017-11-21 18:37:58.874015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:07:00.0
totalMemory: 11.17GiB freeMemory: 11.11GiB
2017-11-21 18:37:58.874455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix
2017-11-21 18:37:58.874488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1
2017-11-21 18:37:58.874499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y
2017-11-21 18:37:58.874505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y
2017-11-21 18:37:58.874523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)
2017-11-21 18:37:58.874532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
E1121 18:37:59.075062187   32596 ev_epoll1_linux.c:1051]     grpc epoll fd: 31
2017-11-21 18:37:59.081712: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}
2017-11-21 18:37:59.081747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}
2017-11-21 18:37:59.083912: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:15405
2017-11-21 18:37:59.084202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)
2017-11-21 18:37:59.084222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
2017-11-21 18:37:59.090916: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}
2017-11-21 18:37:59.090943: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}
2017-11-21 18:37:59.091077: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17501
2017-11-21 18:37:59.091268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)
2017-11-21 18:37:59.091286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
2017-11-21 18:37:59.097254: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}
2017-11-21 18:37:59.097277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}
2017-11-21 18:37:59.097399: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17358
2017-11-21 18:37:59.097548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)
2017-11-21 18:37:59.097566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
2017-11-21 18:37:59.104080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}
2017-11-21 18:37:59.104127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}
2017-11-21 18:37:59.104281: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:24102
2017-11-21 18:38:09.222879: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2017-11-21 18:38:09.222931: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1
2017-11-21 18:38:09.222941: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1
2017-11-21 18:38:19.223062: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2017-11-21 18:38:19.223102: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1
2017-11-21 18:38:19.223110: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1
2017-11-21 18:38:29.223253: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2017-11-21 18:38:29.223284: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1
2017-11-21 18:38:29.223291: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1
2017-11-21 18:38:39.223379: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2017-11-21 18:38:39.224059: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1
2017-11-21 18:38:39.224068: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1