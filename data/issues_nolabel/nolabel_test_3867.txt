parallel_iterations option behaves differently if per_process_gpu_memory_fraction is set

I'm seeing inconsistent behavior with the parallel_iterations option in dynamic RNNs. For large RNNs, if I set parallel_iterations to too high a number, I run out of memory. However, setting parallel_iterations to 1 or 2 solves the problem. This is only true however if per_process_gpu_memory_fraction is not set. If it is set, even to 1.0, then no matter what value I set parallel_iterations to, I always run out of memory (for this large RNN).
I am not sure if this is related to #2610 or not.