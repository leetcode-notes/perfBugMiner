Feature Request: Locally Connected Conv2d

Describe the problem
Need new features, the local conv is avaiable in Keras.
https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D
But tensorflow does not have this feature. I have writen this layer with tensorlayer, it can work, but the initializing is too slow. So I want offical to release a better implementation. Thanks
Source code / logs
class LocalConnectedConv(Layer):
    def __init__(
        self,
        layer = None,
        filters=0,
        size=3,
        multiplexH=1,
        multiplexW=1,
        stride=1,
        overlap=True,
        act = tf.identity,
        name ='lcconv2d',
    ):
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        channels = int(self.inputs.get_shape()[-1])
        inputH=int(self.inputs.get_shape()[1])
        inputW = int(self.inputs.get_shape()[2])
        if inputH%multiplexH==0 and inputW%multiplexW==0:
            print('ok')
        else:
            return 0
        CellH= int(np.floor(inputH/multiplexH))
        CellW=int(np.floor(inputW/multiplexW))
        CellHm= int(np.floor(inputH/multiplexH))
        CellWm=int(np.floor(inputW/multiplexW))
        if overlap:
            CellH = np.floor(inputH / multiplexH*2)
            CellW = np.floor(inputW / multiplexW*2)
            CellH=int(CellH)
            CellW=int(CellW)
        Wd=False
        Hd=False
        if CellH%2==0:
            Hd=True
        if CellW%2==0:
            Wd=True
        with tf.variable_scope(name) as vs:
            Welist=[]
            Bilist=[]
            for i in range(multiplexH):
                for j in range(multiplexW):

                    We = tf.get_variable(name='weights%d-%d'%(i,j), shape=[size, size, channels, filters],
                                               initializer=tf.truncated_normal_initializer(stddev=0.03),
                                               dtype=tf.float32, trainable=True)
                    bi = tf.get_variable(name='biases%d-%d'%(i,j), shape=[filters, ],
                                              initializer=tf.constant_initializer(value=0.1),
                                              dtype=tf.float32, trainable=True)
                    Welist.append(We)
                    Bilist.append(bi)
        Convij=[]
        for i in range(multiplexH):
            for j in range(multiplexW):
                ci=np.floor((i+0.5)*CellHm-0.01)+1
                cj=np.floor((j+0.5)*CellWm-0.01)+1
                if not overlap:
                    if i==0:
                        hcs=0
                        hce=hcs+CellH+size-1
                    elif i==multiplexH-1:
                        hce=inputH
                        hcs = hce-CellH-size+1
                    elif Hd:
                        hcs=ci-(CellH+size-1)/2
                        hce=ci+(CellH+size-1)/2
                    else:
                        hcs=ci-np.floor((CellH+size-1)*0.5-0.01)-1
                        hce=ci+np.floor((CellH+size-1)*0.5-0.01)
                    if j==0:
                        wcs=0
                        wce=wcs+CellW+size-1
                    elif j==multiplexW-1:
                        wce=inputW
                        wcs = wce-CellW-size+1
                    elif Wd:
                        wcs=cj-(CellW+size-1)/2
                        wce=cj+(CellW+size-1)/2
                    else:
                        wcs=cj-np.floor((CellW+size-1)*0.5-0.01)-1
                        wce=cj+np.floor((CellW+size-1)*0.5-0.01)
                else:
                    if i == 0:
                        hcs = 0
                        hce = hcs + CellH
                    elif i == multiplexH - 1:
                        hce = inputH
                        hcs = hce - CellH
                    elif Hd:
                        hcs = ci - (CellH) / 2
                        hce = ci + (CellH) / 2
                    else:
                        hcs = ci - np.floor((CellH ) * 0.5 - 0.01)-1
                        hce = ci + np.floor((CellH ) * 0.5 - 0.01)
                    if j == 0:
                        wcs = 0
                        wce = wcs + CellW
                    elif j == multiplexW - 1:
                        wce = inputW
                        wcs = wce - CellW
                    elif Wd:
                        wcs = cj - (CellW ) / 2
                        wce = cj + (CellW ) / 2
                    else:
                        wcs = cj - np.floor((CellW ) * 0.5 - 0.01)-1
                        wce = cj + np.floor((CellW) * 0.5 - 0.01)
                hcs=int(hcs)
                wcs=int(wcs)
                hce=int(hce)
                wce=int(wce)
                it=self.inputs[:,hcs:hce,wcs:wce,:]
                convtemp=tf.nn.conv2d(it,Welist[multiplexW*i+j], strides=[1, stride, stride, 1], padding='VALID')
                convtemp=tf.add(convtemp, Bilist[multiplexW*i+j])
                Convij.append(convtemp)
        convli=[]
        for i in range(multiplexH):
            convlii=[]
            for j in range(multiplexW):
                convlii.append(Convij[multiplexW * i + j])
            convli.append(convlii)
        convt=[]
        for i in range(multiplexH):
            convt.append(tf.concat(convli[i],axis=2))
        convfin=tf.concat(convt,axis=1)
        self.outputs =act(convfin)
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend([self.outputs])
        self.all_params.extend(Welist)
        self.all_params.extend(Bilist)