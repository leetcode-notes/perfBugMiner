Definition of epoch_size in PTB model leads to skipping last sample batch

Not 100% sure this is a bug, please take a look:
The PTB RNN model (tensorflow/models/rnn/ptb/ptb_word_lm.py, line 92) defines a variable called epoch_size as
self.epoch_size = ((len(data) // batch_size) - 1) // num_steps
(This variable is also defined in the same way in tensorflow/models/rnn/ptb/reader.py, line 108.) The variable is used on line 278 in ptb_word_lm.py:
for step in range(model.input.epoch_size):
I'm not sure why the "- 1" term is in the equation -- that is, I think it should be just
self.epoch_size = (len(data) // batch_size) // num_steps
The equation already uses floor division to round down. Currently, if both batch_size and num_steps are 1, epoch_size will be equal to data length - 1, so the last data sample will be skipped.