tf slim, default initializers for relu

The defaults in layers such as conv and conv_transpose have the default activations as relu and the default initializer as xavier. From the paper https://arxiv.org/pdf/1502.01852v1.pdf, would this be the time to change the default initializer to use tf.contrib.layers.variance_scaling_initializer to account for the variance of the distributions being halved at each layer of activation?
Andrej Karpathy has a lecture which shows the effects of this https://youtu.be/gYpoJMlgyXA?t=47m6s
edit:forgot to add time in the video