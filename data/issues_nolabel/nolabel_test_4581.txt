How can user control the communication in distributed tensorflow

sorry to trouble.
When I try to train model with ps and worker. I found that TensorFlow passed the whole gradients of each op between ps and each worker.
If I explicitly calculate the gradient of inputs, which is defined as placeholder, TensorFlow passed these gradients. While the batch size of the inputs is too big, it makes the time of communication too long.
Is there any way to choose the parameters which user want to pass between ps and worker.