Protobuf size explosion for TFLite

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.3 LTS
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf 1.6.0
Python version: Python 3.5.2
Bazel version (if compiling from source): bazel release 0.7.0
GCC/Compiler version (if compiling from source): gcc 4.9.4
CUDA/cuDNN version: /
GPU model and memory: /
Exact command to reproduce:
bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/tmp/gru_rnn.pb --input_format=TENSORFLOW_GRAPHDEF --input_types=FLOAT --output_format=TFLITE --output_file=/tmp/gru_rnn.tflite --inference_type=FLOAT --inference_input_type=FLOAT --output_arrays=model/softmax,model/rnn_states

Describe the problem
I successfully built a tflite model for mobile usage on November, 2017, and the size of .tflite was almost the same as .pb (no quantized), which was bout 950k.
However, when I try to convert the same .pb to tflite model with tensorflow 1.6 master branch (HEAD commit f7acdf2), I find that the size of the .tflite file is 17M, which is 17x larger than before!!
So I make a binary search of the commits in tensorflow/contrib/lite history, then I find out at which commit that thing changes: the commit 90ce801 can produce 950k .tflite model, but right after it 528d0c5 will make the tflite model become 17M.
Here are some additional information:

Both of these two tflite models can be successfully run on mobile.
I did not check the commits (if there have) between these two, since I only trace the /tensorflow/contrib/lite history.
I guess the problem happened due to some of the operations I used in my frozen graph, in which I have 2 layer gru and some linear transforms. But I cannot upload the .pb file here...