Lack of 'name' argument in the tf.contrib.learn.Classifier.evaluate method

Tensorflow version 0.10.0rc0 (Installed today by pip from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl)
I'm working with examples from here and here:
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np

tf.logging.set_verbosity(tf.logging.INFO)

# Data sets
IRIS_TRAINING = "iris_training.csv"
IRIS_TEST = "iris_test.csv"

# Load datasets.
training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,
                                                       target_dtype=np.int)
test_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TEST,
                                                   target_dtype=np.int)

# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                            hidden_units=[10, 20, 10],
                                            n_classes=3,
                                            model_dir="/tmp/iris_model",
                                            config=tf.contrib.learn.RunConfig(
                                                save_checkpoints_secs=1))

validation_metrics = {("metrics/accuracy", "classes"): tf.contrib.metrics.streaming_accuracy,
                      ("metrics/precision", "classes"): tf.contrib.metrics.streaming_precision,
                      ("metrics/recall", "classes"): tf.contrib.metrics.streaming_recall}

validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(
    test_set.data,
    test_set.target,
    every_n_steps=50,
    metrics=validation_metrics)

# Fit model.
classifier.fit(x=training_set.data,
               y=training_set.target,
               steps=2000,
               monitors=[validation_monitor])

# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=test_set.data,
                                     y=test_set.target)["accuracy"]
print('Accuracy: {0:f}'.format(accuracy_score))

# Classify two new flower samples.
new_samples = np.array(
    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)
y = classifier.predict(new_samples)
print('Predictions: {}'.format(str(y)))

and everything is fine for now, but if I use my own model:
def my_model(x, y, mode):
    net = tf.contrib.layers.fully_connected(x, num_outputs=10)
    net = tf.contrib.layers.fully_connected(net, num_outputs=20)
    net = tf.contrib.layers.fully_connected(net, num_outputs=10)

    logits = tf.contrib.layers.fully_connected(net, 3)
    prediction = tf.nn.softmax(logits)

    if mode == tf.contrib.learn.ModeKeys.INFER:
        return prediction, None, None

    loss = tf.contrib.losses.softmax_cross_entropy(logits, tf.one_hot(y, 3))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
    train_op = tf.contrib.slim.learning.create_train_op(loss, optimizer)

    return prediction, loss, train_op


classifier = tf.contrib.learn.Classifier(model_fn=my_model,
                                         n_classes=3,
                                         model_dir="/tmp/iris_model",
                                         config=tf.contrib.learn.RunConfig(
                                            save_checkpoints_secs=1))

instead of:
classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                            hidden_units=[10, 20, 10],
                                            n_classes=3,
                                            model_dir="/tmp/iris_model",
                                            config=tf.contrib.learn.RunConfig(
                                                save_checkpoints_secs=1))

Following error occurs:
Traceback (most recent call last):
  File "/mnt/nfs/dnn/workspace/deep-learning/test/minimal_working_example.py", line 70, in <module>
    monitors=[validation_monitor])
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 240, in fit
    max_steps=max_steps)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 578, in _train_model
    max_steps=max_steps)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py", line 280, in _supervised_train
    None)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py", line 270, in run
    run_metadata=run_metadata)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/recoverable_session.py", line 54, in run
    run_metadata=run_metadata)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py", line 70, in run
    self._coord.join(self._coordinated_threads_to_join)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py", line 357, in join
    six.reraise(*self._exc_info_to_raise)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py", line 66, in run
    return self._sess.run(*args, **kwargs)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py", line 107, in run
    induce_stop = monitor.step_end(monitors_step, monitor_outputs)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py", line 396, in step_end
    return self.every_n_step_end(step, output)
  File "/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py", line 687, in every_n_step_end
    steps=self.eval_steps, metrics=self.metrics, name=self.name)
TypeError: evaluate() got an unexpected keyword argument 'name'

Solution
Add name argument to the Classifier.evaluate method.