[beginner][bug] TensorFlow doesn't seem to be decomposing GradientDescent into XLA ops

System information

Tensorflow compiled on a branch of a fork: https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops. Contains extra LOG(INFO) and std::cout statements to notify the names of the functions being called.
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): ('v1.2.0-1878-ga5066f6', '1.2.1')
Python version: 2.7.12
Bazel version (if compiling from source): release 0.5.2
CUDA/cuDNN version: CUDA 8
GPU model and memory: NVIDIA Quadro K1200,  4 GB
Exact command to reproduce: python <tflow_parent>/tensorflow/tensorflow/example/tutorials/mnist/mnist/mnist_softmax_xla.py
tf_env.txt

Context
My work involves securing Tensorflow's computations. The approach I've planned to take is to add XLA ops during the conversion of TF ops to XLA ops,  which are lowered into functions calls (in LLVM-IR) that send data to and receive data from a secure environment.
I'm currently verifying if both training and inference TF Ops are lowered to XLA ops.
The Problem
I've made some changes to the code to highlight when some OpKernels and XlaOpKernels are being invoked / lowered. I've used the mnist_softmax_xla.py example to observe tensorflow's behaviour.
I found that "ApplyGradientDescentGPU" was being called 2000 times (2 layer network and 1000 iterations, clearly not JIT) instead of ResourceApplyGradientDescent, yet other Ops including MatMul and subtraction seem to have been optimized by XLA.
Why is gradient descent not optimized by XLA ?
Source code / logs
The output text lost its structure when I pasted it. So, here are the screenshots.



 Note that the first 2 kernels have been called a 100 times each.
Related
Also, it would be helpful if someone could answer Tensow - XLA | Passing tensors to external functions at runtime , as it is the important assumption of my approach that Tensorflow can allow such XLA ops. I appreciate any information on its feasibility as I could start on a different approach as soon as possible.
Thanks,
Sanjay