cifar10_eval.py stop problem

Environment info
Operating System: Ubuntu 14.04.4 LTS
If installed from binary pip package, provide:

Which pip package you installed.
: tensorflow-0.7.1-cp27-none-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.version)".
: 0.7.1

Steps to reproduce

cd git/tensorflow/tensorflow/models/image/cifar10/
python cifar10_eval.py

I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.3545
pciBusID 0000:01:00.0
Total memory: 6.00GiB
Free memory: 5.56GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB
and stop this line.
but sometimes not stop this line when run same as above. and
==========check3.25==========
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 5.27GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x706400000 extends to 0x857ab6000
==========check3.5==========
==========check3.75==========
==========check3.25==========
.
.
.
==========check3.25==========
==========check3.5==========
==========check3.75==========
2016-03-01 15:41:09.935657: precision @ 1 = 0.801
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb37405e7b0 Compute status: Cancelled: Enqueue operation was cancelled
[[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](input_producer, input_producer/RandomShuffle)]]
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb350035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb36403ada0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3223f5240 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb35c03ada0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb338035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb36c00c600 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb354035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb340035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb34c035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3480095e0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb33000c470 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb370011050 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb328010190 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3600095e0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb358035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb33c05b3b0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.
[[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]
What have you tried?

Add some instruction for checking stop point.

cifar10_eval.py
def eval_once(saver, summary_writer, top_k_op, summary_op):
# Start the queue runners.
coord = tf.train.Coordinator()
try:
  threads = []
  for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):
    threads.extend(qr.create_threads(sess, coord=coord, daemon=True,
                                     start=True))
  num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))
  true_count = 0  # Counts the number of correct predictions.
  total_sample_count = num_iter * FLAGS.batch_size
  step = 0
  while step < num_iter and not coord.should_stop():
    print("==========check3.25==========")
    predictions = sess.run([top_k_op])
    print("==========check3.5==========")
    true_count += np.sum(predictions)
    step += 1
    print("==========check3.75==========")

  # Compute precision @ 1.
  precision = true_count / total_sample_count
  print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))

  summary = tf.Summary()
  summary.ParseFromString(sess.run(summary_op))
  summary.value.add(tag='Precision @ 1', simple_value=precision)
  summary_writer.add_summary(summary, global_step)
except Exception as e:  # pylint: disable=broad-except
  coord.request_stop(e)

coord.request_stop()
coord.join(threads, stop_grace_period_secs=10)

look like #389 issue but different. and many time stop
predictions = sess.run([top_k_op])
this line.