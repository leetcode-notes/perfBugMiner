how to use multiple parameters server under synchronous gradient update

recently, I try to train the inceptionV3 mode with multi machine. As described in https://github.com/tensorflow/models/tree/master/inception,  tutorial describes how to train inceptionv3 using tensorflow  distributed version, but only use a parameter server.
I want to know how to use multiple parameters server under synchronous gradient update, anyone can give guidance?