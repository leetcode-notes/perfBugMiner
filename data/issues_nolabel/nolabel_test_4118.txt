Undefined Symbols When Compiling User Op for GPU

Summary
I can compile, but not load, a user-defined TF shared library using CUDA. The library is based off of the TF zero_out example. I've modified the example to support CPU and GPU devices. Upon loading the shared library I get the following error: tensorflow.python.framework.errors.NotFoundError: zero_out.so: undefined symbol: _ZN10tensorflow7functor14ZeroOutFunctorIN5Eigen9GpuDeviceEEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi1ELi1ElEELi16EEENS7_INS8_IfLi1ELi1ElEELi16EEEi.
I suspect that the problem has something to do with my particular combination of compiler and OS environment. However after trying many ideas I've hit a wall.
Similar Issues
#2097: My problem looks similat to this one.
#1569: There are two suggestions made in this issue:

Try a gcc-4.* compiler
Use the compiler variable D_GLIBCXX_USE_CXX11_ABI=0

Neither of these suggestions are working for me.
Detailed Explanation
I give details below about my environment, the source code, the steps I've taken to compile, and the error.
Environment
OS: Ubuntu 16.04 LTS
Kernel : 4.4.0-34-generic
Compiler: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609
Cuda: 7.5
Cudnn:  5.1.3
My problem exists whether I use a TF binary or compile my own version from source. Note that in order to successfully compile TF from source I must add the following three lines to CROSSTOOL.tpl:
cxx_flag: "-D_MWAITXINTRIN_H_INCLUDED"
cxx_flag: "-D_FORCE_INLINES"
cxx_builtin_include_directory: "/usr/local/cuda-7.5/include"

Source Code
I have three source files and a Makefile:
zero_out.h
#ifndef TENSORFLOW_KERNELS_ZERO_OUT_OP_H_
#define TENSORFLOW_KERNELS_ZERO_OUT_OP_H_

namespace tensorflow {

namespace functor {

// Generic helper functor for the ZeroOut Op.
template <typename Device>
struct ZeroOutFunctor;

}  // namespace functor
}  // namespace tensorflow

#endif  // TENSORFLOW_KERNELS_ZERO_OUT_OP_H_
zero_out.cc
#define EIGEN_USE_THREADS
#include "zero_out.h"
#include "tensorflow/core/framework/op_kernel.h"
#include "tensorflow/core/framework/tensor.h"

namespace tensorflow {

REGISTER_OP("ZeroOut")
.Input("to_zero: float")
.Output("zeroed: float")
.Doc(R"doc(
Zeros all elements of the tensor except the first.
zeroed: A Tensor.
  output[0] = input[0]
  output[1:N] = 0
)doc");;

typedef Eigen::ThreadPoolDevice CPUDevice;
typedef Eigen::GpuDevice GPUDevice;

namespace functor {

template <typename Device>
struct ZeroOutFunctor {
  void operator()(const Device& d,
          typename TTypes<float>::ConstFlat input,
          typename TTypes<float>::Flat output,
          const int N);
};

template <>
struct ZeroOutFunctor<CPUDevice> {
  void operator()(const CPUDevice& d,
          typename TTypes<float>::ConstFlat input,
          typename TTypes<float>::Flat output,
          const int N) {
    for (int i = 1; i < N; i++) {
      output(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output(0) = input(0);
  }
};
} // namespace functor    

template <typename Device>
class ZeroOutOp : public OpKernel {
public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<float>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                             &output_tensor));

    auto output = output_tensor->template flat<float>();
    const int N = input.size();
    functor::ZeroOutFunctor<Device>()(context->eigen_device<Device>(),
                      input, output, N);
  }
};

REGISTER_KERNEL_BUILDER(Name("ZeroOut")                 \
            .Device(DEVICE_CPU),        \
            ZeroOutOp<CPUDevice>);

#if GOOGLE_CUDA
REGISTER_KERNEL_BUILDER(Name("ZeroOut")                 \
            .Device(DEVICE_GPU),        \
            ZeroOutOp<GPUDevice>);
#endif // GOOGLE_CUDA
} // namespace tensoroflow

zero_out_gpu.cu.cc
#if GOOGLE_CUDA

#define EIGEN_USE_GPU

#include "zero_out.h"

#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"
#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/framework/tensor_types.h"


namespace tensorflow {

namespace functor {

using GPUDevice = Eigen::GpuDevice;

__global__ void ZeroOutKernel(const float* in, float* out, const int N) {
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N;
       i += blockDim.x * gridDim.x) {
    if (i == 0) {
      out[i] = in[i];
    } else {
      out[i] = 0;
    }
  }
}

template <>
struct ZeroOutFunctor<GPUDevice> {
  void operator()(const GPUDevice& d,
          typename TTypes<float>::ConstFlat input,
          typename TTypes<float>::Flat output,
          const int N) {
    // How to compute the optimal block count and threads per block?
    // tensorflow/core/util/cuda_kernel_helper.h isn;t included in the binary
    // distribution
    ZeroOutKernel<<<32, 256, 0, d.stream()>>>(input.data(), output.data(), N);
  }
};

template struct ZeroOutFunctor<GPUDevice>;  
} // namespace functor 
} // namespace tensorflow
#endif // GOOGLE_CUDA
Makefile
INCLUDE += -I /usr/local/cuda-7.5/include
INCLUDE += -I $(shell python -c \
    'import tensorflow as tf; print(tf.sysconfig.get_include())')

CXX = gcc -std=c++11
CXXFLAGS =                          \
    -D_MWAITXINTRIN_H_INCLUDED  \
    -D_FORCE_INLINES            \
    $(INCLUDE) -fPIC -lcudart   \

NVCC = nvcc -std=c++11 -c
NVCCFLAGS =                         \
    -D_MWAITXINTRIN_H_INCLUDED  \
    -D_FORCE_INLINES            \
    $(INCLUDE) -x cu -Xcompiler -fPIC

LDFLAGS = -shared
CUDA_SRCS = zero_out_gpu.cu.cc
SRCS = zero_out.cc
RM = rm -f
TARGET_LIB = zero_out.so
CUDA_OBJ = zero_out.cu.o

all: $(TARGET_LIB)

# This target (CPU and GPU) does not find the right symbols
$(TARGET_LIB): $(SRCS) $(CUDA_OBJ) 
    $(CXX) $(LDFLAGS) -o $@ $^ $(CXXFLAGS) -DGOOGLE_CUDA=1

# This target (CPU only) is fine
# $(TARGET_LIB): $(SRCS) 
#   $(CXX) $(LDFLAGS) -o $@ $^ $(CXXFLAGS) -DGOOGLE_CUDA=0

$(CUDA_OBJ): $(CUDA_SRCS)
    $(NVCC) -o $@ $^ $(NVCCFLAGS) -DGOOGLE_CUDA=1

.PHONY: clean
clean:
    -$(RM) $(TARGET_LIB)
    -$(RM) *~
    -$(RM) *.o
Compilation
Compilation runs without errors:
$ make
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
nvcc -std=c++11 -c -o zero_out.cu.o zero_out_gpu.cu.cc -D_MWAITXINTRIN_H_INCLUDED -D_FORCE_INLINES -I /usr/local/cuda-7.5/include -I /home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include -x cu -Xcompiler -fPIC -DGOOGLE_CUDA=1
/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/allocator.h(155): warning: missing return statement at end of non-void function "tensorflow::Allocator::RequestedSize"

/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/allocator.h(155): warning: missing return statement at end of non-void function "tensorflow::Allocator::RequestedSize"

I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
gcc -std=c++11 -shared -o zero_out.so zero_out.cc zero_out.cu.o -D_MWAITXINTRIN_H_INCLUDED -D_FORCE_INLINES -I /usr/local/cuda-7.5/include -I /home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include -fPIC -lcudart  -DGOOGLE_CUDA=1
Error When Loading the Library:
$ python -c "import tensorflow as tf; tf.load_op_library('zero_out.so')t.so')"
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py", line 75, in load_op_library
    raise errors._make_specific_exception(None, None, error_msg, error_code)
tensorflow.python.framework.errors.NotFoundError: zero_out.so: undefined symbol: _ZN10tensorflow7functor14ZeroOutFunctorIN5Eigen9GpuDeviceEEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi1ELi1ElEELi16EEENS7_INS8_IfLi1ELi1ElEELi16EEEi
Notes and Some Things I've Tried

I have no difficulties compiling and loading a shared library for CPU-only. (See the commented target in the Makefile.)
My problem persists whether using a TF bleeding-edge source (with bazel) or TF binary installation (with make)
I've tried compiling the shared library with g++, as well as using some earlier gcc-4.* compilers
I've tried mimicking the nvcc and gcc options provided in CROSSTOOL.tpl
I don't have immediate access to an earlier Linux distro, otherwise I would have tried it

Any help is greatly appreciated!