feeding_functions._GeneratorFeedFn does not return correct num of batches

feeding_functions._GeneratorFeedFn does not return correct #samples in mini batch. It actually returns mini batches with batch_size/key_size (key size is the length of dict yielded by the generator func).
gff = feeding_functions._GeneratorFeedFn(
    placeholders=placeholders,
    generator=generator_fn, # yields { 'feature_set_1': [] (shape 1x13), 'feature_set_2': [] (shape 1x20) }
    batch_size=32,
    random_start=False,
    seed=None,
    num_epochs=1,
    pad_value=None
)
actual = gff()
for k in actual:
    print(actual[k].shape)
# (16, 1, 13)
# (16, 1, 20)
https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/estimator/inputs/queues/feeding_functions.py
list_dict_size compared with self._batch_size in _GeneratorFeedFn.__call__
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, snippet above
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.13.1
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.4.0
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: Code snippet above