Optimzer: Better handling of gradients for min/max ops.

Please don't kick me too hard for this. If this ticket should not be here, please direct me to the proper place. It's not a help or support request, just a very naive idea/request/improvement.
Image a model has this op in the graph: y = tf.maximum(a, b). If y is contributing error to the loss (higher y higher is loss), then you are interested in minimizing both a and b. Otherwise, your optimizer will play whack-a-mole game forever. Especially, if you have something like this in your model: b = 1 - a and y = tf.maximum(a, b)
I've researching how does optimizer works and looks like each op has a function that defines how to calculate its gradients.
In the current implementation for maximum/minimum (python's version) is located here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L901
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L883
In the current implementation, it discards gradients for a, if b is bigger than a and other way around.
I think, that there is some place for improvement of training speed simply by considering gradients for all variables in min/max operations (including reduce_min/reduce_max and all max/min pooling). There are a lot of models with max pooling. It makes sense that they will train faster if they stop playing whack-a-feature each time it has max pooling layer.
If it make sense, I would like to do a small PoC.