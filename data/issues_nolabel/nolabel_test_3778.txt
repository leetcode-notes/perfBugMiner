Reduce memory usage and increase performance for convolution on iOS

We've had lots of problems with large convolutions hitting memory limits on iOS. This new implementation of the operator breaks the work into chunks so we never use more than 16 MB, and uses Apple's Accelerate framework to optimize the matrix multiplication.
Testing shows that it's between 5% to 10% faster than the existing implementation on various models, and keeps memory usage to a minimum.