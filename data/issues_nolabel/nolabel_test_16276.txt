Linking against system-installed cuda and cudnn

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian sid
TensorFlow installed from (source or binary): source (trying)
TensorFlow version (use command below): git master (commit 9fb9ac6) or any version before.
Python version: 3.6
Bazel version (if compiling from source): 0.9.0
GCC/Compiler version (if compiling from source): 7.2.0
CUDA/cuDNN version: CUDA 9.0, cuDNN 6.0
GPU model and memory: GeForce GTX 1070
Exact command to reproduce:
bazel build --config=opt --config=mkl --config=cuda //tensorflow/tools/pip_package:build_pip_package

Describe the problem
The build system currently require that all the libraries from the CUDA toolkit are stored in a specific directory called cuda_toolkit_path, both in the bazel scripts and in the configure.py script. However, some systems (like Debian) have a packaged version of CUDA which installs the libraries in the standard path which cannot be found by configure.py.
The compiler can find those libraries just right with nothing more than -lcuda. It would be nice if the build system could rely on the compiler's ability to find its libraries instead of relying on the knowledge of their full path.
Source code / logs
As a feature-request / enhancement-request, this section seems irrelevant.