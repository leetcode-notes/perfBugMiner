Tensor with inconsistent dimension size?

I've been implementing a convolutional neural network for object detection and I met the issue below:
For object detection task, usually, one input image is associated with an undetermined number of object bounding boxes. Each bounding box can be represented by 4 coordinates. Thus, to represent bounding boxes as a tensor, the shape will be:
[batch_size, variable_num_bbox(?), 4].
Note that here, it's not just that variable_num_bbox can't be determined before the graph construction, but also, even within one batch input, different images can have different numbers of bounding boxes.
As an illustrative example, I would like to convert the following array into a tensor:
[[[1, 2, 3, 4], [2, 3, 4, 5]], [[3, 4, 5, 6]]]
Here, variable_num_bbox is 2 for the first image, but it's 1 for the second image.
I've tried and failed several ways to convert the above nested list to a tensor, which leads me to the question whether tensorflow support tensors with inconsistent dimension size? If no, is there any plan to support it to give developers such flexibility? And if it's not going to be supported, is there a way to by-pass this issue for object detection task? One solution would be to set batch_size=1 , and a bounding box can be represented as [variable_num_bbox(?), 4], so yes, the dimension inconsistency is gone, but that will hurt the efficiencyy significantly.