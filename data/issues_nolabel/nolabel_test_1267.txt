learning rate reset to 0 in seq2seq example

In the translate.py example, I used four layers without attention, the learning rate suddenly becomes zero...
 itr 1/311830 epoch 1/10 lr 0.7000 cost 9.77 ppl 17455.77
 itr 2/311830 epoch 1/10 lr 0.0000 cost 9.86 ppl 19082.18
 itr 3/311830 epoch 1/10 lr 0.0000 cost 9.80 ppl 18084.05
 itr 4/311830 epoch 1/10 lr 0.0000 cost 9.78 ppl 17700.32

This is parameter specific, when I set the batch size to be 128, learning rate to be 0.7 or 1.
When I change the batch size to be 64, or set the learning rate to be 0.5, the problem DISAPPEARS.
In debugging, when I try to set it back to 0.7 after it becomes 0., it will automatically go back to 0 in the next time step...
I confirm the learning rate update operation is not called.
I am using the 0.7.1 version with cuda 7.5