NNAPI/libneuralnetwors.so does not work for other models than default mobilenet_quant_v1_224.tflite

I have made changes to tensorflowlite to force it use nnapi on my android 8.1 phone, and the nnapi only worked normally for the default default mobilenet_quant_v1_224.tflite.
The nnapi/libneuralnetworks.so did not work for the tensorflow_inception_graph.tflite inside the attached asserts.zip, which was built by toco, from tensorflow_inception_graph.pb of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android
My steps,

generated tensorflow_inception_graph.tflite from tensorflow_inception_graph.pb by toco
add 7 extra lines to labels.txt "a\nb\nc\nd\ne\ng\ng"
copy new labels.txt and tensorflow_inception_graph.tflite to my bazel cache
apply the attached patch a.txt to tensorflow
bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
adb install -r <apk_file>

If the nnapi is not forced to use, then the apk run normally, though the results are wrong. But if I forced it to use nnapi, then the apk crashed.
I guess the tensorflow_inception_graph.tflite contained some operators which were not supported by nnapi/libneuralnetwors.so. How can I confirm it?
Thank you.
a.txt
assets.zip