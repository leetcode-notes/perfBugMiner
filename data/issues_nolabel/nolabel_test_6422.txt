[Feature request] Ability to reimplement backprop.

Currently there is no straightforward way to do things like implementing different learning rates for different layers. Much less apply learning rate masks for individual unit adjustment.
Torch allows you to easily reimplement backprop however you want. TF seems limited to this: https://www.tensorflow.org/api_docs/python/framework/core_graph_data_structures#Graph.gradient_override_map