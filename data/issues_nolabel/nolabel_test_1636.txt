dynamic_rnn got  an error

I want to test dynamic_rnn based on ptb_word_lm.py.
when I use rnn,it's ok!
inputs = [tf.squeeze(input_, [1])
          for input_ in tf.split(1, num_steps, inputs)]
outputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)

But,when I use dynamic_rnn,like this:
outputs,state=rnn.dynamic_rnn(cell,inputs,initial_state=self._initial_state)

It's got an error:
Traceback (most recent call last):
  File "ptb_word_lm.py", line 391, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "ptb_word_lm.py", line 368, in main
    m = PTBModel(is_training=True, config=config)
  File "ptb_word_lm.py", line 207, in __init__
    self._train_op=tf.train.GradientDescentOptimizer(0.01).minimize(loss)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 190, in minimize
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 241, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py", line 481, in gradients
    in_grads = _AsList(grad_fn(op, *out_grads))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py", line 137, in _TensorArrayPackGrad
    grad_source = _GetGradSource(grad)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py", line 62, in _GetGradSource
    % op_or_tensor.name)
ValueError: Expected op/tensor name to start with gradients, got: model/gradients/model/RNN/transpose_grad/transpose:0