refresh model for spark streaming with sv = tf.train.Supervisor() under sv.managed_session()

System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I made custom distributed mnist code with spark streaming, based on TensorFlowOnSpark example.
OS Platform and Distribution: CentOS 7
TensorFlow installed from (source or binary): Unmodified source with HDFS enabled
TensorFlow version (use command below): 1.2.1
Python version: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)
Bazel version (if compiling from source): bazel release 0.5.2
CUDA/cuDNN version: 8.0/5.1.10
GPU model and memory: NVIDIA Geforce 1070 8GB (shared by two yarn containers, each has two excuetors)
The code works fine, but with --mode inference a yarn container will restore the model from logdir at the very beginning and use the same model during the whole prediction periode(based on spark logging info for restoring event).
My question is, is there a way that I can reload some new trained model(same architecture) under sv.managed_session() periodically or after new model is generated? Because at the same time, the other yarn container with --mode train produced new model.
Thanks for helping!