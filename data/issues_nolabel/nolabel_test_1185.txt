Logistic Regression(LR) Results looks strange when using bias but good without bias

I tried the LR for sparse dataset using the tensorflow package(0.7.0)
The following is part of my procedure:
weight_values=generateWeight([trainset.feature_num,1],name='weight')
bias=init_bias([1,1],name='bias')
sp_shape=tf.placeholder(tf.int64)
sp_indices=tf.placeholder(tf.int64)
sp_ids_value=tf.placeholder(tf.int64)
sp_features_value=tf.placeholder(tf.float32)
Y=tf.placeholder('float',name='Y')
sp_ids=tf.SparseTensor(sp_indices,sp_ids_value,sp_shape)
sp_values=tf.SparseTensor(sp_indices,sp_features_value,sp_shape)
#Z=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values);
Z_b=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values)+bias
predict_op=tf.sigmoid(Z_b,name='result')
cost=tf.nn.sigmoid_cross_entropy_with_logits(Z,Y)
cost=tf.nn.sigmoid_cross_entropy_with_logits(Z_b,Y)
train_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
...
but I find the results is  wired when using the bias,using the bias I find the abs(cost) became larger and larger and even the abs(bias) also became larger and larger.
IN the early iterations the results like this :
1. labels[-1], j-sample 1662, i-bias 0.100000 cost 0.761811, z_b 0.045602
2 labels[1], j-sample 823, i-bias 0.084886 cost 0.653277, z_b 0.081396
3 labels[-1], j-sample 20802, i-bias 0.089683 cost 0.826316, z_b 0.088132
4 labels[-1], j-sample 25965, i-bias 0.074462 cost 0.806052, z_b 0.074804
5 labels[1], j-sample 10322, i-bias 0.059276 cost 0.664358, z_b 0.058433
6 labels[-1], j-sample 23946, i-bias 0.064129 cost 0.795182, z_b 0.067642
......
but with more iteration the results like this :
270504 labels[-1], j-sample 446, i-bias -248.318787 cost -250.818787, z_b -250.818787
270505 labels[1], j-sample 10314, i-bias -248.328781 cost 248.306259, z_b -248.306259
270506 labels[1], j-sample 3820, i-bias -248.318787 cost 247.367340, z_b -247.367340
270507 labels[1], j-sample 2922, i-bias -248.308792 cost 248.276184, z_b -248.276184
270508 labels[-1], j-sample 20797, i-bias -248.298798 cost -255.061432, z_b -255.061432
270509 labels[-1], j-sample 19755, i-bias -248.308792 cost -251.686646, z b -251.686646
270510 labels[1], j-sample 9528, i-bias -248.318787 cost 248.405624, z b 248.405624
However if I donot use the bias the results looks good, what maybe the problems and I wonder the reasons very much.
Can anynone help me, Thank you!