[feature request] large scale embedding for sparse features

In my scene, training data is very large, we have 10e10 unique values(string) for embedding in one sparse column. In short, model structure is “input->embedding->NNs”. I set hash_bucket_size to 15e10 for low hash collision. sample code:
col0=tf.contrib.layers.sparse_column_with_hash_bucket("feature_id", hash_bucket_size=1.5e11) cols=[tf.feature_column.embedding_column(categorical_column=col0, dimension=8)]
in above example, Tensorflow new a Variable with shape [1.5e11, 8]
Some problems:

memory waste, more than 0.5e1184Bytes memory not used.
hash collision, though enlarging the hash_bucket_size, it might occurs conflict. I don't know how much it influence on model.
Is there any suggestions in Tensorflow in this case ?
In my opinion:
Define a new Variable for embedding, need not define the first demension. It will malloc memory for this Variable when the new value is embedding_lookup. It will solve the two problems mentioned above.

Thanks