Training Cifar10 on two GPUs crashed.

Environment info
Operating System:16.04.1 LTS (Xenial Xerus)
Installed version of CUDA and cuDNN: 8.0 and 5.0.5
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
-rw-r--r-- 1 root root   558720 Nov 19 10:33 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Nov 19 10:33 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn_static.a
Installed from source:

The commit hash: 8157ae2
The output of bazel version:
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
python cifar10_multi_gpu_train.py --num_gpus=2
(Both 'python cifar10_train.py' and 'python cifar10_multi_gpu_train.py --num_gpus=1' work well.)
Logs or other output that would be helpful
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)
ERROR:tensorflow:Exception in QueueRunner: Expected begin[0] in [0, 32], but got -6
[[Node: tower_1/random_crop = Slice[Index=DT_INT32, T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:1"](tower_1/Cast_1, tower_1/random_crop/mod, tower_1/random_crop/size)]]
[[Node: tower_1/Div/_82 = _Recvclient_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:1", send_device_incarnation=1, tensor_name="edge_131_tower_1/Div", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]]
Caused by op u'tower_1/random_crop', defined at:
File "cifar10_multi_gpu_train.py", line 289, in 
tf.app.run()
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/platform/app.py", line 30, in run
sys.exit(main(sys.argv))
File "cifar10_multi_gpu_train.py", line 285, in main
train()
File "cifar10_multi_gpu_train.py", line 189, in train
loss = tower_loss(scope)
File "cifar10_multi_gpu_train.py", line 76, in tower_loss
images, labels = cifar10.distorted_inputs()
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10.py", line 156, in distorted_inputs
batch_size=FLAGS.batch_size)
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10_input.py", line 172, in distorted_inputs
distorted_image = tf.random_crop(reshaped_image, [height, width, 3])
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/random_ops.py", line 326, in random_crop
return array_ops.slice(value, offset, size, name=name)
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/array_ops.py", line 328, in slice
return gen_array_ops.slice(input, begin, size, name=name)
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/gen_array_ops.py", line 2009, in _slice
name=name)
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/op_def_library.py", line 703, in apply_op
op_def=op_def)
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py", line 2322, in create_op
original_op=self._default_original_op, op_def=op_def)
File "/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py", line 1244, in init
self._traceback = _extract_stack()
Other threads reported similar errors.
Please help! I'm looking forward to your reply.
Sincerely