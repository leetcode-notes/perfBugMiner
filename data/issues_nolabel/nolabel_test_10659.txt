seq2seq.dynamic_rnn_decoder not working for multilayered encoder in TF 1.0

I am trying to implement Multilayer Bidirectional Attention based seq2seq in TF 1.0 using **tf.contrib.seq2seq library**.
This is my implementation.
It works perfectly fine for single layered encoder and decoder but gives error with multi layer.
Error : "ValueError: Shape must be rank 2 but is rank 4 for 'Decoder/dynamic_rnn_decoder/Decoder/attention_decoder/concat' (op: 'ConcatV2') with input shapes: [?,10], [2,2,?,20], []."
The problem is , when running single layer, the encoder just returns LSTM state tuple(for 1 layer) but in multi layer it returns an array of LSTMStateTuple(i.e. one for each layer.) which is fed into the decoder and creating problem I guess.
So isn't seq2seq.dynamic_rnn_decoder made to work MultiRNNCell, i.e. multilayered decoder ?
Encoder part is running perfectly fine for multi layers and returning encoder states for each layer.
**_If u want to run my implementation :

clone repo
switch to tf 1.0 env
python generate_questions.py

This will just create computation graph. In case of num_layer=1, graph is created is successfully but fails with num_layer>1._**