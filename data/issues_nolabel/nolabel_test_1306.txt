Multiple bugs in dynamic_rnn

I know this feature remains alpha and unsupported, but I've run into a number of various bugs so I thought it may be worth reporting.
Just a simple RNN with a simple loss function + optimizer doesn't work. Take this code snippet for example:
num_stepss = [34, 20, 44, 18]
inputs = tf.constant(npr.rand(50, 4, 20).astype('float32'))
cell = tf.nn.rnn_cell.LSTMCell(num_units=100, 
                               input_size=20, 
                               initializer=tf.constant_initializer(0.05),
                               use_peepholes=True,
                               cell_clip=None)
outputs, _ = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, sequence_length=num_stepss, dtype=tf.float32, time_major=True)
loss = tf.reduce_sum(outputs)
trainer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

This fails with the following error:
ValueError: Shapes () and (120, 400) must have the same rank
A slightly fancier version of the above that encloses it within scope 'my_scope' using tf.variable_scope gives the following error:
ValueError: Expected op/tensor name to start with gradients, got: my_scope/gradients/my_scope/Sum_grad/Tile:0
I've run into other issues as well, but I don't yet know if they're related to the above or they're distinct bugs.