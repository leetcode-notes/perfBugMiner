Issue while backpropagating through sparse_tensor_dense_multiply

I have a simple network defined as follows:
h1 = tf.sparse_tensor_dense_matmul(x, W1)
h2 = tf.matmul(h1, W2)
y = tf.matmul(h2, W3)
loss = tf.nn.l2_loss(y - y_)
train = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)

where x is a sparseTensor, rest are dense.
Dimensions (shapes) of W1 = [1000,200], W2 = [200,400] and W3 = [ 400, 500].
When I run the following:
sess.run([train], feed_dict={x:X, y_:Y})

where X is sparseTensor of shape [N, 1000] and Y is a tensor of shape [N, 500]
I get an error saying: OOM when allocating tensor with shape[3684773,200].
This is happening while the the the gradient for W is being computed. 3684773 also happens to be the number of non-zero elements in X.
Note:

When I compute gradients using tf.gradients, they work completely
fine.
When I run the same network using dense X and dense multiply( tf.matmul ), it works completely fine.