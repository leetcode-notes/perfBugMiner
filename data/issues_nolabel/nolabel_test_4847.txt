input tensor lost in --mode eightbit quantization

Quantizing with --mode eightbit somehow loses the input tensor, whereas other methods like weights_rounded don't. If I do this:
python quantize_graph.py --input ~/proc/frozen_inference_optimized.pb --output ~/proc/frozen_inference_optimized_quantized.pb --output_node_names on_logits --mode eightbit
the following error pops up in the iOS tensorflow runtime:
Running model failed:Not found: FeedInputs: unable to find feed output preprocess/centered_bgr
Here preprocess/centered_bgr is the node I want to use as the input. If instead I use something like --mode weights_rounded no error is raised and the network functions as expected.
Environment info
Operating System: ubuntu / iOS
Installed version of CUDA and cuDNN:
root@0f1fd91e29a1:~/w/tensorflow/tensorflow/contrib/quantization/tools# ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a

Commit I'm using: 8915f0f. bazel version 0.3.0.
cc @petewarden