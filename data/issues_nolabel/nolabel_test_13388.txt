Add fp16 support to fused batchnorm op

Attention @zheng-xq

This commit adds a mixed-precision fused_batch_norm_v2 op.
The inputs and outputs are fp16, while the scale, offset, mean
and variance are kept in fp32.
The tf.nn.fused_batch_norm op has been modified to use the v2
fused batchnorm whenever inputs are fp16 (this does not affect
compatibility because fp16 was not previously supported).
The high-level layers API has also been updated to store the
scale, offset, mean and variance variables as fp32.