dynamic_rnn gradients unable to be applied

Operating System: Ubuntu 14.04.3 LTS
Installation: 0.7.0 via pip
Output from python -c "import tensorflow; print(tensorflow.__version__)": 0.7.0
I realize dynamic_rnn isn't official yet, but I'm hoping to see whether I'm using it correctly / whether there's a bug.
If I use rnn.rnn, optimizer.apply_gradients(grad_var_pairs) works with no issues. If I use rnn.dynamic_rnn, with the only change being how inputs must be handled, I get an error: ValueError: Shapes () and (276, 1024) must have the same rank
So I looked at the gradients. All gradients have identical shapes/names except two.
In the rnn.rnn case, we have
<tf.Tensor 'gradients/AddN_596:0' shape=(?, ?) dtype=float32> <tf.Tensor 'gradients/AddN_595:0' shape=<unknown> dtype=float32>
Whereas in the rnn.dynamic_rnn case, we have
<tf.Tensor 'gradients/model/model/rnn_fw/RNN/While/cond/LSTMCell/MatMul/Enter_grad/b_acc_3:0' shape=() dtype=float32> <tf.Tensor 'gradients/model/model/rnn_fw/RNN/While/cond/LSTMCell/BiasAdd/Enter_grad/b_acc_3:0' shape=() dtype=float32>
Do I have to manipulate these in some way before applying them?