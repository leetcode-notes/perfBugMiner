"ValueError: No gradients provided for any variable" when using bidirectional_dynamic_rnn

I got the following error when using tf.nn.bidirectional_dynamic_rnn.
Traceback (most recent call last):
  File "main_cnn.py", line 63, in <module>
    main()
  File "main_cnn.py", line 50, in main
    model.run(num_epoch, learning_rate=learning_rate)
  File "/home/s1510032/research/programs/textsum-cnn/model.py", line 266, in run
    self.optim = self.opt.apply_gradients(zip(grads, params), global_step=self.global_step)
  File "/home/s1510032/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 298, in apply_gradients
    (grads_and_vars,))
ValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c570d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd58e10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd63690>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd7c5d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c0f1d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c22890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c35850>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103da890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dac50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dad10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103e13d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f1038cf10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103aabd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e119ecf90>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e1197f150>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3d50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3dd0>))


If I keep everything and use tf.nn.bidirectional_rnn instead, this problem goes away. Here is the working code and non-working code. Loss is calculated based on outputs and that part is the same for both cases.
Working code:
outputs, fw_states, _ = tf.nn.bidirectional_rnn(self.lstm_fw_cell,
                         self.lstm_bw_cell,
                         self.sent_cnn_outputs,
                         dtype=tf.float32)

 # calculate loss using outputs

Non-working code:
outputs, output_states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, 
                    self.lstm_bw_cell,
                    self.sent_cnn_outputs,
                    time_major=True,
                    sequence_length=self.sequence_length,
                    dtype=tf.float32)

outputs = tf.concat(2, outputs)
outputs = tf.unpack(outputs, 0)

# calculate loss using outputs

I suspect the issue here could because of tf.unpack function, which I use to make outputs become a list (for using in zip function in python with other list. Do you have any suggestion to resolve this?