translate.py -- dev set batches

The code to evaluate the dev set at each checkpoint selects a random sample from the dev set for each bucket which is only as large as the minibatch size.
This seems very non-standard to have such a small dev set that changes from checkpoint to checkpoint.
I modified my code to use a larger fixed dev set, but getting errors because the shape of the model depends on the batch size.
tensorflow.python.framework.errors.InvalidArgumentError: Incompatible shapes: [260,512] vs. [64,512]
     [[Node: model_with_buckets/sequence_loss/sequence_loss_by_example/sampled_softmax_loss_9/add_2 = Add[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](model_with_buckets/sequence_loss/sequence_loss_by_example/sampled_softmax_loss_9/add_1, model_with_buckets/sequence_loss/sequence_loss_by_example/sampled_softmax_loss_9/SparseToDense)]]
Caused by op u'model_with_buckets/sequence_loss/sequence_loss_by_example/sampled_softmax_loss_9/add_2', defined at:


Any suggestions for constructing model so that it accommodates a flexible batch size?
I suppose this a feature request to update translate.py, otherwise let me know if I should take this issue over to Stack Overflow.