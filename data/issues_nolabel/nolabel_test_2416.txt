Memory error running tensorflow code on GPU

Environment info
Operating System: Scientific Linux release 7.2 (Nitrogen)
Installed version of CUDA and cuDNN:
$ ll /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 185K Mar 18 15:29 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root   16 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root   19 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 305K Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 545K Mar 18 15:29 /usr/local/cuda/lib/libcudart_static.a

Installed version 0.8.0 with anaconda.
I'm getting a MemoryError when I try to run a tensorflow script on a server with GPU support. The same code, with the same inputs, runs without problems on my local machine, which is CPU only and has 8 GB of RAM.
I tried to allocate up to 64 GB to run the script, and the same problem occurred. Here's the stacktrace:
Traceback (most recent call last):
  File "src/train.py", line 93, in <module>
    learning_rate=args.rate, l2_constant=args.l2)
  File "/hltsrv0/rocha/rte-lstm/src/rte_lstm.py", line 189, in __init__
    gradients, v = zip(*optimizer.compute_gradients(self.loss))
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 241, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 485, in gradients
    in_grads = control_flow_ops.tuple(in_grads)
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1784, in tuple
    tpl.append(with_dependencies([gate], t))
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1664, in with_dependencies
    return _Identity(output_tensor, name=name)
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 114, in _Identity
    return array_ops.identity(data, name=name)
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 609, in identity
    return _op_def_lib.apply_op("Identity", input=input, name=name)
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 655, in apply_op
    op_def=op_def)
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2154, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1165, in __init__
    self._recompute_node_def()
  File "/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1318, in _recompute_node_def
    self._control_inputs])
MemoryError

From the stack trace, I see that the error happens sometime during the gradient computation, but I have no idea why it only happens when I run the code in the server.
By the way, I'm not experienced in GPU computation. I just tried running the same code in an environment with the cuda libraries and tensorflow with GPU support installed.