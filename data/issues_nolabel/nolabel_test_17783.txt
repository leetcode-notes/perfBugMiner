Segmentation fault in Eigen::internal::InnerMostDimReducer<...>::reduce when passing large tensor to sparse_softmax_cross_entropy_with_logits

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have written custom code. A simple reproduction script is included below.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (also occurring on Linux Ubuntu 14.04)
TensorFlow installed from (source or binary): TensorFlow installed from binary (also occurring after building from source)
TensorFlow version (use command below): TensorFlow v1.6 (also occurring on v1.4, v1.5, v1.7rc0)
Python version: Python 2.7 (Ubuntu base and also occurring on Anaconda Python 2.7)
Bazel version (if compiling from source): Bazel version: 0.11.1
GCC/Compiler version (if compiling from source): GCC 4.9.4
CUDA/cuDNN version: CUDA not used, CPU only
GPU model and memory: GPU not used, CPU only (Intel(R) Xeon(R) CPU E5-2650 and Intel(R) Xeon(R) Platinum 8175M)
Exact command to reproduce: Command to reproduce using script given below: "python sfi.py 300000"

Describe the problem
A segmentation fault is occurring with the following gdb backtrace when a "logits" tensor of sufficient size is passed to sparse_softmax_cross_entropy_with_logits. The single argument to the demonstration code below adjusts the size. I have found that there is a point below which the SegFault does not seem to ever occur and above which the SegFault always seems to occur, but around that point (e.g. within +/- 2) the SegFault behaviour is intermittent. Right on the change point I can run the same code with the same argument and it will sometimes generate a SegFault and sometimes not (though the random data generated in the demo code may be causing this randomness).
Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fffcdffb700 (LWP 2440)]
0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::internal::MaxReducer<float>, true>::reduce(Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice> const&, int, int, Eigen::internal::MaxReducer<float>&) ()
   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
(gdb) bt
#0  0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::internal::MaxReducer<float>, true>::reduce(Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice> const&, int, int, Eigen::internal::MaxReducer<float>&) ()
   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007fffef12a90f in Eigen::internal::EvalRange<Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, int, true>::run(Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>*, int, int) () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fffedcd5541 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fffebcecb70 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#11 0x00007fffebceb8e2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#12 0x00007fffe20355b0 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#13 0x00007ffff77e7184 in start_thread (arg=0x7fffcdffb700) at pthread_create.c:312
#14 0x00007ffff6e0703d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111

The problem occurs in all of these configurations:

Binary CPU-only install of TF: v1.4, v1.5, v1.6, and v1.7
Build from source code: v1.6, v1.7
Building with MKL and without MKL
Ubuntu Python and Anaconda Python (but only Python 2.7 in both cases)
In clean virtual/conda envs with only the minimum TF dependencies installed

Source code / logs
I've been able to distil the problem down to the following code which reliably reproduces the problem for me on my local hardware and also on a m5.2xlarge EC2 instance running Ubuntu 16.04 Server or Amazon Linux. The following code has no external data or code dependencies other than tensorflow.
The script has a single argument which sets the "vocabulary size" (this was originally an RNN LM); if this value is large enough a SegFault occurs. The only operation of note is the sparse_softmax_cross_entropy_with_logits.
#!/usr/bin/env python
from __future__ import absolute_import, division, print_function, unicode_literals
import sys
import tensorflow as tf

def main():
    vocabulary_size = int(sys.argv[1])
    batch_size = 256
    step_size = 32

    print("Vocabulary size:", vocabulary_size)

    labels = tf.get_variable("labels", shape=[batch_size, step_size], dtype=tf.int32)
    logits = tf.get_variable("logits", shape=[batch_size, step_size, vocabulary_size])
    costs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)

    with tf.Session() as session:
        session.run(tf.global_variables_initializer())

        print("Executing...")
        session.run(costs)
        print("SUCCESS!")


if __name__ == "__main__":
    main()