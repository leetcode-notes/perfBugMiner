Add float16 support for Elu, EluGrad ops

Support for tf.float16 dtype was recently added (#1300) to a bunch of ops, starting with matmul, conv2D and then many more. Can we add it for elu also please?
Looking at tensorflow/nn_ops.cc, I see that the Elu and EluGrad ops are registered for types {float, double} whereas for a whole bunch of other activation functions I see the type as T: realnumbertype or {half, float, double}.
To help with triaging, here's the paper that motivates elu activation: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
And FWIW @alexatknit in #1300 commented Re: looking forward to float16 support for elu too.