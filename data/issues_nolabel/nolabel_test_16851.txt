Output of Inceptionv3 slim 2016 tflite model is problematic

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A, this is a bug report of a tflite model released by Google
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any one
TensorFlow installed from (source or binary): both
TensorFlow version (use command below): after TF Lite released
Python version: both
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: see descriptions below

It seems the output of the inceptionv3 slim 2016 tflite model released by google is problematic. If you run label_image for tflite, you'll get
> ./label_image -m inceptionv3_slim_2016.tflite
Loaded model inceptionv3_slim_2016.tflite
resolved reporter
invoked 
average time: 1009.48 ms 
8.06111: 653 military uniform
6.19022: 668 mortarboard
5.83456: 401 academic gown
5.26993: 835 suit
4.80701: 855 theater curtain
If you convert InceptionV3 yourself,
> curl http://download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz \
 | tar -C /tmp -xzf -
> bazel run --config=opt   //tensorflow/contrib/lite/toco:toco --  \
--input_file=/tmp/inception_v3_2016_08_28_frozen.pb  \
--output_file=/tmp/inceptionv3.tflite   --input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,299,299,3 \
--input_array=input   --output_array=InceptionV3/Predictions/Reshape_1
and push the /tmp/inceptionv3.tflite to your android devices, then you can see expected results like
> ./label_image -m inceptionv3.tflite             
Loaded model inceptionv3.tflite
resolved reporter
invoked 
average time: 1020.93 ms 
0.496246: 653 military uniform
0.0764156: 668 mortarboard
0.0535454: 401 academic gown
0.030444: 835 suit
0.0191629: 855 theater curtain