Serialising checkpoints and models to byte arrays

System information

Have I written custom code: Yes
OS Platform and Distribution: Linux, Oracle Linux 7.4
TensorFlow installed from: source
TensorFlow version: 1.7 & 1.8
Python version: 2.7 & 3.6
Bazel version: 0.11.1
GCC/Compiler version: 4.8.5
CUDA/cuDNN version: CUDA 9.0, cuDNN 7
GPU model and memory: Titan X
Exact command to reproduce: N/A

Describe the problem
I'm wrapping Tensorflow in a higher level Java prediction API, using the provided Tensorflow Java API. This higher level API controls the serialisation of models in a uniform way, allowing a user to serialise models to disk, or over the network, and to load models as resources from jar files or non-disk sources.
I can train and test a model from Java, after creating the graph protobuf in Python, however the checkpointing and saved model functionality is extremely fragile as it requires specific structures in the filesystem. Currently I can either serialise a path to disk which means the model requires both the path to be valid as well as the serialised blob the high level API creates, or I can tar up the checkpoint/model bundle directory and serialise that as a byte array before untar-ing it to a temp directory and loading. Neither of these options are particularly satisfactory.
I had originally considered adding an extra couple of tensor endpoints to Saver which accepted a byte array wrapped in a tensor, deserialising that into the session, and one which emitted a byte array which was a serialised form of that session, but given how far down the assumption of filesystem access is in a saver I'm not sure that's the right approach.
I'm now thinking that adding another Saver class which has the byte array operations would be the best way to go. I'd like to contribute this back to tensorflow, so I thought it best to discuss the design first (and even if this kind of contribution is acceptable).
My ultimate goal is to add a Java implementation of freeze_graph.py, as that operation would make prediction time deployment much simpler, but there would still be a checkpointing issue while a model is training. Given #17390 I'm not sure if the Java API has enough operations implemented to freeze a model yet.