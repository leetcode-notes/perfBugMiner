Synchronized BatchNorm statistics across GPUs

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): n/a
TensorFlow installed from (source or binary): n/a
TensorFlow version (use command below): n/a
Python version:  n/a
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: n/a

Following the discussion with @isaprykin last Friday, there should be an option for batchnorm to compute the mean & variance over the entire batch across GPUs, when running with distribution strategy.
For typical image classification models it's not needed, however this is essential to other applications where batch-per-GPU is highly limited, such as object detection.
Some related papers:
https://arxiv.org/pdf/1711.07240.pdf
https://arxiv.org/abs/1803.08494