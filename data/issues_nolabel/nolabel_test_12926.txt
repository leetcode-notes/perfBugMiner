Elastic Average optimizer

I have implemented a new distributed optimizer "ElasticAverageOptimizer". It is mentioned in the following issue:
#12472
EASGD is an async optimizer. During the training, Each worker will update the local variables and maintains its own local_step, which starts from 0 and is incremented by 1 after each update of local variables. Whenever the communication period divides the local step, the worker requests the current global center variables and then computed the elastic difference between global center variables and local variables. The elastic difference then be used to update both local variables and global variables.
[Reference]: https://arxiv.org/pdf/1412.6651.pdf