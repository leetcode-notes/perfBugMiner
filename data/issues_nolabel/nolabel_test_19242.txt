Can TFLite treat input/output tensors as flat buffers just like TFMobile used to?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 17.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.7.0
Python version:  3.5.4
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce:  feature?

Describe the problem
TF Mobile allows treating input/output as flat arrays (e.g. float[]). Why does the TF Lite design deviate? Moreover, TF Lite treats inputs differently from outputs which feels odd, especially when chaining models.
Is there a way to get the old behavior back? Essentially, treating input/output tensor as flat buffers (appropriately sized) enables abstracting over the exact network input/output shapes in app code, making it more reusable. One can think of the network + shape as an existential package, which is quite convenient.
Source code / logs
TF Lite Android example requires instantiating float[][] which is hard-coding network output shape into the app.