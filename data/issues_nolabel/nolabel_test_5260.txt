Why the embedding_lookup() returns zeros when the index exceed embedding matrix size?

NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
I'm now using TF version 0.10.0 installed from source.
I'm not sure its a bug or intended implementation but the embedding_lookup() returns zeros when the index exceed embedding matrix size. For example,
 import tensorflow as tf
 import numpy as np

 embd_mat = np.linspace(1,10,10).reshape([10,1])*np.array([1,2,3]).reshape([1,3])
 idx = np.linspace(0,19,20)

 embd_in = tf.placeholder(tf.float32,[10,3])
 idx_in = tf.placeholder(tf.int32,[20])

 output = tf.nn.embedding_lookup(embd_in,idx_in)

 with tf.Session() as sess:
     sess.run(tf.initialize_all_variables())

     embd_out = sess.run(output,feed_dict={embd_in:embd_mat, idx_in:idx})

     print embd_out

The output of above code is,
[[  1.   2.   3.]
 [  2.   4.   6.]
 [  3.   6.   9.]
 [  4.   8.  12.]
 [  5.  10.  15.]
 [  6.  12.  18.]
 [  7.  14.  21.]
 [  8.  16.  24.]
 [  9.  18.  27.]
 [ 10.  20.  30.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]]

Is there any reason it returns zeros not raises an error?
I think that case happens only when programmers make a mistake.
Unless there is any specific reason which I don't know of, I think it should raise a value error that the index is exceeding the matrix size.