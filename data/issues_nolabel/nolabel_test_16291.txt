[bug] Specify GPU device error when using session_options.config.mutable_gpu_options()->set_visible_device_list

System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
session_options.config.mutable_gpu_options()->set_visible_device_list("0");
session->reset(tensorflow::NewSession(session_options)); // Error in this line
Status session_create_status = (*session)->Create(graph_def);


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04


TensorFlow installed from (source or binary):
source


TensorFlow version (use command below):
1.4.0


Python version:
2.7.12


Bazel version (if compiling from source):
0.9.0


GCC/Compiler version (if compiling from source):
5.4


CUDA/cuDNN version:
CUDA 8.0  /  cuDNN 6.0.21


GPU model and memory:
Quadro P6000, 24G GPU memory


Exact command to reproduce:


Describe the problem
I built the tensorflow C++ API from the source using Bazel 0.9.0. When I link the shared library libtensorflow_cc.so, my code works fine. (I do not need to link libtensorflow_framework.so, since I used '--config=monolithic' when I build tensorflow using bazel.)
However, I want to specify the GPU device in my code using this function to set gpu options:
session_options.config.mutable_gpu_options()->set_visible_device_list("0");
session->reset(tensorflow::NewSession(session_options)); // Error in this line
Source code / logs
Logs:
2018-01-22 09:39:57.262843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1202] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:02:00.0
totalMemory: 23.87GiB freeMemory: 22.46GiB
2018-01-22 09:39:57.262897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1296] Adding visible gpu device 0
2018-01-22 09:39:57.584754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21801 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-01-22 09:39:57.584802: E tensorflow/core/common_runtime/gpu/process_state.cc:130] Invalid allocator type: 0
Segmentation fault (core dumped)
Source Codes:
// Reads a model graph definition from disk, and creates a session object you
// can use to run it.
Status LoadGraph(const string& graph_file_name, std::unique_ptrtensorflow::Session* session)
{
tensorflow::GraphDef graph_def;
Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);
if (!load_graph_status.ok())
{
return tensorflow::errors::NotFound("Failed to load compute graph at '",
graph_file_name, "'");
}
//tensorflow::SessionOptions options;
tensorflow::SessionOptions session_options;
session_options.config.mutable_gpu_options()->visible_device_list();
std::cout<<"list GPU done"<<std::endl;
session_options.config.mutable_gpu_options()->set_visible_device_list("0");
std::cout<<"GPU assign is done"<<std::endl;
session->reset(tensorflow::NewSession(session_options));
std::cout<<"new session is created. "<<std::endl;
Status session_create_status = (*session)->Create(graph_def);
std::cout<<"Graph is loaded. "<<std::endl;
if (!session_create_status.ok()) 
{
    return session_create_status;
}
return Status::OK();

}