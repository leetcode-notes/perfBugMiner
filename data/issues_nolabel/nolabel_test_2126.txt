Auto device placement for distributed runtime

In a distributed TF setting, we need to place variables and ops to different devices. This is annoying to manually assign each variable and op, especially when we have GPU resources in our environment.
TF offer a context named tf.train.replica_device_setter which place variables to ps devices in round-robin manner, and this is helpful but not enough. @mrry can you shed some light on the auto distributed devices placement problem?