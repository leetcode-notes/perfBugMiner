Mean of cross entropy vs sum in summaries tutorial

The mnist_with_summaries.py and mnist_softmax.py tutorials compute the sum of the cross_entropy over the data points in the batch. This seems to deviate from the more common approach of taking the mean, such as in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py#L99
The current form works as a loss function, but might confuse newcomers who are figuring out how gradients are aggregated in TensorFlow (as experienced by me, see http://stackoverflow.com/questions/35731506/unaggregated-gradients-gradients-per-example-in-tensorflow :-))