syntaxnet: Global training is worse than greedy trainning when the training data size is 1 millon data

As you can see the following is my experimental configuration parameters and experiment results,  I'd like to know on a fairly large amount of data， how to adjust the super parameters can get a result that global better than greedy;
when the trainning data size is 20,000， We got the global trainning better than greedy trainning using default configuration parameters, But when the training data increased to 1 million， We can't reproduce the results;
This is our experimental result:

This is our experimental configuration:
greedy trainging parameters:

global trainging parameters:

I guess that's the parameter setting problem， Anyone who has experience can help me with this problem? Thanks