allocate output tensor for every operation before the first inference

Describe the problem
I was wondering if it's possible to allocate memory in advance, for internal output tensors of every operation in my inference workload.
I notice TF will automatically allocate (and de-allocate) memory for every internal result in a graph. If my inference workload repeats many times, all these allocations/deallocations will also repeat. Why don't we allocate these internal output tensors just once before the first inference run, and delete them after the last?
For example, I have the following workload (which will run thousands of times):
    M = tf.MatMul(A, B)
    D = tf.MatMul(M, C)

Is it possible to allocate M before the first run and use the memory for all inference? To avoid allocation of M in every single inference?