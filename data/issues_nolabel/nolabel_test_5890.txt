catch22 situation with tf.nn.sampled_softmax_loss and tf.nn.softmax

Hi,
referring to this:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.nn.sampled_softmax_loss.md
and this:
#4138
to make samlped_softmax efficient we need to invert w and w_t to take transpose out of the loss function.
but then it becomes an issue at test / inference time when you need to use the full softmax with the tranpose inside.
ie: either training is slow or testing is slow.
what's the solution ?