fix training_hook and training_chief_hook processing of distributed training

The changes are listed as followed.
"""
training_hooks = get_hooks_from_the_first_device(
    grouped_estimator_spec.training_hooks)
training_chief_hooks = get_hooks_from_the_first_device(
    grouped_estimator_spec.training_chief_hooks)
"""
training_hooks = [
    get_hooks_from_the_first_device(per_device_hook)
    for per_device_hook in grouped_estimator_spec.training_hooks
]
training_chief_hooks = [
    get_hooks_from_the_first_device(per_device_chief_hook)
    for per_device_chief_hook in grouped_estimator_spec.training_chief_hooks
]
grouped_estimator_spec.training_hooks is a list of PerDevice, and function get_hooks_from_the_first_device is only able to pick head item of one PerDevice, thus the old version doesn't work and keeps the PerDevice, leading to such errors:
TypeError: All hooks must be SessionRunHook instances, given: PerDevice:{'/job:localhost/replica:0/task:0/device:GPU:2': <XXXXXHook object at 0x7fc6f43f3080>, '/job:localhost/replica:0/task:0/device:GPU:0': <XXXXXHook object at 0x7fc71430bf60>, /job:localhost/replica:0/task:0/device:GPU:3': <XXXXXHook object at 0x7fc6b467e080>, '/job:localhost/replica:0/task:0/device:GPU:1': <XXXXXHook object at 0x7fc70430d048>}

(XXXXXHook inherits from SessionRunHook, and is hidden for some reason.)