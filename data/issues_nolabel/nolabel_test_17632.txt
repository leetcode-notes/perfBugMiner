got Nan when powered float32 tensors

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Windows 10 Pro
TensorFlow installed from (prebuild version here https://github.com/fo40225/tensorflow-windows-wheel  with avx2)
TensorFlow version 1.6.0
Python version 3.6.4:
Bazel version (if compiling from source)
GCC/Compiler version (if compiling from source)
CUDA 9.1.85/cuDNN 1.7.1 version
GPU Nvidia MX150 2GB
Exact command to reproduce:

Describe the problem
On my setup tensorflow get nan when I powered tensors with float32 type, but with float64 it's ok.
But if just product tensors by itself in float32 it's ok too.
Source code / logs
Scenario 1
X = tf.placeholder('float32')

x = np.linspace(-3, 3)

s = tf.Session()

X_pow2 = tf.pow(X, 2)
X_pow3 = tf.pow(X, 3)
X_prod = X*X

print('X_pow2 =', X_pow2.eval({X:x}, session=s), '\n')
print('X_pow3 =', X_pow3.eval({X:x},session=s),'\n')
print('X_prod =', X_prod.eval({X:x},session=s),'\n')

I get this
X_pow2 = [          nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
 3.7484397e-03 3.3735953e-02 9.3710966e-02 1.8367349e-01 3.0362350e-01
 4.5356098e-01 6.3348603e-01 8.4339863e-01 1.0832988e+00 1.3531864e+00
 1.6530614e+00 1.9829239e+00 2.3427739e+00 2.7326119e+00 3.1524367e+00
 3.6022491e+00 4.0820494e+00 4.5918374e+00 5.1316123e+00 5.7013750e+00
 6.3011241e+00 6.9308624e+00 7.5905871e+00 8.2803001e+00 9.0000000e+00] 

X_pow3 = [          nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
           nan           nan           nan           nan           nan
 2.2949636e-04 6.1964002e-03 2.8687032e-02 7.8717217e-02 1.6730276e-01
 3.0545944e-01 5.0420320e-01 7.7454978e-01 1.1275151e+00 1.5741148e+00
 2.1253648e+00 2.7922804e+00 3.5858786e+00 4.5171747e+00 5.5971832e+00
 6.8369217e+00 8.2474060e+00 9.8396511e+00 1.1624674e+01 1.3613489e+01
 1.5817106e+01 1.8246557e+01 2.0912844e+01 2.3826992e+01 2.7000002e+01] 

X_prod = [9.0000000e+00 8.2803001e+00 7.5905881e+00 6.9308619e+00 6.3011246e+00
 5.7013745e+00 5.1316123e+00 4.5918365e+00 4.0820489e+00 3.6022491e+00
 3.1524365e+00 2.7326117e+00 2.3427739e+00 1.9829239e+00 1.6530612e+00
 1.3531862e+00 1.0832986e+00 8.4339857e-01 6.3348603e-01 4.5356098e-01
 3.0362347e-01 1.8367347e-01 9.3710959e-02 3.3735946e-02 3.7484383e-03
 3.7484383e-03 3.3735946e-02 9.3710959e-02 1.8367347e-01 3.0362347e-01
 4.5356098e-01 6.3348603e-01 8.4339857e-01 1.0832986e+00 1.3531862e+00
 1.6530612e+00 1.9829239e+00 2.3427739e+00 2.7326117e+00 3.1524365e+00
 3.6022491e+00 4.0820489e+00 4.5918365e+00 5.1316123e+00 5.7013745e+00
 6.3011246e+00 6.9308619e+00 7.5905881e+00 8.2803001e+00 9.0000000e+00] 


Scenario 2
X = tf.placeholder('float64')

x = np.linspace(-3, 3)

s = tf.Session()

X_pow2 = tf.pow(X, 2)
X_pow3 = tf.pow(X, 3)

print('X_pow2 =', X_pow2.eval({X:x}, session=s), '\n')
print('X_pow3 =', X_pow3.eval({X:x},session=s),'\n')

get this
X_pow2 = [9.00000000e+00 8.28029988e+00 7.59058726e+00 6.93086214e+00
 6.30112453e+00 5.70137443e+00 5.13161183e+00 4.59183673e+00
 4.08204915e+00 3.60224906e+00 3.15243648e+00 2.73261141e+00
 2.34277384e+00 1.98292378e+00 1.65306122e+00 1.35318617e+00
 1.08329863e+00 8.43398584e-01 6.33486047e-01 4.53561016e-01
 3.03623490e-01 1.83673469e-01 9.37109538e-02 3.37359434e-02
 3.74843815e-03 3.74843815e-03 3.37359434e-02 9.37109538e-02
 1.83673469e-01 3.03623490e-01 4.53561016e-01 6.33486047e-01
 8.43398584e-01 1.08329863e+00 1.35318617e+00 1.65306122e+00
 1.98292378e+00 2.34277384e+00 2.73261141e+00 3.15243648e+00
 3.60224906e+00 4.08204915e+00 4.59183673e+00 5.13161183e+00
 5.70137443e+00 6.30112453e+00 6.93086214e+00 7.59058726e+00
 8.28029988e+00 9.00000000e+00] 

X_pow3 = [-2.70000000e+01 -2.38269854e+01 -2.09128424e+01 -1.82465554e+01
 -1.58171085e+01 -1.36134859e+01 -1.16246717e+01 -9.83965015e+00
 -8.24740542e+00 -6.83692169e+00 -5.59718315e+00 -4.51717397e+00
 -3.58587833e+00 -2.79228043e+00 -2.12536443e+00 -1.57411453e+00
 -1.12751490e+00 -7.74549720e-01 -5.04203181e-01 -3.05459460e-01
 -1.67302740e-01 -7.87172012e-02 -2.86870267e-02 -6.19639776e-03
 -2.29496213e-04  2.29496213e-04  6.19639776e-03  2.86870267e-02
  7.87172012e-02  1.67302740e-01  3.05459460e-01  5.04203181e-01
  7.74549720e-01  1.12751490e+00  1.57411453e+00  2.12536443e+00
  2.79228043e+00  3.58587833e+00  4.51717397e+00  5.59718315e+00
  6.83692169e+00  8.24740542e+00  9.83965015e+00  1.16246717e+01
  1.36134859e+01  1.58171085e+01  1.82465554e+01  2.09128424e+01
  2.38269854e+01  2.70000000e+01] 

So on more serious or complicated tasks this give unpredictable behavior and optimizers don't optimize, losses get NaN, everything going crazy or just NaN everywhere.
So I exactly had the same problem on tensorflow version 1.5.0 with CuDNN 7.0.5
And I can't understand, is it my setup and maybe videocard just bad, or is it really bug?