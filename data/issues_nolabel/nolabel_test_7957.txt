tf.contrib.layers.sparse_column_with_hash_bucket() lead to 'out of memory error' with large hash_bucket_size?

I'm using the wide_n_deep model on my own data, which has feature dimension about 100million. When I use the sparse_column_with_hash_bucket() and set the hash_bucket_size to 10million (because some categorical features have that much different values), the program always failed with the out of memory error.
So I wonder if tensorflow's wide_n_deep model could handles data with high dimension like the case above?