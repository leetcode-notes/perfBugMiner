Multi-GPU training drastically slow after recent commit

I recently upgraded to the latest master, in order to fix the issues caused by this bug: #7038
Though the above fix does resolve the issue of same batch being pulled from the GPU, now my model training has become drastically slow (> 4x), and most time is being spent in dequeues. A timeline snapshot is here:

Any ideas as to what might be going wrong?
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#7038
Environment info
Operating System: CentOS 6.5
Installed version of CUDA and cuDNN: 8.0.27 and 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):

The commit hash (git rev-parse HEAD): 084b37a
The output of bazel version

Build label: 0.4.3- (@non-git)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Dec 23 16:35:28 2016 (1482510928)
Build timestamp: 1482510928
Build timestamp as int: 1482510928

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
My code is based on the tensorflow/models/slim interface