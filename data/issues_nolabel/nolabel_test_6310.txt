Multiple GPUs: Out of Memory

Hi,
I used to train my model on a single GPU with a batch size of 20 (images) without a problem. Now I want to take advantage of multiple GPU training. Currently I'm using 4 GPUs and want to do data-parallel replicated training with a batch size of 20*4.
If I understood it correctly, each batch will be splitted into 4 equal parts (in this case for every model on each GPU, the batch size will be 20) and be feeded to each GPU.  But I'm getting out of memory error:
...
W tensorflow/core/common_runtime/bfc_allocator.cc:274] ********************************************************************************************
W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 1.32GiB.  See logs for memory state.
W tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[80,921600,6]
...
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[80,921600,6]

any suggestions will be greatly appreciated!