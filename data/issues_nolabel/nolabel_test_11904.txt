Unclear about how to integrate AttentionWrapper with BeamSearchDecoder

TF Version: 1.2.1
I cannot find information about how to integrate AttentionWrapper with BeamSearchDecoder on website / nmt tutorial, in particular how to feed the beam-search-tiled (tf.contrib.seq2seq.tile_batch) states to the attention-cloned (zero_state(...).clone(...)) states.
In my attempt, there seems to be an inconsistency between the requirement of the batch_size of the zero states generated by AttentionWrapper  in the training stage and predicting stage. In training stage, initial state of decoder requires batch size, however in predicting stage it requires (batch_size * beam_width).
the minimal code to demonstrate this problem is:
import tensorflow as tf
from tensorflow.python.layers.core import Dense

BEAM_WIDTH = 5
BATCH_SIZE = 128

# INPUTS
X = tf.placeholder(tf.int32, [BATCH_SIZE, None])
Y = tf.placeholder(tf.int32, [BATCH_SIZE, None])
X_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])
Y_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])

# ENCODER         
encoder_out, encoder_state = tf.nn.dynamic_rnn(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128), 
    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),
    sequence_length = X_seq_len,
    dtype = tf.float32)

# ATTENTION
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units = 128, 
    memory = encoder_out,
    memory_sequence_length = X_seq_len)

decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128),
    attention_mechanism = attention_mechanism,
    attention_layer_size = 128)

# DECODER COMPONENTS
Y_vocab_size = 10000
decoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))
projection_layer = Dense(Y_vocab_size)

# TRAINING DECODER
training_helper = tf.contrib.seq2seq.TrainingHelper(
    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),
    sequence_length = Y_seq_len,
    time_major = False)
training_decoder = tf.contrib.seq2seq.BasicDecoder(
    cell = decoder_cell,
    helper = training_helper,
    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),
    output_layer = projection_layer)
training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = training_decoder,
    impute_finished = True,
    maximum_iterations = tf.reduce_max(Y_seq_len))
training_logits = training_decoder_output.rnn_output

# PREDICTING_DECODER
predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
    cell = decoder_cell,
    embedding = decoder_embedding,
    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),
    end_token = 2,
    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(
                    cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, BEAM_WIDTH)),
    beam_width = BEAM_WIDTH,
    output_layer = projection_layer,
    length_penalty_weight = 0.0)
predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = predicting_decoder,
    impute_finished = False,
    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))
predicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]
the error message is:
Traceback (most recent call last):
  File "test.py", line 58, in <module>
    initial_state = decoder_cell.zero_state(BATCH_SIZE*BEAM_WIDTH,tf.float32).clone(
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", line 659, in zero_state
    message=error_message)]):
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py", line 317, in assert_equal
    _assert_static(condition_static, data)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py", line 100, in _assert_static
    raise ValueError('\n'.join(data_static))
ValueError: When calling zero_state of AttentionWrapper attention_wrapper: Non-matching batch sizes between the memory (encoder output) and the requested batch size.  Are you using the BeamSearchDecoder?  If so, make sure your encoder output has been tiled to beam_width via tf.contrib.seq2seq.tile_batch, and the batch_size= argument passed to zero_state is batch_size * beam_width.
Condition x == y did not hold element-wise:
x (AttentionWrapperZeroState_1/assert_equal/x:0) = 
640
y (AttentionWrapperZeroState_1/assert_equal/y:0) = 
128