MemoryError from tensorflow.contrib.learning.datasets in Python3

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0
Python version: 3.5.3
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce: python -c "import tensorflow as tf; tf.contrib.learn.datasets.load_dataset('dbpedia', size='full')"

Describe the problem
The command results in an MemoryError, even though the dataset easily fits into my memory (64GB), and also works with Python 2.
Source code / logs
Here is the traceback:
Traceback (most recent call last):
File "", line 1, in 
File "/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/init.py", line 71, in load_dataset
return DATASETS[name](size, test_with_fake_data)
File "/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py", line 65, in load_dbpedia
train_path, target_dtype=np.int32, features_dtype=np.str, target_column=0)
File "/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py", line 72, in load_csv_without_header
data = np.array(data)
MemoryError