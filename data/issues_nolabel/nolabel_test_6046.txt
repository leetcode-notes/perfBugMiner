ResourceExhaustedError when save model. Both memory and disk are enough

Hi,
I was running tensorflow 0.12.0rc0 on ubuntu 14.04. The training is on CPU only. The crash happened when saver saves the model.

W tensorflow/core/framework/op_kernel.cc:975] Resource exhausted:/home/code/2016_11_28_90d_strict_win10.emb-354830.data-00000-of-00001
Traceback (most recent call last):
File "/home/code/tf_train.py", line 474, in 
save_model_and_log_step(tf_saver=saver, tf_session=session, global_step=step)
File "/home/code/tf_train.py", line 199, in save_model_and_log_step
tf_saver.save(tf_session, FLAGS.out_model_path, global_step=global_step)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1323, in save
{self.saver_def.filename_tensor_name: checkpoint_file})
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 766, in run
run_metadata_ptr)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 964, in _run
feed_dict_string, options, run_metadata)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1014, in _do_run
target_list, options, run_metadata)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1034, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: ~/2016_11_28_90d_strict_win10.emb-354830.data-00000-of-00001
[[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_r$
cv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, embeddings, embeddings/Adagrad, sm_b, sm_b/Adagrad, sm_w_t, sm_w_t/Adagrad)]]
Caused by op u'save/SaveV2', defined at:
File "/home/code/train.py", line 430, in 
saver = tf.train.Saver(max_to_keep=FLAGS.max_to_keep)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1000, in init
self.build()
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1030, in build
restore_sequentially=self._restore_sequentially)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 622, in build
save_tensor = self._AddSaveOps(filename_tensor, saveables)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 229, in _AddSaveOps
save = self.save_op(filename_tensor, saveables)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 172, in save_op
tensors)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py", line 552, in save_v2
tensors=tensors, name=name)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 759, in apply_op
op_def=op_def)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2240, in create_op
original_op=self._default_original_op, op_def=op_def)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1128, in init
self._traceback = _extract_stack()
ResourceExhaustedError (see above for traceback): ~/2016_11_28_90d_strict_win10.emb-354830.data-00000-of-00001
[[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_re
cv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, embeddings, embeddings/Adagrad, sm_b, sm_b/Adagrad, sm_w_t, sm_w_t/Adagrad)]]

No OOM or OOD happened on the machine. I remember this was a bug in 0.9.0 when saving a large tensor it has tensor size too large (>2G) problem because of a proto field is defined as int32. I am not sure if it's related?