const variable placement issue in distributed tensorflow

Describe the problem
In my model, I use the FTRL Optimizer like below:
self.optimizer = tf.train.FtrlOptimizer(0.005, learning_rate_power=-0.5, initial_accumulator_value=0.1, l1_regularization_strength=1.0, l2_regularization_strength=0.00001)
Inside the FtrlOptimizer it will create several const variables for the parameters, such as learning rate, learning rate power, etc.
When I run the distributed tensorflow job, from the timeline I can see that for each session run I can see that the worker will send the above const variables to all the ps nodes. This is a cost since the variables are const and not needed to sent to ps nodes repeatedly.
I was wondering is there a way to pin those const variables to the ps and save the transferring cost during each session run.
Thanks.