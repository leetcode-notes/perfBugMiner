the embedding_lookup() returns zeros when the index exceed embedding matrix size?

NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
I wrote an issues a month ago about the same problem #5260
I recently update the TF to latest master branch(0.11.head) without any custom modification and the commit hash is 55dbc54
The tf.embedding_lookup() still do not raise error even though the index is exceeding the embedding matrix size. It automatically fills zeros as you can see in the below example.
I tested same code with two different machines and the result was same.
In the previous issue(#5260), @strategist333 said it raise an InvalidArgumentError in binary release.
But when I tested TF installed from source, it didn't...
 import tensorflow as tf
 import numpy as np

 embd_mat = np.linspace(1,10,10).reshape([10,1])*np.array([1,2,3]).reshape([1,3])
 idx = np.linspace(0,19,20)

 embd_in = tf.placeholder(tf.float32,[10,3])
 idx_in = tf.placeholder(tf.int32,[20])

 output = tf.nn.embedding_lookup(embd_in,idx_in)

 with tf.Session() as sess:
     sess.run(tf.initialize_all_variables())

     embd_out = sess.run(output,feed_dict={embd_in:embd_mat, idx_in:idx})

     print embd_out

The output of above code is,
[[  1.   2.   3.]
 [  2.   4.   6.]
 [  3.   6.   9.]
 [  4.   8.  12.]
 [  5.  10.  15.]
 [  6.  12.  18.]
 [  7.  14.  21.]
 [  8.  16.  24.]
 [  9.  18.  27.]
 [ 10.  20.  30.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]
 [  0.   0.   0.]]