Add optimizers with decoupled weight decay.

This pull request implements decoupled weight decay as described in 'Fixing Weight Decay Regularization' by  Loshchilov & Hutter https://arxiv.org/abs/1711.05101.
This paper shows that for adaptive gradient algorithms, the implemented method regularizes variables with large gradients more than L2 regularization would and that this  yields better training loss and generalization error.
For SGD variants, this simplifies hyperparameter search since it decouples the settings of weight decay and learning rate, which is nicely visualized in Fig. 2 in the paper:

This implementation explicitly adds the optimizers described in the paper (AdamW and MomentumW) to tf.contrib.opt and provides a factory function extend_with_decoupled_weight_decay that can be used to create a new optimizer class with decoupled weight decay.
Closes #15237.