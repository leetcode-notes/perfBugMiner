Dynamic Partition Gradient

For bugs/issues, please fill in the following.  The more information you
provide, the more likely we can help you.
Environment info
Operating System: Linux (CentOS) cluster, custom build. tensorflow built from source by IT folks managing the cluster.
If installed from sources, provide the commit hash:
Steps to reproduce


The functions of interest contain many dynamic_partitions


I then compute the gradient with respect to the variables for optimization


I obtain the following error
train_op = optimizer.minimize(loss_instance, global_step=global_step)
File "/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py", line 186, in minimize
aggregation_method=aggregation_method)
File "/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py", line 232, in compute_gradients
aggregation_method=aggregation_method)
File "/share/sw/free/tensorflow/0.6.0/tensorflow/python/ops/gradients.py", line 426, in gradients
(op.name, op.type))
LookupError: No gradient defined for operation 'DynamicPartition_1' (op type: DynamicPartition)


What have you tried?

I am currently trying to see if I can remove the dynamic partition from my code, but it would be nice if we could keep the dynamic partition inside; however, I understand why the dynamic partition might not be differentiable if variable indices are used. In my case, I just have a constant placeholder that performs that indexing. Could there be a way to compute gradients in this case? Thank you so much!