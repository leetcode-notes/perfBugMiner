Float16 (half or Eigen::half) for conv3d ops

Registrations of conv3d operations with fp16,  fp16 for batch_norms in tf.layers and tf.contrib.layers.  Related issue: #11341
I don't understand how fp16 operations must be implemented in low-level with CUDA and I didn't make any additional optimizations.
There is no implementation for fused batch_norm yet.
I copied part of code for dtypes from conv2d test to conv3d test and seems like all works.
With fp16 I get inf and large absolute errors with large numbers. I suppose that it's fine. At least CPU and GPU  implementations return almost similar values. I skip such cases in test.
Example of such case:

use_gpu: False
dtype: <dtype: 'float32'>
data_format: NDHWC
expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]
actual =  [[[[[  36564.   38022.   39480.]
[  37824.   39354.   40884.]
[  39084.   40686.   42288.]]
[[  46644.   48678.   50712.]
[  47904.   50010.   52116.]
[  49164.   51342.   53520.]]]
[[[ 107124.  112614.  118104.]
[ 108384.  113946.  119508.]
[ 109644.  115278.  120912.]]
[[ 117204.  123270.  129336.]
[ 118464.  124602.  130740.]
[ 119724.  125934.  132144.]]]]]
use_gpu: False
dtype: <dtype: 'float16'>
data_format: NDHWC
expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]
actual =  [[[[[ 36544.  38016.  39488.]
[ 37824.  39360.  40896.]
[ 39072.  40704.  42304.]]
[[ 46656.  48672.  50688.]
[ 47936.  50016.  52128.]
[ 49184.  51328.  53536.]]]
[[[    inf     inf     inf]
[    inf     inf     inf]
[    inf     inf     inf]]
[[    inf     inf     inf]
[    inf     inf     inf]
[    inf     inf     inf]]]]]
fp16 using may result in inf values and large absolute errors when used with large numbers, skipping
use_gpu: True
dtype: <dtype: 'float32'>
data_format: NDHWC
expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]
actual =  [[[[[  36564.   38022.   39480.]
[  37824.   39354.   40884.]
[  39084.   40686.   42288.]]
[[  46644.   48678.   50712.]
[  47904.   50010.   52116.]
[  49164.   51342.   53520.]]]
[[[ 107124.  112614.  118104.]
[ 108384.  113946.  119508.]
[ 109644.  115278.  120912.]]
[[ 117204.  123270.  129336.]
[ 118464.  124602.  130740.]
[ 119724.  125934.  132144.]]]]]
use_gpu: True
dtype: <dtype: 'float16'>
data_format: NDHWC
expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]
actual =  [[[[[ 36576.  38016.  39488.]
[ 37824.  39360.  40896.]
[ 39072.  40672.  42304.]]
[[ 46656.  48672.  50720.]
[ 47904.  50016.  52128.]
[ 49152.  51328.  53504.]]]
[[[    inf     inf     inf]
[    inf     inf     inf]
[    inf     inf     inf]]
[[    inf     inf     inf]
[    inf     inf     inf]
[    inf     inf     inf]]]]]
fp16 using may result in inf values and large absolute errors when used with large numbers, skipping

fp16 is not fully covered by tests because I not sure how to do it.