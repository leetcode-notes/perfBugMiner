Training accuracy falling down after some epoches since upgrading to 0.8.0rc0

I'm trying to use a simple network to classify some 64*64 images. in v0.7 it worked fine and consistent. The training phase was showing a consistent  average growth in accuracy. But since I'm trying to use 0.8, the accuracy fall down occasionally from ~0.9 to ~0!
I'm using cpu  version for python 2.7
is it a bug in TF or I'm doing something wrong?

Environment info
Operating System: Ubuntu 14.04
I've installed pip with the following command : sudo apt-get install python-pip python-dev

The output from python -c "import tensorflow; print(tensorflow.version)".
0.8.0rc0
#############################################
my learning code is as below:


sess = tf.InteractiveSession()

x = tf.placeholder(tf.float32, shape=[None, 64,64])
y_ = tf.placeholder(tf.float32, shape=[None, 31])
input = tf.reshape(x,shape=[-1,64*64])
w1 = tf.Variable(tf.random_uniform([64*64,128],minval=-0.1,maxval=0.1),dtype=tf.float32)
b1 = tf.Variable(tf.random_uniform([128],minval=-0.1,maxval=0.1),dtype=tf.float32);
y1_ = tf.nn.softmax(tf.matmul(input, w1) + b1)
keep_prob = tf.placeholder(tf.float32)
y1 = tf.nn.dropout(y1_, keep_prob)
w2 = tf.Variable(tf.random_uniform([128,31],minval=-0.1,maxval=0.1),dtype=tf.float32)
b2 = tf.Variable(tf.random_uniform([31],minval=-0.1,maxval=0.1),dtype=tf.float32);
y2 = tf.nn.softmax(tf.matmul(y1, w2) + b2)
cross_entropy = -tf.reduce_sum(y_*tf.log(y2))
learn_rate = tf.placeholder(tf.float32)
train_step = tf.train.AdamOptimizer(learn_rate).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
sess.run(tf.initialize_all_variables())
d = Dataset()
dat = d.load()
for i in range(9000):
    if i%200==0:
        a = accuracy.eval(feed_dict={x: dat.data, y_: dat.label,keep_prob: 1.0,learn_rate:1e-2})
        print("step %d, training accuracy %g" % (i, a))
    train_step.run(feed_dict={x: dat.data, y_: dat.label,keep_prob: 0.8 ,learn_rate:1e-2})

for i in range(5000):
    if i%100==0:
        a = accuracy.eval(feed_dict={x: dat.data, y_: dat.label,keep_prob: 1.0,learn_rate:1e-3})
        print("step %d, training accuracy %g" % (i, a))
    train_step.run(feed_dict={x: dat.data, y_: dat.label,keep_prob: 0.8,learn_rate:1e-3 })


some line of the output is as below:
step 7400, training accuracy 0.870539
step 7600, training accuracy 0.875406
step 7800, training accuracy 0.877677
step 8000, training accuracy 0.882868
step 8200, training accuracy 0.884491
step 8400, training accuracy 0.887735
step 8600, training accuracy 0.888709
step 8800, training accuracy 0.893251
step 0, training accuracy 0.894549
step 100, training accuracy 0.895198
step 200, training accuracy 0.894873
step 300, training accuracy 0.895198
step 400, training accuracy 0.894873
step 500, training accuracy 0.895198
step 600, training accuracy 0.895198
step 700, training accuracy 0.895522
step 800, training accuracy 0.895522
step 900, training accuracy 0.895847
step 1000, training accuracy 0.896171
step 1100, training accuracy 0.896171
step 1200, training accuracy 0.896496
step 1300, training accuracy 0.89682
step 1400, training accuracy 0
step 1500, training accuracy 0
step 1600, training accuracy 0
step 1700, training accuracy 0