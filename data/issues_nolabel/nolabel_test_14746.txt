How to use `tf.nn.dropout` to implement embedding dropout

Recent papers in language modeling use a specific form of embedding dropout that was proposed in this paper. The paper also proposed variational recurrent dropout which was discussed already in this issue.
In embedding dropout, the same dropout mask is used at each timestep and entire words are dropped (i.e. the whole word vector of a word is set to zero). This behavior can be achieved by providing a noise_shape to tf.nn.dropout.  In addition, the same words are dropped throughout a sequence:
"Since we repeat the same mask at each time step, we drop the same words throughout the sequence – i.e. we drop word types at random rather than word tokens (as an example, the sentence “the dog and the cat” might become “— dog and — cat” or “the — and the cat”, but never “— dog and the cat”). "
I couldn't find a way to implement this functionality of embedding dropout efficiently. Are there any plans to incorporate these advances?