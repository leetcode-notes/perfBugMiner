Matrix Vector multiply not parallelized.

What is the problem?
Matrix vector multiply is not parallelized. Please see my example code. No matter how I change the intra_op_parallelism_threads, the running time is always similar. I used "top" to confirm that only one thread was used. However, the parallel speedups for square matrix matrix multiply are quite noticeable. Again, "top" confirmed that multiple threads were used.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

A link to the pip package you installed:
The output from python -c "import tensorflow; print(tensorflow.__version__)".

If installed from source, provide

The commit hash (git rev-parse HEAD)
2e22f1b
The output of bazel version
0.4.3

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import tensorflow as tf
import numpy as np
import time
n = 10000
#approach 1:
matrix1 = tf.constant(np.ones(nn), shape = [n,n])
matrix2 = tf.constant(np.ones(n1), shape = [n,1])
product1 = tf.matmul(matrix1, matrix2)
start = time.time()
sess = tf.Session(config=tf.ConfigProto(
inter_op_parallelism_threads=1,
intra_op_parallelism_threads=12))
sess.run(product1)
end = time.time()
print('\n Approach 1 took: %s%%' % (end - start))
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).