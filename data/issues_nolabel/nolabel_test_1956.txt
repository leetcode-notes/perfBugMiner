Why rnn final state shape[0] is uncertain when has sequence_length?

inputs = [tf.placeholder(tf.float32, [batch_size])] * 12
initial_state = cell.zero_state(batch_size, tf.float32)
outputs, final_state = rnn.rnn(cell, inputs, initial_state=initial_state, sequence_length=early_stop)
Why the final_state shape is Tensor(... shape=(?, 256)), instead of batch number?