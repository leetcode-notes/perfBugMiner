Understanding LSTM cell Kernel values

Hi all,
I want to understand better those values in LSTM cell Kernel values extracted from:
<tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(97, 280) dtype=float32_ref>
from tf.trainable_variables()
My model is very simple, input is a 27 element vector at each time step where the sequence length can be variable. The model is one layer LSTM model with 70 hidden states.
        def lstm_cell():
            cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)
            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropKeepRate)
            return cell
        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)], state_is_tuple=True)
        

So that is probably why the LSTM kernel is a matrix of 97 rows (27 input features and 70 hidden states). As to the columns, I can clearly see four groups by the pattern of values. I guess each group is consist of 70 columns, so totally 4 groups.
So my guess is that this LSTM kernel, extracted from tf.trainable_variables(), maps input and previous hidden states to current hidden states in order of forget, input, update cell states and output. However, it is hard time for me to figure out the correspondence of these gates and their matrices with the Kernel matrix extracted from tf.trainable_variables().
Could somebody help? Thanks!

Update:
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
TensorFlow installed from (source or binary): from pip
TensorFlow version (use command below): 1.3
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 8.0, 6.1
GPU model and memory: k2200