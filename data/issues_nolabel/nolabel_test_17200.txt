TimeDistributed wrapper works with "standalone" Keras, but not with "built-in" Keras

The following lines worked with Keras (2.0.8) and Tensorflow (pre 1.4).
input_tensor = Input((input_epochs_per_output_epoch, samples_per_epoch, features_per_epoch))
inner_intermediate = TimeDistributed(BatchNormalization(axis = -1), name="bn_input")(input_tensor)
many lines...
inner_out = TimeDistributed(Bidirectional(GRU(intermediate_features, return_sequences=False, recurrent_dropout=recurrent_dropout, dropout=normal_dropout, kernel_initializer=kernel_initializer, kernel_regularizer=l2(l2_lambda), implementation=lstm_implementation), merge_mode='concat'), name="inner_out")(inner_intermediate)
However, with Tensorflow 1.5.0, using the Keras within Tensorflow, the final line fails with:

ValueError: as_list() is not defined on an unknown TensorShape.

Because there are a lot of lines between 2 and 3, which have various layers wrapped by TimeDistributed, I've figured out that it is only GRU and LSTM layers that cause this error. I've tried specifying the input_shape for both the the wrappers and/or the layers themselves, to no avail.