Errors while running Translate.py sample code with multiple GPUS

GitHub issues are for bugs / installation problems / feature requests.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
Environment info
Operating System: Ubuntu
Installed version of CUDA and cuDNN: 7.5 and 7
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

Which pip package you installed.
The output from python -c "import tensorflow; print(tensorflow.__version__)". 0.8

If installed from sources, provide the commit hash:
Steps to reproduce

running the translate.py example
2.Crashes while creating models after tokenizing the training data.

The sample code seems to run fine with single GPU of 4 GB memory. I am using AWS GPU instances. But it works fine for basic values, ie vocab size of 40000 and size of 512. I want to train on a vocab size of 500000 and size of 1024. This is the reason why I opted to go for 4 GPUs now.
(In AWS terms, g2.2xlarge to g2.8xlarge).


Logs or other output that would be helpful
(If logs are large, please upload as attachment).