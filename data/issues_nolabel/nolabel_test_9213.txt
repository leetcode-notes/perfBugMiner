System hangs when computing gradients on GPU

Problem description
My system hangs when computing gradients on the GPU.  I am able to compute gradients on the CPU without issue.  I can compute all the nodes in my graph, including my loss function, on the GPU without issue.  My system hangs when computing gradients on the GPU irrespective of which optimizer I use.  The code that produces this issue is:
sess = tf.Session()

# ...

cnn = CNNClass()

#...

step_size = tf.placeholder(tf.float32, name="step_size") 
global_step = tf.Variable(0, name="global_step", trainable=False) 
optimizer = tf.train.MomentumOptimizer(step_size, 0.9)

#...

# CRASHES WHEN I RUN THIS
grads_and_vars = optimizer.compute_gradients(cnn.loss)
sess.run([grads_and_vars], feed_dict)

# DOES NOT CRASH WHEN I RUN THIS
sess.run([cnn.loss])

Further information
I tried setting gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333) in my ConfigProto but that didn't help.
Launching watch -n 0.1 nvidia-smi before running the code shows Volatile GPU-Util goes to 100% just before hanging.
Computing gradients on the CPU and printing them out shows normal output (I'm not getting any NaNs or 0's or anything like that).
System information

System: Ubuntu 16.04
TensorFlow installed from: binary
TensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1
CUDA version: 8.0, V8.0.61
cuDNN version: 5.1
GPU model and memory: NVidia GeForce GTX TITAN X (12GB)
Video card driver: 375.39