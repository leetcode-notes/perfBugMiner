[Bug] LuongMonotonicAttention in contrib/seq2seq/python/ops/attention_wrapper.py

LuongMonotonicAttention.__init__(...) calls its parent _BaseAttentionMechanism with query_layer as follows:
        query_layer=layers_core.Dense(
            num_units, name="query_layer", use_bias=False),

But, it doesn't apply it on query in LuongMonotonicAttention.__call__(...).
  def __call__(self, query, previous_alignments):
    """...
    """
    with variable_scope.variable_scope(None, "luong_monotonic_attention",
                                       [query]):
      score = _luong_score(query, self._keys, self._scale)
      score_bias = variable_scope.get_variable(
          "attention_score_bias", dtype=query.dtype,
          initializer=self._score_bias_init)
      score += score_bias
    alignments = self._probability_fn(score, previous_alignments)
    return alignments

Guessing from the way LuongAttention works, there should be query_layer=None in LuongMonotonicAttention.__init__(...).