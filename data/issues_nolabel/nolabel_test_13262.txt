Training Mini-batches of data (without labels) for unsupervised learning problem

Hi Everyone,
Has anyone trained mini-batches of data for an unsupervised learning problem?
The feed_dict uses the label and in an unsupervised setting. How do you overcome that? Could we use fake labels that never contribute to the loss function?
Basically, I want to iterate over my huge dataset and then optimize a custom loss function. However, I couldn't figure out how to retain my training parameters (weights) when using a new mini-batch from the data explicitly.
For example, the whole dataset is 6000 points and the mini-batch size is 600.
Currently, for every mini-batch I could only use new independent weight parameters because the weights are initialized based on the data points from this mini-batch. When we optimize the loss over the first mini-batch of 600 data points, we get some optimized weights. How does one use these weights to optimize the next mini-batch of 600 data points and so on. The problem is we cannot use a shared global variable.
Any help or pointers in this regard would be really appreciated. Thanks in advance!