Training time for CIFAR-10 is higher on multi GPU cards compared to single

When running CIFAR-10 example on two GPU cards in a single machine, the training time is higher compared to a single GPU card.
It is utilizing both cards but overall execution time is higher compared to single GPU.

One more thing I noticed is that the queue is filled twice. I got following messages in log.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Ideally, It should be filled once and distribute across two GPUs but that is not happening.
Could anyone please help me out to resolve this?