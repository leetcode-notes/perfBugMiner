Enhancement: Better Model Saving & Loading

Please go to Stack Overflow for help and support:
http://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug or a feature request.
The form below must be filled out.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.something
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: Feature Request; not bug

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Much of it is documented here in depth; especially the pain part: http://stackoverflow.com/questions/43708616/tensorflow-inference
I would really really appreciate a way to save JUST the model and load JUST the model as a graph.  I might even be able to help in coding this feature.
You can do it however you want; but the easiest might be to use a scoping prefix such as 'model/stuff'  I would very much like to do this:
def inference(X):
create a model
return prediction_op
do your training however; with TFRecords; with feed dicts; with your custom queue runners etc...There are a million options...
tf.train.ModelSaver(SCOPE_PARENT, PATH)
where SCOPE_PARENT is the top level scope to save; so if I structure my scoping as such:
'inputs/inputstuffs'
'model/layer1' , 'model/layer2' etc etc
'adams etc'
it would save just 'model/layer1' (but not any of the gradient stuff...)
then at inference time I can do...
model = tf.train.ModelLoader(PATH)
that model takes the initial input of the first layer of that graph; whatever that is.  Most commonly for inference I see feed_dict.
so I can set up whatever kind of processing I want; either in memory based with feed_dicts or perhaps batch processing via queue runners; but the point is that the input is seperate from the model.
As it stands right now; I'm looking at TensorFlow for our use case as a non starter if the training input system is really this tightly bound to the inference system.  This type of thing really needs to be built...
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.