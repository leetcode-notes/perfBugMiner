Distributed tensorflow - possible memory leak on Linux

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.1.rc2 (v1.1.0-rc2-675-gd0042ed)
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: 8/5.0
GPU model and memory: Geforce670 2GB (GPU not used in example)
Exact command to reproduce:

import tensorflow as tf
server = tf.train.Server.create_local_server()
session = tf.Session(server.target)
c = tf.constant("Hello, distributed TensorFlow!")
init = tf.global_variables_initializer()
session.run(init)
session.graph.finalize()
with session as sess:
 while True:
    out = sess.run(c)
Describe the problem
Running the above example, memory usage keeps slowly increasing. If the session is initialized as non-distributed with:
session = tf.Session()
The amount of memory used remains stable. This is tested on several machines both with binary and source code installations. Could someone else verify, if this is a problem with tensorflow.