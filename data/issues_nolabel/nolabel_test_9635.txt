Implement of attention mechanisms

Here is a question(maybe bug) about implements of different attention mechanisms, i.e. LuongAttention & BahdanauAttention, in contrib/seq2seq/python/ops/attention_wrapper.py. It seems that the only difference is their score functions(alignment models).
In BahdanauAttention, the alignment model is as described in https://arxiv.org/abs/1412.7449
 score = math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query),
                                    [2])

In LuongAttention, the alignment model is a dot function:
score = math_ops.matmul(query, self.keys, transpose_b=True)
Then, the context vector is used all in same way. However, there should be more differences between these two attention mechanisms. Is this a modified version of Bahdanau attention proposed in https://arxiv.org/abs/1409.0473?
Any response would be appreciated. Thanks!