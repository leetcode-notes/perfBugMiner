Sudden slowdowns due to the TLB shootdowns

Environment info
Operating System: Arch Linux
Installed version of CUDA and cuDNN: CUDA 7.5, CUDNN v4
If installed from binary pip package, provide:

Which pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.version)": 0.8.0

Steps to reproduce
I am experiencing slowdowns due to some problem with memory caching. My system has 32GB of RAM. I am training a CNN on one large image dataset. During training at some point almost 100% of memory is cached which is expected when working with a large dataset. But then soon after TF starts to experience slowdowns (3-5x slower execution per batch). I used netdata and observed that the slowdowns are accompanied by a sudden huge spikes in TLB shootdowns. I found a way how to fix a problem from outside, the slowdowns disappear for quite some time if I just clear all the cached memory like this:
$ free && sync && echo 3 > /proc/sys/vm/drop_caches && free
              total        used        free      shared  buff/cache   available
Mem:       32825492     3454324      209124      638072    29162044    28463116
              total        used        free      shared  buff/cache   available
Mem:       32825492     3455692    28603760      637636      766040    28476376


Then later when memory is fully cached again they reappear. First row is the state at which the slowdown is happening. You can see that most memory is cached and I am actually using only around 10% of RAM.
Does anyone have an idea what could cause these TLB shootdowns or what can I do to find out more about what is going on?