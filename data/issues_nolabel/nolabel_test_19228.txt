optimize_for_inference.py has conversion error for float16 Conv2D

System information

Have I written custom code: No
OS Platform and Distribution: Linux Mint 18.3
TensorFlow installed from: source
TensorFlow version: v1.8.0
Python version: 3.5.2
Bazel version (if compiling from source): 0.13.0
GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
CUDA/cuDNN version: Cuda compilation tools, release 9.1, V9.1.85
GPU model and memory: 1080 Ti

Describe the problem
optimize_for_inference.py has conversion error for float16 Conv2D.
I have a model with all floats as float16. After optimization by optimize_for_inference.py and then loaded in C++, the following error occurs.
Source code / logs
Invalid argument: No OpKernel was registered to support Op 'FusedPadConv2D' with these attrs.  Registered devices: [CPU,GPU,XLA_CPU,XLA_GPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]

	 [[Node: Conv2D = FusedPadConv2D[T=DT_HALF, mode="REFLECT", padding="VALID", strides=[1, 1, 1, 1]](MirrorPad_5, MirrorPad_6/paddings, Deep-Q-learning/conv1)]]

The format of the output of Registered kernels is so weird.
I have read the source code of optimize_for_inference.py but found no clue.