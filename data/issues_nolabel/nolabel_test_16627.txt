TF consumes all available RAM with a particular combination of conv2d, batch_norm and LSTM

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.5.0-0-g37aa430d84 1.5.0
Python version: 3.6.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0.176 / 7.0.5
GPU model and memory: Tesla V100 (AWS P3), 16GB
Exact command to reproduce: python debug_tf.py

Describe the problem
When the following code is run on a p3.2xlarge, the python process starts consuming RAM indefinitely, until the entire server RAM is used and the server dies. That's system RAM, not GPU memory.
It doesn't look it, but the code below is the smallest I could find that produces the bad behavior.

Replacing residual_conv with a simple convolution makes the code work (i.e not hang).
Reducing the repetition number in layers.repeat (to e.g 2) makes the code work.
Removing the batch normalization from residual_conv makes the code work.
Removing the LSTM makes the code work.
The weirdest of all, if I set is_training=True in batch_norm instead of is_training=is_training_var whose value is set to True in the feed_dict, then the code works.

The same code runs successfully on a AWS P2 server with TensorFlow 1.3.
Source code / logs
import numpy as np
import tensorflow as tf
import tensorflow.contrib.layers as layers
from tensorflow.contrib.framework import arg_scope


def residual_conv(incoming, num_filters, scope, bn=True):
	with tf.variable_scope(scope):
	    input_filters = incoming.get_shape().as_list()[-1]
	    if input_filters != num_filters:
	        incoming = layers.conv2d(incoming, num_filters, scope='adjust_conv')

	    after_conv1 = layers.conv2d(incoming, num_filters)
	    after_conv2 = layers.conv2d(after_conv1, num_filters, normalizer_fn=None, activation_fn=None)

	    net = incoming + after_conv2

	    if bn:
	        net = layers.batch_norm(net)

	    return net

def tf_bilstm(incoming, n_units, name):
	net = incoming

	lstm_f = tf.contrib.rnn.LSTMCell(n_units)
	lstm_b = tf.contrib.rnn.LSTMCell(n_units)

	with tf.variable_scope(name):
	    results, _ = tf.nn.bidirectional_dynamic_rnn(lstm_f, lstm_b, net, 
	                                                 dtype=tf.float32, time_major=True)
	return tf.concat(results, axis=2)

def main():
	x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))
	is_training_var = tf.placeholder(dtype=bool)

	net = x_var

	with arg_scope([layers.batch_norm], is_training=is_training_var, decay=0.99, scale=True
	               ), \
	     arg_scope([layers.conv2d], padding='SAME', kernel_size=(3, 3)), \
	     arg_scope([layers.max_pool2d], padding='SAME', kernel_size=(2, 2), stride=(2, 2)):

	    net = layers.repeat(net, 20, residual_conv, 64, scope='block1')
	    net = layers.max_pool2d(net)

	    net = tf.squeeze(net, 2)
	    net = tf.transpose(net, [1, 0, 2])
	    net = tf_bilstm(net, 512, 'lstm1')

	sess = tf.Session()
	sess.run(tf.global_variables_initializer())
	
	X = np.zeros([10, 100, 2, 512])

	print('Right before TF call!')

	a = sess.run(net, {x_var: X, is_training_var: True})

	print(a)


if __name__ == '__main__':
	main()
Edited: Simplified the code.