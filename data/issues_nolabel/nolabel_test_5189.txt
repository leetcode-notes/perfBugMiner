lstm with variable bath_size

we know ,during training,we need to feed_dict a batch.However,in the last epoch, the number of data is less than a batch size.how could i do? i try to use variable batch_size,but i meet some problems. the code is below:
input_data = tf.placeholder(tf.int32, [None, num_steps])
with tf.variable_scope('forward'):
cellL = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
state_init_L = tf.get_variable("init_L", initializer=cellL.zero_state(tf.shape(input_data)[0],tf.float32))
here,  i want to initialize it,error is
ValueError: initial_value must have a shape specified: Tensor("model/init_variable_L/zeros:0", shape=(?, 100), dtype=float32, device=/device:GPU:0)
thanks~