faster-rcnn incompatible shapes randomly during training custom dataset

I'm trying to train faster-rcnn with resnet101 on a custom dataset, which I have formatted appropriately for tf. When I run training, it can run anywhere from 30-1000 steps before it gives me an error like this:
File "train.py", line 195, in main
   worker_job_name, is_chief, FLAGS.train_dir)
 File "/home/chris/tensorflow/models/object_detection/trainer.py", line 192, in train
   clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
 File "/home/chris/tensorflow/models/slim/deployment/model_deploy.py", line 193, in create_clones
   outputs = model_fn(*args, **kwargs)
 File "/home/chris/tensorflow/models/object_detection/trainer.py", line 133, in _create_losses
   losses_dict = detection_model.loss(prediction_dict)
 File "/home/chris/tensorflow/models/object_detection/meta_architectures/faster_rcnn_meta_arch.py", line 1173, in loss
   groundtruth_classes_with_background_list))
 File "/home/chris/tensorflow/models/object_detection/meta_architectures/faster_rcnn_meta_arch.py", line 1329, in _loss_box_classifier
   batch_reg_targets, weights=batch_reg_weights) / normalizer
 File "/home/chris/tensorflow/models/object_detection/core/losses.py", line 71, in __call__
   return self._compute_loss(prediction_tensor, target_tensor, **params)
 File "/home/chris/tensorflow/models/object_detection/core/losses.py", line 158, in _compute_loss
   diff = prediction_tensor - target_tensor
 File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py", line 846, in binary_op_wrapper
   return func(x, y, name=name)
 File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 2582, in _sub
   result = _op_def_lib.apply_op("Sub", x=x, y=y, name=name)
 File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py", line 767, in apply_op
   op_def=op_def)

InvalidArgumentError (see above for traceback): Incompatible shapes: [1,61,4] vs. [1,64,4]
    [[Node: gradients/Loss/BoxClassifierLoss/Loss/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device="/job:localhost/replica:0/task:0/gpu:0"](gradients/Loss/BoxClassifierLoss/Loss/sub_grad/Shape, gradients/Loss/BoxClassifierLoss/Loss/sub_grad/Shape_1)]]
    [[Node: gradients/FirstStageFeatureExtractor/resnet_v1_101/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/BatchNorm/batchnorm/mul_grad/tuple/control_dependency_1/_3949 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_22091_gradients/FirstStageFeatureExtractor/resnet_v1_101/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/BatchNorm/batchnorm/mul_grad/tuple/control_dependency_1", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]

The dimension mismatch is from what I can tell from from looking in losses.py the number of anchors, but I really don't know how to try debug this. FWIW, the training on ssd runs without issue so far.
A bit of info about the dataset, the images all contain multiple (likely crowded) of the same object.