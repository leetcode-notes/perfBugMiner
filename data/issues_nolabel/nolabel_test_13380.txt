'train_x, train_y = sess.run([train_x, train_y])'  leads machine run slowly

Hi Team,
My program run without error, but it seems not work, while my GPU shows running. The main code are as follows:
`def main(file_name, batch_size, iter_times):
x = tf.placeholder('float', (batch_size, 32, 32, 3) )
y = tf.placeholder('float', shape = [batch_size, 10] )
predictions, _, _, _ = inference_op(x, keep_prob = 0.5)
predictions = tf.cast(predictions, tf.float32)

cost = -tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits (labels = y, logits = predictions))
correct_prediction = tf.equal(y, predictions)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
for i in xrange(iter_times):
    train_x, train_y = read_data.fetch_data(file_name, batch_size) 
    train_x, train_y = sess.run([train_x, train_y]) 
    if i % 10 == 0 :
        train_accuracy = accuracy.eval(feed_dict = {x: train_x, y: train_y})
        print "%d step accuarcy is %f" % (i, sess.run(train_accuracy))
    sess.run(train)

main('train.tfrecords', 5, 2000)`
This net is based on  VGG (without the 3rd,4th,5th hidden layers) and the dataset is SVHN. I don't know how to deal with this phenomenon......