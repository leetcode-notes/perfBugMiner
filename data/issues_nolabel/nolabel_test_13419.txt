BUG: variables outside won't update in DNNLinearCombinedRegressor

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.11.6
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.3.0
Python version: 3.5
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

Describe the problem
Variables outside won't update for DNNLinearCombinedRegressor, while everything is OK for DNNRegressor.
The bug stems from that only variables in dnn / linear scope will be updated in the code below:

  
    
      tensorflow/tensorflow/python/estimator/canned/dnn_linear_combined.py
    
    
        Lines 214 to 227
      in
      107cc77
    
    
    
    

        
          
           if dnn_logits is not None: 
        

        
          
             train_ops.append( 
        

        
          
                 dnn_optimizer.minimize( 
        

        
          
                     loss, 
        

        
          
                     var_list=ops.get_collection( 
        

        
          
                         ops.GraphKeys.TRAINABLE_VARIABLES, 
        

        
          
                         scope=dnn_parent_scope))) 
        

        
          
           if linear_logits is not None: 
        

        
          
             train_ops.append( 
        

        
          
                 linear_optimizer.minimize( 
        

        
          
                     loss, 
        

        
          
                     var_list=ops.get_collection( 
        

        
          
                         ops.GraphKeys.TRAINABLE_VARIABLES, 
        

        
          
                         scope=linear_parent_scope))) 
        
    
  


Source code / logs
This is a tiny code to see whether w is updated or not.
import numpy as np

import tensorflow as tf
from tensorflow import feature_column as fc
from tensorflow.python.summary import summary

tf.logging.set_verbosity(tf.logging.DEBUG)


BATCH_SIZE = 4


def input_fn():
    x = tf.constant(np.random.randn(BATCH_SIZE, 4), dtype=tf.float32)

    w = tf.Variable(np.array([1, 2, 3, 4]).reshape((4, 1)), dtype=tf.float32, name="test/w")
    summary.scalar("test/w_0_0", w[0][0])
    summary.scalar("test/w_1_0", w[1][0])
    summary.scalar("test/w_2_0", w[2][0])
    summary.scalar("test/w_3_0", w[3][0])

    y = tf.matmul(x, w)
    label = tf.constant(np.random.randint(0, 1, size=(BATCH_SIZE,)))

    return {"y": y}, label

f = fc.numeric_column("y")

def gen_estimator(cls, model_dir):
    if cls == "dnn":
        return tf.estimator.DNNRegressor(
                feature_columns=[f],
                hidden_units=[2],
                model_dir=model_dir)
    else:
        return tf.estimator.DNNLinearCombinedRegressor(
                dnn_feature_columns=[f],
                dnn_hidden_units=[2],
                model_dir=model_dir)

gen_estimator("dnn", model_dir="/tmp/tf/facai/test_dnn").train(input_fn, steps=1000)
gen_estimator("deep_and_wide", model_dir="/tmp/tf/facai/test_wide_and_deep").train(input_fn, steps=1000)
Results

For DNNRegressor, w variables are updated:



For DNNLinearCombinedRegressor, w keeps constant: