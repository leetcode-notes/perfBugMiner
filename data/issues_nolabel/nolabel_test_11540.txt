Unclear about how to make AttentionWrapper work

Hello, I am using the TensorFlow 1.2.1 and I want to implement a Seq2Seq model with Attention through tf.contrib.seq2seq. I didn't find enough information of how to use tf.contrib.seq2seq.AttentionWrapper on documentation, however I successfully find how to use that in the NMT tutorial.
Before adding Attention, I already make the Seq2Seq model work, however, it seems very difficult to insert the AttentionWrapper by simply wrapping the decoder cell. It is not fair to ask anyone to read my own code, which is quite long. So here I provide the minimal code to demonstrate the problem:
import tensorflow as tf
from tensorflow.python.layers.core import Dense

# INPUT
X = tf.placeholder(tf.int32, [None, None])
Y = tf.placeholder(tf.int32, [None, None])
X_seq_len = tf.placeholder(tf.int32, [None])
Y_seq_len = tf.placeholder(tf.int32, [None])

# ENCODER         
encoder_out, encoder_state = tf.nn.dynamic_rnn(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128), 
    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),
    sequence_length = X_seq_len,
    dtype = tf.float32)

# ATTENTION
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units = 128, 
    memory = encoder_out,
    memory_sequence_length = X_seq_len)

decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    cell = tf.nn.rnn_cell.BasicLSTMCell(128),
    attention_mechanism = attention_mechanism,
    attention_layer_size = 128)

# DECODER
Y_vocab_size = 10000
decoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))
projection_layer = Dense(Y_vocab_size)

training_helper = tf.contrib.seq2seq.TrainingHelper(
    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),
    sequence_length = Y_seq_len,
    time_major = False)
training_decoder = tf.contrib.seq2seq.BasicDecoder(
    cell = decoder_cell,
    helper = training_helper,
    initial_state = encoder_state,
    output_layer = projection_layer)
training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder = training_decoder,
    impute_finished = True,
    maximum_iterations = tf.reduce_max(Y_seq_len))
training_logits = training_decoder_output.rnn_output

# LOSS
masks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)
loss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)

# BACKWARD
params = tf.trainable_variables()
gradients = tf.gradients(loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
train_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))
The graph cannot be built, due to error message
Traceback (most recent call last):
  File "seq2seq_attn.py", line 44, in <module>
    maximum_iterations = tf.reduce_max(Y_seq_len))
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 286, in dynamic_decode
    swap_memory=swap_memory)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2766, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2595, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2545, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 234, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py", line 139, in step
    cell_outputs, cell_state = self._cell(inputs, state)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py", line 180, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py", line 439, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File "/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", line 677, in call
    cell_inputs = self._cell_input_fn(inputs, state.attention)
AttributeError: 'LSTMStateTuple' object has no attribute 'attention'

I have also tried using GRU instead of LSTM, the error still exists