[r1.7][TensorRT] Questions about the calibration in INT8 mode of TensorRT optimization

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64
TensorFlow installed from (source or binary): pip (python 2.7)
TensorFlow version (use command below): tensorflow-gpu==1.7.0
Python version:  python 2.7
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): gcc 5.3
CUDA/cuDNN version: CUDA9.0, cuDNN7.0.5
GPU model and memory: Tesla P4, 8GB
Exact command to reproduce: NA

Describe the problem
I tried to evaluate the accuracy of TF integrated TRT INT8 optimization in Python. However, I followed the procedure as the example test: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py
but the inference accuracy is not good (FP32: 70%, INT8-without-calib: 16%, INT8-with-calib: 35%).
I tried to trace the code but on the API level, but I'm a little confused about how does the calibration work. looks like the function run_calibration() just created a session and accepted the calib dataset to run the session, and then the return value was abandoned.
I checked this introduction:https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/, seems that the TRT python INT8 calibration will generate a calibration cache in file and then be used in the later inference. But in this TFTRT, the calibration didn't generate a calibration cache.
Another observation is that, I previously tried the TensorRT optimization in C++ independently, which is not integrated in the Tensorflow. At that time, the accuracy lose on INT8 mode optimization is about 10% (FP32: 70%, INT8-with-calib: 60%). Note that my C++ and Python experiments are using the same model and same calibration dataset and the same test dataset, so from my point of view, perhaps TRT INT8 Python API (at least the TF1.7 integrated version) seems somehow different from the TRT original C++ INT8 API.
So let me conclude my questions:

Is the TF integrated TRT INT8 optimization in Python has a different back-end process compared from the independent TRT INT8 optimization in C++?
Could you share a hint of how does the python version INT8 calibration is realized? Shall the calibration data be generated so that can be loaded in later inference?

Source code / logs
Please refer to the test:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py
@samikama