The problem in saving and restoring LSTM models

I have trained a LSTM model and saved it as a small part of another model. the key code as follows:
       initializer = tf.truncated_normal_initializer()
        with tf.variable_scope('RNN_model', reuse=None, initializer=initializer):
            train_rnn = RNNmodel.LSTMmodel(True, RNNmodel.TRAIN_BATCH_SIZE, RNNmodel.NUM_STEP)
        with tf.variable_scope('RNN_model', reuse=True, initializer=initializer):
            test_rnn = RNNmodel.LSTMmodel(False, RNNmodel.EVAL_BATCH_SIZE, RNNmodel.NUM_STEP)
        init = tf.global_variables_initializer()
        sess.run(init)
        # train here
        saver.save(sess, saver_path)
everything looks normal when i restore it saver.restore(sess, saver_path), but when i run the epoch and reach to the code sess.run(op), it throws an error saying that

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value RNN_model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/kernel

and i checked out the ckpt files and found that there was not any variables about LSTM cells so i wonder if Saver().save could save the variables in LSTM cells.Or i just had a wrong practice.