Inception v3 model crashes with out of memory with batch size of 128 on a GPU with 12 GB memory..

When I run the inception v3 network, with a batch size of 32 , from here on an nVidia GPU with 12 GB memory, I get the below messages during initialization about raising pool_size_limit. The initialization takes a long time. But it eventually manages to run. Is there a way to speed this up ? The pool limit starts from 100 and keeps increasing until it reaches 2997, is there a way to increase the pool limit ? Perhaps, some sort of an environment variable ?
If I try running with batch_size of 128, it crashes with out of memory. The log of which is attached to this report. Is there a way to fix this ?
tensorflow-inception-v3-bs128.txt
Environment info
Operating System:
Stock gpu-dev Docker image on CentOS 7
Installed version of CUDA and cuDNN:
root@d4239a28fc92:~# ls /usr/local/cuda-7.5/lib64/libcud*
/usr/local/cuda-7.5/lib64/libcuda.so         /usr/local/cuda-7.5/lib64/libcudart.so
/usr/local/cuda-7.5/lib64/libcuda.so.1       /usr/local/cuda-7.5/lib64/libcudart.so.7.5
/usr/local/cuda-7.5/lib64/libcuda.so.352.93  /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudadevrt.a     /usr/local/cuda-7.5/lib64/libcudart_static.a
If installed from binary pip package, provide:

Which pip package you installed.
tensorflow-0.8.0-cp27-none-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".
0.8.0

Steps to reproduce
bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --max_steps=100
What have you tried?

Running with a smaller batch size of 32 instead of 128 to get the model to run successfully.
Pass the command -m 240000000000 to nvidia-docker.

Logs or other output that would be helpful
(If logs are large, please upload as attachment).
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=2169 evicted_count=1000 eviction_rate=0.461042 and unsatisfied allocation rate=0.722842
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=4230 evicted_count=3000 eviction_rate=0.70922 and unsatisfied allocation rate=0.68447
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 146 to 160
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2019 evicted_count=2000 eviction_rate=0.990589 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2028 evicted_count=2000 eviction_rate=0.986193 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3560 evicted_count=2000 eviction_rate=0.561798 and unsatisfied allocation rate=0.605911
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 449 to 493
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1065 evicted_count=1000 eviction_rate=0.938967 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3333 evicted_count=1000 eviction_rate=0.30003 and unsatisfied allocation rate=0.422349
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 1158 to 1273
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 65569 get requests, put_count=65760 evicted_count=1000 eviction_rate=0.0152068 and unsatisfied allocation rate=0.0161052
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 2725 to 2997
2016-07-26 13:27:52.039502: step 0, loss = 13.02 (2.4 examples/sec; 13.434 sec/batch)
2016-07-26 13:28:21.119108: step 10, loss = 14.07 (25.9 examples/sec; 1.234 sec/batch)
2016-07-26 13:28:33.393687: step 20, loss = 14.80 (26.0 examples/sec; 1.229 sec/batch)
2016-07-26 13:28:45.642208: step 30, loss = 14.68 (26.3 examples/sec; 1.217 sec/batch)
2016-07-26 13:28:57.914613: step 40, loss = 13.67 (26.0 examples/sec; 1.229 sec/batch)
2016-07-26 13:29:10.188971: step 50, loss = 13.41 (26.1 examples/sec; 1.226 sec/batch)
2016-07-26 13:29:22.507990: step 60, loss = 13.20 (25.9 examples/sec; 1.235 sec/batch)
2016-07-26 13:29:34.767687: step 70, loss = 13.15 (26.2 examples/sec; 1.223 sec/batch)
2016-07-26 13:29:47.051145: step 80, loss = 13.08 (26.0 examples/sec; 1.229 sec/batch)
2016-07-26 13:29:59.302955: step 90, loss = 13.09 (26.1 examples/sec; 1.228 sec/batch)