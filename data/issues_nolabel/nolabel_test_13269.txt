TensorFlow 1.0 and 1.2 behave differently on MultiRNNCell.

The following code works well on TF 1.0.1, but doesn't work on TF 1.2 and above, leaving error massage,
File "pb_OE_column_on_the_spot.py", line 318, in 
outputs1, _states = tf.nn.dynamic_rnn(_multi_cells, tf.one_hot(_X, data_dim), dtype=tf.float32)
ValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (256, 512) and found shape (132, 512).
I followed the change after version 1.2: https://github.com/tensorflow/tensorflow/releases,
where states: To get 5 layers each with their own parameters, write: MultiRNNCell([LSTMCell(...) for _ in range(5)])
and I also reviewed similar posts regarding same issue: https://github.com/udacity/deep-learning/issues/132
seq_length = 6
data_dim =4
hidden_dim = 12
X = tf.placeholder(tf.int32, [None, seq_length])
Y = tf.placeholder(tf.int32, [None]) 
keep_prob = tf.placeholder(tf.float32) 

X_one_hot = tf.one_hot(X, data_dim)
Y_one_hot = tf.one_hot(Y, 2) 

def lstm_cell():
    lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)
    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)
    return drop

multi_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)], state_is_tuple=True)
outputs1, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)

Why does this error happen?
.
.
.
by the way, would 12 for the number of units in the LSTM cell be too small?