Fix training of decoder embeddings.

When loop_function is provided, rnn_decoder and attention_decoder do not propagate gradients over the loop function. However,  when feed_previous for embedding_rnn_decoder, embedding_tied_rnn_seq2seq and embedding_attention_decoder is True, the output symbols are embedded by the loop function extract_argmax_and_embed. The embedding for output symbols will learning nothing if there is no gradient propagating over the loop function.
I think it will be better if the propagation of gradients is specified within the loop function itself instead of rnn_decoder and attention_decoder.