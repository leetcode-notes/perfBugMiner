Saver producing large meta files after long run

The code is for deep q-learning and for reference it's here and the keras model is here
Training after 400 timesteps:
save file is 9.7MB
meta file is 25MB
after 2,750,000 timesteps
save file is 9.7MB
meta file is 1.4GB
after 3M timesteps
save file is 9.7MB
meta file is 1.5GB
So the meta file grows over 100MB per 275,000 timesteps. The saver keeps the last 5 checkpoints by default so you could image my shock when I woke up and my Dropbox was full!
Anyway, this kind of growth is unreasonable is there a way to keep it in check?