word2vec_optimized.py fails with large training files

Here is the relevant part of the error for a 2400000000 byte file:
external/re2/re2/re2.cc:571: RE2: invalid startpos, endpos pair. [startpos: 0, endpos: -1894967296, text size: -1894967296]
W tensorflow/models/embedding/word2vec_kernels.cc:38] Invalid argument: The text file _train.txt contains too little data: 0 words
E tensorflow/core/framework/op_segment.cc:52] Create kernel failed: Invalid argument: The text file _train.txt contains too little data: 0 words
E tensorflow/core/common_runtime/executor.cc:262] Executor failed to create kernel. Invalid argument: The text file _train.txt contains too little data: 0 words

This doesn't happen for a 1000000000 byte file. I suppose there might be an overflow somewhere.
As an aside, how do I dump the embeddings from the final checkpoint?