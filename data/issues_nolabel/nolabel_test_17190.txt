tf.contrib.quantize: layer not quantized in absence of activation

In absence of an activation function, e.g., tf.layers.Conv2D(...,activation=None,...), the graph matcher using the activation_pattern at

  
    
      tensorflow/tensorflow/contrib/quantize/python/quantize.py
    
    
        Lines 185 to 192
      in
      671baf0
    
    
    
    

        
          
           activation_pattern = graph_matcher.OpTypePattern( 
        

        
          
               '|'.join(_ACTIVATION_TYPES), 
        

        
          
               inputs=[ 
        

        
          
                   graph_matcher.OneofPattern([ 
        

        
          
                       bias_add_pattern, folded_bias_add_pattern, bypass_pattern_a, 
        

        
          
                       bypass_pattern_b 
        

        
          
                   ]) 
        

        
          
               ]) 
        
    
  


won't match the layer that has no activation function, and consequently, that layer won't be quantized.
Maybe previously, instead of no activation function, tf.identity was used? The package tf.contrib.quantize searches for the identity op.

  
    
      tensorflow/tensorflow/contrib/quantize/python/quantize.py
    
    
         Line 35
      in
      671baf0
    
    
    
    

        
          
           _ACTIVATION_TYPES = {'Relu', 'Relu6', 'Identity'} 
        
    
  


Note that if activation = None, then no operation is inserted:

  
    
      tensorflow/tensorflow/python/layers/convolutional.py
    
    
        Lines 192 to 193
      in
      671baf0
    
    
    
    

        
          
           if self.activation is not None: 
        

        
          
             return self.activation(outputs) 
        
    
  


Example: Box predictors in SSD from the TF Object Detection API.
https://github.com/tensorflow/models/blob/be9b80251af1bc798553c9e5135f8b0f19fa0a81/research/object_detection/core/box_predictor.py#L688-L689
@suharshs