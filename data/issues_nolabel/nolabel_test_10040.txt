Virtual memory Leak when using gpu

Hello,
I have little problem with virtual memory in tensorflow when running on GPU:
When I run any network/graph, for example squeezenet:
args = ...
config = tf.ConfigProto()
config.gpu_options.allow_growth=True

with tf.Session(config=config) as session:
  model = SqueezeNet(args, session)
  raw_input()

I get proper allocation when I look on nvidia-smi:
+-----------------------------------------------------------------------------+
|  NVIDIA-SMI 367.48                 Driver Version: 367.48
|  ....
|   0  Quadro K620         Off  | 0000:11:00.0     Off |                  N/A |
| 34%   35C    P8     1W /  30W |     65MiB /  1999MiB |      0%      Default |
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|    0     12128    C   python                                          63MiB |
+-----------------------------------------------------------------------------+

However the top utility on the process looks very dangerous:
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12128 xuser 20   0 64.696g 461928 162552 S   2.3  4.9   0:11.24 python


64 gigs in virtual memory is probably not normal, however when i run it on the CPU (just switching CUDA_VISIBLE_DEVICES to some big number):
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12193 xuser 20   0 4468872 236988  74008 S   0.0  0.5   0:07.66 python


Then everything looks fine.
Do you know where can be the problem?
I am using tensorflow-gpu 1.1.0 on linux ubuntu16, cuda8, python2.7