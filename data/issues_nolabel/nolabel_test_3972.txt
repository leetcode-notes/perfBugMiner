tf.gradients returns None in tf.map_fn

GitHub issues are for bugs / installation problems / feature requests.
For general support from the community, see StackOverflow.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
Environment info
Operating System:
Mac OS 10.10
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
None
If installed from binary pip package, provide:

Which pip package you installed.
The output from python -c "import tensorflow; print(tensorflow.__version__)".
0.10.0rc0

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

Steps to reproduce
I want to evaluate the diagonal of Hessian with tf.map_fn, which takes a function that maps each dimension (scalar) to its Hessian
import tensorflow as tf


def hessian_factory(f):
    def hessian1(x):
        g = tf.gradients(f, x)[0]
        h = tf.gradients(g, x)[0]
        return h

    return hessian1

sess = tf.Session()
x = tf.Variable(1.0)
sess.run(tf.initialize_all_variables())

f = 1. / (1 + tf.exp(-x))
func = hessian_factory(f)
h = tf.map_fn(func, x)

print(sess.run(h))

However, it seems tf.gradients(f, x) produces None.
Even the following statements gives None
import tensorflow as tf

sess = tf.Session()
x = tf.Variable(1.0)
sess.run(tf.initialize_all_variables())

f = 1. / (1 + tf.exp(-x))
h = tf.map_fn(lambda x: tf.gradients(f, x)[0], x)

What have you tried?



Logs or other output that would be helpful
(If logs are large, please upload as attachment).