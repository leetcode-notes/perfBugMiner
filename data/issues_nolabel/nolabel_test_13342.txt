slim.learning.train can't restore variables if new variable have created.

Look at my code:
def slim_train_init_fn_test():
    '''
        In the model_ckpt/slim_train_init_fn_test.ckpt,
        {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}
    '''
    x_y = slim.variable('x/y', initializer=4.0)
    x_z = slim.variable('x/z', initializer=5.0)
    y_z = slim.variable('y/z', initializer=6.0)

    variables_to_restore = slim.get_variables_to_restore(include=['x'])

    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(
        'model_ckpt/slim_train_init_fn_test.ckpt', variables_to_restore)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)
    loss = slim.variable('loss', initializer=10.0)
    train_op = slim.learning.create_train_op(loss, optimizer)
    slim.learning.train(train_op, 'log', init_fn=init_fn, number_of_steps=1)
In the model_ckpt/slim_train_init_fn_test.ckpt,  there is a map  {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}, and I use
get_variables_to_restore(include=['x']) to restore variable x_y and  x_z which are in  model_ckpt/slim_train_init_fn_test.ckpt, but I create a new variable loss which isn't in model_ckpt/slim_train_init_fn_test.ckpt, the code raise an error:  Can't find key loss in check point file.
So I can't understand why check point file should have key loss,  does init_fn not pass variables_to_restore to tf.train.Saver ? Is it a bug?
View the assign_from_checkpoint_fn, it indeed uses assign_from_checkpoint_fn to initialize tf.train.Saver .  And there is another parameter saver  in  slim.learning.train, it is  just used to save  parameters of model, not restore.
So what's  wrong?