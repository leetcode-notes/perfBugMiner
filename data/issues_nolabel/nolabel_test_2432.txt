Feature Request: Staircase function

Right now for learning rates we have exponential_decay, which is useful but doesn't handle fine-tuned scheduling. For example, regardless of whether we did this manually or with exponential_decay, there'd be boilerplate code to use a learning rate of 1.0 for 100000 steps, 0.5 for the next 10000 steps, and 0.1 for all remaining steps.
This could be handled with a staircase function, maybe with usage like this:
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate = staircase(global_step, boundaries, values)

This would create a Tensor that evaluates to 1.0 when x <= 100000, 0.5 when x > 100000 and x <= 110000, and 0.1 when x > 110000.
Shouldn't be hard to build using tf.case.
Is this of interest or too specific?