[XLA] Add DT_HALF to the list of floating point types

This should not affect the existing CPU and GPU devices, as they do not claim support for D_HALF, and the final registration for each device is the intersection of their advertised list and the global lists.
This merely adds DT_HALF as a kind of floating point number, which will allow MatMul to work with DT_HALF.
Lots of other arithmetic ops don't restrict themselves to any specific types, so they accept DT_HALF by default.
Why isn't Convolution restricted to kFloatTypes?  I don't see why convolution and matmul are not very similar in backend implementation?
[I can't imagine why I had not spotted this before]