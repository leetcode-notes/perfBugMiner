[Critical some questions] Tensorflow-lite, Neural Networks API

Hi.
Some question.
1.

Current i use  tensorflow library compile 'org.tensorflow:tensorflow-lite:+'
and i want to use Neural Networks Api, to reduce inference time.
but wrapper function is not existing in Interpreter.java class
private static native void useNNAPI(long var0, boolean var2);
so i can't use it..
If I create a wrapper function, can I use Neural Networks API? (my device level >  8.1)

What is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/testhelper/java/org/tensorflow/lite/TestHelper.java
?
2.

i'm converting pb to tflite. mobilenet_v1_224.pb (17 mb) mobilenet_v1_224_uint8.tflite (4 mb)
inference speed about 290 ms -> 73ms decreased


Command line(mobileNet)


bazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \
  //tensorflow/contrib/lite/toco:toco -- \
  --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_graph.pb \
  --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint_graph.tflite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=1,224,224,3 \
  --input_array=input \
  --output_array=MobilenetV1/Predictions/Reshape_1 \
  --default_ranges_min=0 \
  --default_ranges_max=6 \
  --mean_value=127.5 \
  --std_value=127.5


In a similar way,
But frozen_cpm.pb(120 mb) convert to frozen_cpm_uint8.tflite(30 mb)
inference speed about 11000 ms -> 11000ms.
If the model size is reduced, does not the inference time generally decrease?