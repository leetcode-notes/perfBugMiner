distributed seq2seq: too much device placement logs

I'm trying the distributed seq2seq model. But when I run the program, there are too much device placement logs. Just like this below:
sync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0
sync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0
ScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0
ScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0
ScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0
ScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0
Variable_1/initial_value: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_1/initial_value: /job:ps/replica:0/task:0/cpu:0
mul/y: /job:worker/replica:0/task:1/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] mul/y: /job:worker/replica:0/task:1/gpu:0
GradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0
Fill_3/dims: /job:ps/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] Fill_3/dims: /job:ps/replica:0/task:0/gpu:0
GradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0
Fill_2/dims: /job:ps/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] Fill_2/dims: /job:ps/replica:0/task:0/gpu:0
GradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0
Fill_1/dims: /job:ps/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] Fill_1/dims: /job:ps/replica:0/task:0/gpu:0
GradientDescent/value: /job:ps/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent/value: /job:ps/replica:0/task:0/cpu:0
Fill/dims: /job:ps/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] Fill/dims: /job:ps/replica:0/task:0/gpu:0
Variable/initial_value: /job:ps/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] Variable/initial_value: /job:ps/replica:0/task:0/gpu:0

While when I run the original seq2seq model in single machine, I don't have the problem. I locate the simple_placer.cc:818, finding they are really log ouputs.
Why does it output in the distributed model?
simple_placer.cc
void SimplePlacer::AssignAndLog(const string& assigned_device,
                                Node* node) const {
  node->set_assigned_device_name(assigned_device);
  // Log placement if log_device_placement is set.
  if (options_ && options_->config.log_device_placement()) {
    printf("%s: %s\n", node->name().c_str(),
           node->assigned_device_name().c_str());
    LOG(INFO) << node->name() << ": " << node->assigned_device_name();
  }
}