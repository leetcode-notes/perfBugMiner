[Feature Request]Read the last batch in Dataset

The programmer guide provides an example of work flow like this
`filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]
dataset = tf.contrib.data.TFRecordDataset(filenames)
dataset = dataset.map(...)
dataset = dataset.batch(32)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()
Compute for 100 epochs.
for _ in range(100):
sess.run(iterator.initializer)
while True:
try:
sess.run(next_element)
except tf.errors.OutOfRangeError:
break`
if we have 33 examples in dataset then we will miss the last example, Is there an API to adjust the batch size automatically(e.g. in this case 1) in order to feed all the examples into model?