Incorrect behavior from `tf.layers.batch_normalization()` when `training=0`

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: 8/5
GPU model and memory: Nvidia Titan X
Exact command to reproduce: gist

Describe the problem
I've noticed that tf.layers.batch_normalization doesn't seem to give reasonable results when training=0 (i.e. use distribution statistics instead of just the batch), especially if you apply BN before activations (e.g. ResNet-like architectures).
Using the Gist above, if you try to fit a model to noise with SGD (lr=0.01) using repeated applications of dense matrix multiplication -> batch normalization -> ReLU activations, you get this loss for the same inputs over time: (blue: training=1, green: training=0)

Using an Adam (lr=0.001) optimizer instead gets even weirder results:

However, if I use my own implementation of batch norm (included in gist) I get reasonable results, with the loss for each being similar to each other: (Adam has similar behavior)

(Interestingly this doesn't seem to be as much of a problem if you have ReLU before BN, I haven't thought too deeply about why.)
Am I seeing things and just have some misunderstanding about what that function is doing, or is this actually a bug?