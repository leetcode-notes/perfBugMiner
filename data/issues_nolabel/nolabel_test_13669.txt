monotonic attention is buggy

System information

**Have I written custom code **: Yes
OS Platform and Distribution: Manjaro Linux, kernel 4.13.5
**TensorFlow installed from **: binary
TensorFlow version (use command below): 1.3, 1.4 nightly (11 oct)
Python version:  3.6
Bazel version (if compiling from source):
CUDA/cuDNN version: cuda8, cudnn 7 & 6
GPU model and memory: gtx 1080

the problem
The two monotonic attention mechanisms, LuongMonotonicAttention and BahdanauMonotonicAttention, do not seem to work as expected on my task.
In the case of LuongMonotonicAttention, the alignment that I obtain looks like a horizontal line drawn on the first row of the image. However, the alignment has a diagonal shape using BahdanauAttention or LuongAttention, and I am instantiating these classes with the same parameters.
In the case of BahdanauMonotonicAttention, I simply receive an error message:
https://pastebin.com/raw/mPPWnEH5
Is there any preprocessing I should do in addition to the non-monotonic case ?