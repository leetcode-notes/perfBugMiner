tf.get_variable() behavior somewhat inconsistent across reuse=True, reuse=None

By default, calling tf.get_variable() for a variable that does not exist yet creates that variable in the current scope, if the scope has reuse=Nne. However, if the scope has reuse=True, then  a Under-sharing error is raised.
import tensorflow as tf
with tf.variable_scope("scope1",reuse=None):
  x=tf.get_variable("x", [1,1]) # this works
with tf.variable_scope("rnnscope",reuse=True):
  w=tf.get_variable("w",[1,1]) # this fails
ValueError: Under-sharing: Variable rnnscope/w does not exist, disallowed. Did you mean to set reuse=None in VarScope?
I think I understand the motivation behind this behavior - variables retrieved in a reuse=True scope should be in "reuse-mode", so an error should be raised if it hasn't been created yet.
The way I get around this is a global variable DO_SHARE=None that is permanently set to True for all t > 0.
An example implementation of building an RNN this way is here: https://github.com/ericjang/draw/blob/master/draw.py#L163
However, this seems a bit inelegant - wouldn't it be easier to just have get_variable() create nonexistent variables, regardless of the value of reuse? Perhaps there is a better way to build RNNs that I'm not aware of?
Environment info
Operating System: Debian
If installed from binary pip package, provide:

Which pip package you installed.
GPU-enabled wheel for Linux
The output from python -c "import tensorflow; print(tensorflow.version)".
0.6.0

Steps to reproduce
What have you tried?



Logs or other output that would be helpful
(If logs are large, please upload as attachment).