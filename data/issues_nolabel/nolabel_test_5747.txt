Unable to use GPU with Tensorflow 11.0 (regression from 11.0rc2)

Cannot run convolutional network on GPU using TF 11.
It works fine under TF 11rc2. This looks like a regression.
Issue #5555 indicated to install cudnn 5.1. I checked and that is what I appear to have. cudnn.h lists 5.1 as the major/minir version. I also compared the binary file libcudnn.5.dylib downloaded from NVIDIA with the one I have in /usr/local/cuda/lib and they are the same.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#5555
Environment info
Operating System: MacOS
Installed version of CUDA and cuDNN:
CUDA Driver Version: 8.0.51
cuDNN 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
lrwxr-xr-x  1 root  wheel     33 Oct 24 15:40 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib
-rwxr-xr-x  1 root  wheel  13504 Oct 24 22:27 /usr/local/cuda/lib/libcuda.dylib
lrwxr-xr-x@ 1 root  wheel     45 Sep 27 00:00 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a
lrwxr-xr-x@ 1 root  wheel     50 Sep 27 00:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x@ 1 root  wheel     46 Sep 27 00:00 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib
lrwxr-xr-x@ 1 root  wheel     49 Sep 27 00:00 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a
lrwxr-xr-x  1 root  wheel     47 Oct 24 15:34 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib
lrwxr-xr-x  1 root  wheel     45 Oct 24 15:34 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib
lrwxr-xr-x  1 root  wheel     48 Oct 24 15:34 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a


A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0-py3-none-any.whl


The output from python -c "import tensorflow; print(tensorflow.__version__)".
0.11.0


If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_3.0_convolutional.py
(run this file with python3 - you will need three additional files from the same directory, namely tensorflowvisu*. matplotlib is also required: "pip3 matplotlib")
What other attempted solutions have you tried?
It works under Tensorflow 11 rc2
Logs or other output that would be helpful
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:c1:00.0
Total memory: 6.00GiB
Free memory: 5.87GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:c1:00.0)
E tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded runtime CuDNN library: 5105 (compatibility version 5100) but source was compiled with 5005 (compatibility version 5000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
F tensorflow/core/kernels/conv_ops.cc:526] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
Abort trap: 6