explicit device specification of restore operation on distributed training

I'm trying the run the distributed training of Inception, and this is the error I'm getting, the moment workers connect to ps.
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_1268': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0
     [[Node: save/restore_slice_1268 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device="/job:ps/task:0/device:CPU:0"](save/Const, save/restore_slice_1268/tensor_name, save/restore_slice_1268/shape_and_slice)]]

Caused by op u'save/restore_slice_1268', defined at:
  File "/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py", line 65, in <module>
    tf.app.run()
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py", line 61, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File "/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception_distributed_train.py", line 233, in train
    saver = tf.train.Saver()
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 861, in __init__
    restore_sequentially=restore_sequentially)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 519, in build
    filename_tensor, vars_to_save, restore_sequentially, reshape)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 272, in _AddRestoreOps
    values = self.restore_op(filename_tensor, vs, preferred_shard)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 187, in restore_op
    preferred_shard=preferred_shard)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py", line 203, in _restore_slice
    preferred_shard, name=name)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py", line 359, in _restore_slice
    preferred_shard=preferred_shard, name=name)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 703, in apply_op
    op_def=op_def)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2317, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1239, in __init__
    self._traceback = _extract_stack()

I'm using the latest pip TensorFlow (0.10) on Cuda 7.5 and Cudnn v5.