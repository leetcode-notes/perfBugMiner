[BUG REPORT] how to set TF_CONFIG,  bug ?

System information
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
('v1.4.0-19-ga52c8d9', '1.4.1')
Describe the problem
[NORMAL CODE]
  chief_host = ['localhost:20000']
  worker_hosts = ['localhost:20002']
  ps_hosts = ['localhost:20001']

  cluster = {'chief': chief_host,
             'worker': worker_hosts,
             'ps': ps_hosts}
  task_type = 'chief'
  task_index = 0
  os.environ['TF_CONFIG'] = json.dumps(
      {'cluster': cluster,
       'task': {'type': task_type, 'index': task_index}})

[ NORMAL OUTPUT]
INFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'localhost:20001'], u'chief': [u'localhost:20000'], u'worker': [u'localhost:20002']}, u'task': {u'index': 0, u'type': u'chief'}}
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': device_count {
  key: "GPU"
}
, '_keep_checkpoint_max': 5, '_task_type': u'chief', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x5e30bd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://localhost:20000', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/census_model', '_save_summary_steps': 100}
INFO:tensorflow:Start Tensorflow server.
2018-01-02 16:31:58.374169: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
E0102 16:31:58.378815122   44355 ev_epoll1_linux.c:1051]     grpc epoll fd: 3
2018-01-02 16:31:58.384682: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -> {0 -> localhost:20000}
2018-01-02 16:31:58.384723: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20001}
2018-01-02 16:31:58.384736: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:20002}
2018-01-02 16:31:58.391874: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:20000
Parsing data/adult.data
INFO:tensorflow:Create CheckpointSaverHook.
2018-01-02 16:32:10.407610: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2018-01-02 16:32:10.407652: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0
2018-01-02 16:32:20.407746: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2018-01-02 16:32:20.407766: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0
2018-01-02 16:32:30.407834: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2018-01-02 16:32:30.407850: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0

============
[BUG CODE]
  print(type(chief_host), type(worker_hosts), type(ps_hosts))
  print('chief=',chief_host, 'worker=',worker_hosts, 'ps=',ps_hosts)

  chief_host = chief_host
  worker_hosts = worker_hosts
  ps_hosts = ps_hosts

  cluster = {'chief': chief_host,
             'worker': worker_hosts,
             'ps': ps_hosts}
  task_type = 'chief' 
  task_index = 0 
  os.environ['TF_CONFIG'] = json.dumps(
      {'cluster': cluster,
       'task': {'type': task_type, 'index': task_index}})

[BUG OUTPUT]
<type 'list'> <type 'list'> <type 'list'>
chief= ['localhosts:20001'] worker= ['localhost:20003'] ps= ['localhost:20000']

INFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'localhost:20000'], u'chief': [u'localhosts:20001'], u'worker': [u'localhost:20003']}, u'task': {u'index': 0, u'type': u'chief'}}
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': device_count {
  key: "GPU"
}
, '_keep_checkpoint_max': 5, '_task_type': u'chief', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x6264e10>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://localhosts:20001', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/census_model', '_save_summary_steps': 100}
INFO:tensorflow:Start Tensorflow server.
2018-01-02 16:34:09.909573: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
E0102 16:34:09.914156177   44635 ev_epoll1_linux.c:1051]     grpc epoll fd: 3
2018-01-02 16:34:09.920491: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -> {0 -> localhost:20001}
2018-01-02 16:34:09.920540: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20000}
2018-01-02 16:34:09.920558: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:20003}
2018-01-02 16:34:09.927272: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:20001
Parsing data/adult.data
INFO:tensorflow:Create CheckpointSaverHook.

Note: [ BUG OUTPUT ] don't print " CreateSession still waiting for response from worker " ,  i have wait long enough !!!
===========
my train and eval code
  train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(FLAGS.train_data,
      None, True, FLAGS.batch_size), max_steps=30000)
  eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(FLAGS.test_data, 1, False,
      FLAGS.batch_size), steps=5)
  tf.estimator.train_and_evaluate(model, train_spec, eval_spec)

i can run my code on non-distribution and distribution as follows style:
TF_CONFIG='{
    "cluster": {
        "chief": ["localhost:20000"],
        "worker": ["localhost:20002"],
        "ps": ["localhost:20001"]
    },
    "task": {"type": "worker", "index": 0}
}'
TF_CONFIG=$TF_CONFIG python wide_deep_d.py 

......

but can't set TF_CONFIG by os.environ with json.dumps correctly
Need your help, Thanks ,