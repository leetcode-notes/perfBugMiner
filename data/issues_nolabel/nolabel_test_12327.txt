No half-precision support for DepthwiseConv2Native

Hi, I'm trying to convert a network from float32 to float16.
Conversion is done postmortem, for inference only.
I converted both weights and activation tensors to float16.
When trying to run inference, I get the following error:
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
device='CPU'; label='neon'; T in [DT_FLOAT]
device='CPU'; T in [DT_DOUBLE]
device='CPU'; T in [DT_FLOAT]
device='GPU'; T in [DT_DOUBLE]
device='GPU'; T in [DT_FLOAT]
Assuming that this op is not implemented for float16, what is the easiest way to bypass this issue?

implement a python op (run-time is not very crucial)?
implement a CPU version of this op?
-cast to f32 and cast back to f16?

System details

**OS Platform: Ubuntu 14.04
**TensorFlow installed from binary
**TensorFlow version 1.3.0-rc2
CUDA/cuDNN version: cuda 8.0, cudnn 6.0