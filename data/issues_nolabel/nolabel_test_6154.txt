dynamic variants of attention_decoder and embedding_attention_decoder

I've been following @alrojo's PR(#4686) for the tf.contrib.seq2seq.dynamic_rnn_decoder. I recently had to customize tf.nn.seq2seq.embedding_attention_decoder and tf.nn.seq2seq.attention_decoder with while loops for a personal project, so I'd be happy to help write attention and embedding_attention decoder functions for use with tf.contrib.seq2seq.dynamic_rnn_decoder if that would be helpful. I'm unclear on whether this is already being worked on, since I see discussion of a rnn_decoder_attention function in #4761, but it was a commit which followed the old structure of tf.contrib.seq2seq.