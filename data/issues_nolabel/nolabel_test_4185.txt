Use valid GPU for allocating CUDA host memory

This PR fixes (again!) issue #1888.
Before this commit, when allocating CUDA host memory, device 0 would
always be used to allocate the memory (because the memory is allocated
on the host with DMA enabled in the same way for all devices; it does
not matter which stream executor allocates the host memory). This
works, but if device 0 is not a valid device in this session (e.g. it
is turned off using GPUOptions.visible_device_list), doing so would
allocate a context on GPU 0, even when it was not being used. This
change fixes this, so that a visible device is found by looking at
which GPU allocators exist, and then using that device to allocate host
memory.
@vrv