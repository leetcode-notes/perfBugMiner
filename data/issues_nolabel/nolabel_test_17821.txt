Add IndRNN implementation

This PR adds the cell implementation of Independently Recurrent Neural Networks
to contrib together with unit tests.
The implementation is based on Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN (Shuai Li et al., 2018).
The difference to the BasicRNNCell is that each unit only has one recurrent weight connected to its own last hidden state. This makes it independent from other units in the same layer. To prevent vanishing / exploding gradients over time steps, the paper recommends bounds on the absolute value of that recurrent weight. These can be specified via the recurrent_min_abs and recurrent_max_abs constructor arguments.
Additional arguments of the IndRNNCell are recurrent_kernel_initializer and input_kernel_initializer. The paper does not recommend default values for the recurrent and input weights, so the default values for these were taken from 
A Simple Way to Initialize Recurrent Networks of Rectified Linear Units (Quoc V. Le et al., 2015):

Recurrent weights are set to 1 initially
Input weights are initialized using a Gaussian with mean=0 and stddev=0.001

The code is originally from batzner/indrnn.