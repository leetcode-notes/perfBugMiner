tf.nn.softmax_cross_entropy_with_logits_v2 loss function so big

Describe the problem
I am trying to build up a binary image classifcation project (2 classes are balanced) . There are 2 nodes in output layer. However, my loss function is starting from a high point(3000). Is my loss function okay? Should I change my loss function ? I want to use a metric similar to Sklearn's log-loss. At the end of 1st epoch, it reaches to 80% accuracy and after 10 epochs and its accuracy becomes 82 %. During 10 epochs, it decreases from 3000 to 20. The graph of my loss function is as following(1 epoch = 175 steps) .

The graph of accuracy for 10 epochs is below:

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Home
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below):1.6.0
Python version: 3.6.0(no anaconda)
CUDA/cuDNN version:9.0 - 7.0
GPU model and memory:GTX 1050
Bazel Version:N/A

Source code / logs
cross_entropy=tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,logits=y_pred ))