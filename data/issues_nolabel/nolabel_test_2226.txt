Evaluated expressions of variables differ sometimes when using the GPU

The evaluated value of the l1 or l2 loss of variables differ sometimes when using the GPU. This seems to happen when the variables are "large".
Even though the difference is not that much in the example below, these differences have resulted in a 98% test accuracy on a network trained on the CPU, but a 10% test accuracy on the same network trained on the GPU.
Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: 7.5 and 7.0 cuda_info.txt
GPU: Titan X
If installed from binary pip package, provide:

Which pip package you installed.
pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.version)".
python -c "import tensorflow; print(tensorflow.version)"I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
0.8.0

Steps to reproduce
import tensorflow as tf
W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))
W2 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))
sess = tf.InteractiveSession()
sess.run(tf.initialize_all_variables())
l2 = tf.reduce_sum(W1 * W1) + tf.reduce_sum(W2 * W2)
for i in range(100):
    print('%.10f' % sess.run(l2))