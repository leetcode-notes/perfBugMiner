training process dies without warning

I have two GPUs. I start an LSTM training process on one GPU by masking CUDA_VISIBLE_DEVICES in the python script:
gpu_id='0'
os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id
I then start a second training process using the same technique, but with gpu_id='1'. Shortly after launching the second training process, the first one dies without any message.
Each invocation looks something like this
nohup python train.py .... >& train.log &
When I do the same thing without redirecting to a file, I see simply Killed
This only happens when I try to run two training processes. I am able to run training and evaluation at the same time on different cards.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Nothing looks similar.
Environment info
Operating System: CentOS 7
Installed version of CUDA and cuDNN:
CUDA = 8.0 cuDNN = 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
https://paste.debian.net/901121/
If installed from binary pip package, provide:

A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".
0.11.0

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
What other attempted solutions have you tried?

I tried setting CUDA_VISIBLE_DEVICES from the command line before starting the training script. e.g. CUDA_VISIBLE_DEVICES=0

Logs or other output that would be helpful
Driver Version: 367.48