TFGAN ideas for pretraining/training

With the PR #14723 enabling get_hooks_fn to be set manually instead of the default 1 step generator and 1 step discriminator there comes a set of new "problems" / things to consider.


The Estimator saves configured tensor summaries in the background. This does not work when doing something like a 2/2 split for generator/discriminator or a 2/3. Then Estimator will only save one step instead of the actual amounts of steps taken (right?). This means that we have to consider an option to configure the FileWriter. I believe that is possible for the vanilla Estimator with scaffolds but not for the GANEstimator currently. This FileWriter has to be accessible by the RunTrainOpsHook. We would also need a generator + discriminator specific "global_step" that will be used in the RunTrainOpsHook. A quick idea would be to use the overall_global_step * train_steps of a RunTrainOpsHook or to use dummy_global_step_generator and dummy_global_step_discriminator.


When "pre-training" (with a normal call to .train() and a modified sequential_train_hook(10,10)) the generator for e.g. 10 steps and then the discriminator for 10 steps. Does the discriminator loss_fn receive 10 times new data from the generator through gan_model.discriminator_real_outputs or will it always be the same data? For pre-training I would assume I can feed in the same output data from the generator in batches multiple epochs. I believe that is not possible in the current setup, but correct me if I am wrong.


I have different loss functions for both pre-training and training. There is not ModeKeys.PRETRAIN to switch between them.


If I find more things I'll add them here.