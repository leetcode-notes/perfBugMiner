tf.data.Dataset.padded_batch() should support padding to nearest N bytes

System information
TF 1.5
Python 3.6.4 (Anaconda)
Problem description
If using tf.data.Dataset.padded_batch() on input with variable size, it looks like there is significant overhead first time a batch with examples of size X is used on GPU. If batch with examples of size X is used second time, the execution is approx. 4x faster. (I suppose it is due to a way tensorflow handles data.)
In case the input size varies a lot, this slows the computation enormously. It could be solved by padding the batch to nearest N bytes so that there is only limited number of sizes pushed onto GPU and overhead thus becomes negligible.
Solution
It would be great if padded_batch supported argument pad_to_nearest_bytes or/and allowed_batch_sizes for enumeration of possible sizes.
Proof:
TIME    BATCH_SIZE (2nd dimension = the data size)
0.15205 20480
0.06180 20480
0.80608 147456
0.24141 147456
0.74360 135168
0.21659 135168
0.58724 98304
0.16206 98304
0.05387 20480
0.05694 20480
0.53993 90112
0.15452 90112
0.23547 147456
0.23576 147456