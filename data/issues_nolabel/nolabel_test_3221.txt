Distributed tensorflow does not allocate well 'worker' to GPUs

Environment info
Operating System: Ubuntu 14.04 desktop
Installed version of CUDA and cuDNN: 7.5,  5.0.5
tensorflow 0.9.0.rc0 is installed from source.
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   322936  8 16  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16  8 16  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192  8 16  2015 /usr/local/cuda/lib64/libcudart_static.a
-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so
-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rwxr--r-- 1 root root 58775484  6 21 15:39 /usr/local/cuda/lib64/libcudnn_static.a
I'm trying to use distributed tensorflow with 8 Titan-x GPUs in two servers.
4 GPUs are in one server.
I separate workers to each GPUs as follows.
with tf.device(tf.train.replica_device_setter(
worker_device="/gpu:%d" % (FLAGS.task_id%4), cluster=cluster_spec)):
When I execute two 'ps' and eight 'worker', nvidia-smi shows next.
((server1)) workers are allocated on GPU 0,1,2,3

((server2)) workers are allocated on GPU 0

Why the workers are not allocated to GPU 1,2,3 in server1 ?
please help me.