Allocating reusable memory in Tensorflow custom operator

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MaxOS 10.13 and Linux version 2.6.32-696.3.2.el6.x86_64
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
1.5
Python version:
2.7
Bazel version (if compiling from source):
0.11.0
GCC/Compiler version (if compiling from source):
gcc 4.4.7
CUDA/cuDNN version:
8.0/5.1
GPU model and memory:
Tesla K40M

Describe the problem
Hi, I'm having a memory related question when defining a Tensorflow custom operator with cuBLAS/C++.
I defined a custom operator which needs to be called multiple times during training, computed with a BLAS level 3 algorithm.
My algorithm (as well as many other BLAS3 algorithms) requires a 'workspace' memory to exploit the hardware efficiently. However, I cannot seem to find a way to allocate such memory and keep it throughout the training process. Instead, I have to allocate a new chunk of workspace memory and free it during each call to my operator.
I've also tried to define a workspace variable in Tensorflow and feed it to the operator as an input. But I found that TF treats it as a const pointer and does not allow me to write to it.
So I am wondering if there is an efficient way to allocate reusable GPU memory for the operator at the beginning, and keep it throughout the training process. If there is not, would it be better to include such features in the future versions so that many well-developed linear algebra algorithms could avoid such memory allocating issue.