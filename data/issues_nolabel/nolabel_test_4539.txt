Clarification about embedding_attention_seq2seq

This would qualify as a bug depending on what was the intention behind writing the code, the way it was written and whether I understand the workings of Tensorflow well enough.
The cell input given to
embedding_attention_seq2seq function in seq2seq.py  seems to be used both by the encoder and by the decoder. Internally, the EmbeddingWrapper class just does self._cell = cell  which means that it does not create a copy of the cell. Doesn't this mean that the decoder and encoder end up sharing the parameters of their respective LSTMs? is this intentional?
EDIT: realized why they're different cells: because of the scopes.