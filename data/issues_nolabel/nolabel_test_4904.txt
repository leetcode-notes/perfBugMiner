is the documentation about sampled_softmax_loss correct?

Hi,
"tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy='mod', name='sampled_softmax_loss')"
the document says:
"At inference time, you can compute full softmax probabilities with the expression tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases)."
I am not sure if this describes the behavior correctly. First, the labels are not used in the softmax computation; Second, the weights matrix is merely a lookup table that maps each word to a vector (word2vec), the above matmul operation is not meaningful.
Here is my understanding of the code:
Given any word w, we have a vector encoding V1 from inputs, and then with label, we look up the weight matrix to find a vector encoding V2. Then we compute the softmax loss (cross-entropy loss) between V1 and V2. Finally, we sum up the loss for the sampled words.