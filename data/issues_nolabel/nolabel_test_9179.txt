Failed to add dependency with tf.identity

System Information
ArchLinux, TensorFlow 1.0.1 binary for py3.6

Exact command to reproduce:

import tensorflow as tf
a = tf.get_variable('a', shape=[])
update_op = a.assign_add(1.0)

cost = tf.reduce_mean(tf.get_variable('x', shape=[10]))
with tf.control_dependencies([update_op]):
    cost = tf.identity(cost)

train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for k in range(10):
    sess.run([cost])
    print(sess.run([a]))    # a increases

for k in range(10):
    sess.run([train])
    print(sess.run([a]))    # a doesn't increase
Looks like the train op doesn't depend on update_op.
In terms of computation, backprop doesn't depend on the value of cost.  Is this intended?
UPDATE: Turns out that tf.gradients doesn't have a dependency over its first argument, although intuitively I thought the opposite. This is a feature. Closing..