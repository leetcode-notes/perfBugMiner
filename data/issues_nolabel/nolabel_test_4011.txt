Is there anyway to run a tensorflow graph (.pb or .meta) generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes?

Will future versions of tensorflow provide a way to run the tensorflow graph generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes through python interfaces?
Or is it supported right now?
I am trying to build my tf.graph on my notebook (single node) and save then graph into a binary file,
and then loading the binary graph into a distributed environment (with multiply ps and worker nodes) to train and verify it. It seems it is not supported now.
I tried it on tensorflow-0.10 and failed.
By using tf.train.write_graph(sess.graph_def, path, pb_name) interface: The graph saved is not trainable as loading the .pb file through import_graph_def will only g.create_ops according to the '.bp' file but not add then into ops.collections. So the graph loaded is not trainable
By using tf.saver.save to save a ".meta" file: The loaded graph cannot fit into the distributed environment as devices assignment is messy.
I tried the tf.train.import_meta_graph('test_model.meta', clear_devices=True) interface to let the load clean the original device assignment and let the "with tf.device(device_setter)" reassign the device for each variable, but there is a problem as operations belonging to "Saver" and "Restore" still can not be assigned correctly. When creating operations for "Saver" and "Restore" ops through g.create_op inside import_graph_def called by import_meta_graph, the device_setter will not assign ps node to these ops as their name is not "Variable".
Is there any way to do so?