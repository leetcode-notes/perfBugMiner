Non-deterministic result using  a pre-trained word embedding  in TensorFlow

I use pre-trained word embedding to initialize embedding in tensorflow, but got a non-deterministic result. However, if I fix the seed and use random initialization, the result is almost the same.What's the problem here? Thanks.
The code is here:
with tf.variable_scope("embedding"):
    if init_embedding is None:
        self.embedding = tf.get_variable(name='embedding', shape=[vocab_size, word_dim],
                                              dtype=np.float32)
    else:
        self.embedding = tf.get_variable(name="embedding", shape=init_embedding.shape,
                                         initializer=tf.constant_initializer(init_embedding), `trainable=True)`

init_embedding is the pre-trained word embedding