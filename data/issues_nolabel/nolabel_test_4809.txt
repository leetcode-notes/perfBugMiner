Getting Send/Recv timings for distributed TF

I'm trying to troubleshoot some slowness in our distributed models , and it would be useful to have access to timing of Send/Recv ops across graph partition.
cc @suharshs because maybe StatsPublisherInterface is relevant?
For instance, a toy benchmark here adds 100MB vectors of 1's in one process to variable in another process on local machine. If I look at timeline/stepstats, I see 120ms of emptiness, followed by 20ms in AddOp followed by another 700ms of missing time.
Because it's a toy benchmark, I can figure out that 120ms is spent in transferring 100MB from one TF runtime to another, and 700ms is spent making the result available to Python client. But it's harder to do this on a large model