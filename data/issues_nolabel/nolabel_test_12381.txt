Limit GPU memory usage not working in distributed training of inception

I'm using the example provided for distributed trainig of inception. I have three hosts, with one for parameter server and two for workers. All hosts have TensorFlow 1.2 installed with CUDA 8.0, cuDNN 5.1 and Titan X Pascal GPU running Ubuntu 14.04.
I followed the instruction for distributed training as provided in the readme and it runs successfully. But when I try to limit the GPU usage by making the following changes.
Change
sess_config = tf.ConfigProto(
allow_soft_placement=True,
log_device_placement=FLAGS.log_device_placement)
to
sess_config = tf.ConfigProto(
allow_soft_placement=True,
log_device_placement=FLAGS.log_device_placement,
gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.6))
in inception_distributed_train.py
I recompiled the example using 'bazel build //inception:imagenet_distributed_train' and run the example. But the process still occupies all GPU memory available observed using nvidia-smi.