[Python][Quantized Inference] 'Zero is not representable' when doing inference from quantized const graph def

I used the checkpoint file and the graph def file to generate the const graph def file using the freeze_graph script. Then I used the quantize_graph tool to generate a quantized model. The output file sizes from these steps seem reasonable. But I failed to run a inference using the generated quantized model.
Tried both tensorflow 0.8.0 and 0.9.0, they both failed but with different errors though. For 0.80, it will output:
W tensorflow/contrib/quantization/kernels/quantized_conv_ops.cc:201] Zero is not representable in the quantized range used by the input. This means QuantizedConv2d has to fall back to a slow implementation, since the border of zero values can't be represented easily. You should try to construct graphs that avoid this situation."
For 0.9.0, it wouldn't load the lib with:
_quantized_kernels = tf.load_op_library(PATH_TO_KERNAL_OS)
_quantized_ops = tf.load_op_library(PATH_TO_OPS_OS)
I got some signature errors.
Environment info
Operating System:
Ubuntu 14.04
If installed from binary pip package, provide:
Installed the 0.8.0, but tried with 0.9.0 as well.
Steps to reproduce

Generate a checkpoint file and graph def file
Using freeze_graph and quantize_graph to generate a quantized const graph def binary file
Build the quantized_kernels.so and quantized_ops.so
tf.load_op_library() to load the two libraries. version 0.9.0 will fail at this step
Extract the input and output node names, feed the input properly and run the session
version 0.8.0 will fail at this step with the following errors:

W tensorflow/contrib/quantization/kernels/quantized_conv_ops.cc:201] Zero is not representable in the quantized range used by the input. This means QuantizedConv2d has to fall back to a slow implementation, since the border of zero values can't be represented easily. You should try to construct graphs that avoid this situation.
W tensorflow/contrib/quantization/kernels/quantized_conv_ops.cc:201] Zero is not representable in the quantized range used by the input. This means QuantizedConv2d has to fall back to a slow implementation, since the border of zero values can't be represented easily. You should try to construct graphs that avoid this situation.
terminate called after throwing an instance of 'std::bad_alloc'
what():  std::bad_alloc
Aborted (core dumped)
What have you tried?
Described above
Logs or other output that would be helpful
Added above.