Gradients Not Being Computed Correctly While Using tf.contrib.distributions.Categorical

Hello, there appears to be an issue with how TensorFlow gradients are being computed while using Categorical in the graph.  In particular, let's say we were computing the gradient through a single logit of a vector, we would expect only the corresponding column of the weight matrix affecting that logit value to update with gradients. This is indeed the case when I keep this index fixed, say tf.constant(0, dtype=tf.int32), however, this is not the case while using Categorical.  The code below should clarify further.
Thanks!
Liam
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I have not found this particular bug in a quick search.
Environment info
Operating System:  Ubuntu 14.04.4 LTS
Installed version of CUDA and cuDNN:
-rw-r--r-- 1 root root 189170 Mar 17 17:29 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Mar 17 17:29 /usr/local/cuda/lib/libcudart_static.a
TensorFlow from source.

Commit hash:  1b50845ff01200b3f6fc78a2780df49baea674ff
bazel version:

Build label: 0.2.2b
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Apr 25 08:08:53 2016 (1461571733)
Build timestamp: 1461571733
Build timestamp as int: 1461571733
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import tensorflow as tf
from tensorflow.contrib.distributions import Categorical
tf.reset_default_graph()

input_dim = 3
hidden_dim = 5
output_dim = 3
lr = 1e-1
num_iterations = 50
print_every = 1

x = tf.fill([1, input_dim], 1.)
y = tf.fill([1, output_dim], 1.)

with tf.name_scope('Model'):
    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')
    logits = tf.matmul(x, W_gen)

    sample_op = tf.stop_gradient(Categorical(logits).sample(n=1))
    index = tf.squeeze(sample_op)
    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)
    logits = logits * one_hot

    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')
    output = tf.matmul(logits, W_dis)


with tf.name_scope('Loss'):
    loss_op = tf.reduce_mean(tf.squared_difference(output, y))

with tf.name_scope('Train'):
    train_vars = [W_gen]
    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)


with tf.Session() as sess:
    init_op = tf.initialize_all_variables()
    sess.run(init_op)

    for i in xrange(num_iterations):
        if i % print_every == 0:
            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))
            print('Sample: [%d]' % sess.run(index))
            print sess.run(W_gen)
        sess.run(train_op)
    print sess.run(output)

Logs or other output that would be helpful
Here I show you the Sample generated by Categorical and then the corresponding change to the weight matrix.  Notice, that the first training step operates as expected, only the '1th' column is affected.
However, the second training step both column 1 and 2 change.
Sample: [1]
[[ 0.81418228  0.8655349   0.16064     0.55864608  0.35103011]
 [ 0.26089203  0.69035411  0.64020491  0.02805829  0.99758911]
 [ 0.56620026  0.52124786  0.23499095  0.59907818  0.44014001]]

Sample: [1]
[[ 0.81418228  0.76553494  0.16064     0.55864608  0.35103011]
 [ 0.26089203  0.59035414  0.64020491  0.02805829  0.99758911]
 [ 0.56620026  0.4212479   0.23499095  0.59907818  0.44014001]]

Sample: [0]
[[ 0.81418228  0.69852936  0.23505335  0.55864608  0.35103011]
 [ 0.26089203  0.52334857  0.71461827  0.02805829  0.99758911]
 [ 0.56620026  0.35424232  0.30940431  0.59907818  0.44014001]]