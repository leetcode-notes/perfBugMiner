Using 3d Input for seq2seq Models -- Word Vector Input

Hey TF, great code.
I was wondering for your seq2seq models, if it is possible to insert 3d input. The reason being is that you usually want to embed your words to lets say 128 vectors. Google's Word2vec tool does this very well.
However, it looks that for all the seq2seq models, the input to the encoder and decoder are only 2d. This means that each word has to be converted to an integer correct? It would be nice if we could insert a 128 vector instead. Or perhaps I'm missing something? Thanks!