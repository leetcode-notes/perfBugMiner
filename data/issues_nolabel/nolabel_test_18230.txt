TypeError: unsupported format string passed to Tensor.__format__

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X  version: 10.12.1
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):v1.7.0-3-g024aecf414 1.7.0
Python version: 3.6.4
Bazel version (if compiling from source):-
GCC/Compiler version (if compiling from source):-
CUDA/cuDNN version: -
GPU model and memory:  Intel Iris Graphics 6100 1536 MB
Exact command to reproduce: python toy-dataset.py

Describe the problem
I run the toy dataset code in the Eager Execution Guide Document, but I got an error.
Source code / logs
WARNING:tensorflow:From /XXXXXXX/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
Traceback (most recent call last):
  File "toy-dataset.py", line 31, in <module>
    print("Initial loss: {:.3f}".format(loss(W, B)))
TypeError: unsupported format string passed to Tensor.__format__

from __future__ import absolute_import, division, print_function
import tensorflow as tf
import tensorflow.contrib.eager as tfe

# A toy dataset of points around 3 * x + 2
NUM_EXAMPLES = 1000
training_inputs = tf.random_normal([NUM_EXAMPLES])
noise = tf.random_normal([NUM_EXAMPLES])
training_outputs = training_inputs * 3 + 2 + noise
def prediction(input, weight, bias):
  return input * weight + bias

# A loss function using mean-squared error
def loss(weights, biases):
  error = prediction(training_inputs, weights, biases) - training_outputs
  return tf.reduce_mean(tf.square(error))

# Return the derivative of loss with respect to weight and bias
def grad(weights, biases):
  with tfe.GradientTape() as tape:
    loss_value = loss(weights, biases)
  return tape.gradient(loss_value, [weights, biases])

train_steps = 200
learning_rate = 0.01
# Start with arbitrary values for W and B on the same batch of data
W = tfe.Variable(5.)
B = tfe.Variable(10.)

tf.Print(loss(W, B), [loss(W, B)], message="This is a: ")
print("Initial loss: {:.3f}".format(loss(W, B)))

for i in range(train_steps):
  dW, dB = grad(W, B)
  W.assign_sub(dW * learning_rate)
  B.assign_sub(dB * learning_rate)
  if i % 20 == 0:
    # print(i, loss(W, B))
    print("Loss at step {:03d}: {:.3f}".format(i, loss(W, B)))

print("Final loss: {:.3f}".format(loss(W, B)))
print("W = {}, B = {}".format(W.numpy(), B.numpy()))