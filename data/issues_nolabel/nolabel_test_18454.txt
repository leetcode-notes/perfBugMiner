Wrong behavior of tf.nn.conv2d when dilated rate>1 on CPU version of tensorflow.

Required Info

OS Platform and Distribution:Linux Ubuntu 16.04
TensorFlow installed from:binary
Tensorflow Version:1.6.0
Python Version:3.6.3
Have I written custom code:Yes
Bazel version:N/A
CUDA/cuDNN version : N/A
GPU model and memory: N/A
Exact command to reproduce: See Below

Problem description:
When constucting the graph, i use tf.nn.conv2d with diated_rate > 1 and padding='VALID', the wrapper returns a correct shape. However when i feed an input in, and use sess.run to get the output, it turns out that tf.nn.ops is not using diliated conv at all. Then i run the exact commands on a machine with tensorflow_gpu, the gpu_versions output is correct. So i believe that i should be a bug.
Exact Commands:
import tensorflow as tf
import numpy as np
inputs = tf.get_variable(shape=[1, 7, 7, 1], dtype=tf.float32, name='inputs')
filters = tf.get_variable(shape=[3, 3, 1, 1], dtype=tf.float32, name='filters', initializer=tf.constant_initializer(
    [
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]
    ]
))
conv = tf.nn.conv2d(input=inputs,
                    filter=filters,
                    strides=[1] + [1, 1] + [1],
                    padding='VALID',
                    use_cudnn_on_gpu=False,
                    data_format="NHWC",
                    dilations=[1] + [2, 2] + [1],
                    name='conv')
print(conv)

inputs_v = np.zeros([1,7,7,1])
inputs_v[:,0,:,:] = np.reshape([1,2,3,4,5,6,7], newshape=[1,7,1])
sess = tf.Session()
sess.run(tf.global_variables_initializer())

conv_v = sess.run(conv, feed_dict={inputs:inputs_v})
print(conv_v)