Gradient computation through while loop

Computation of gradients through a simple while op doesn't seem to work:
import tensorflow as tf
from tensorflow.python.ops import control_flow_ops

def cond(i, var):
    return tf.less(i, 1)

def body(i, var):
    return [tf.add(i, 1), var]

i = tf.constant(0)
var = tf.Variable(tf.constant(2.))

_, var = control_flow_ops.While(cond, body, [i, var])

out = var*var

optimizer = tf.train.GradientDescentOptimizer(.9)
optimizer_op = optimizer.minimize(out)

with tf.Session() as sess:
    tf.initialize_all_variables().run()
    print sess.run([out, optimizer_op])
    print sess.run([out, optimizer_op])
    print sess.run([out, optimizer_op])
gives the following result:
[4.0, None]
[4.0, None]
[4.0, None]

Whereas commenting out the while op on line 13 gives:
[4.0, None]
[2.5599997, None]
[1.6383994, None]

Since the while op here only iterates once, and doesn't really do anything, I would expect that the result should be the same with or without it.
I know that the while operator is not officially documented or supported, but the wording in #593 made me believe that computation of gradients through a single simple loop was likely to work. (Why experiment with nested loops if single loops didn't work)