Quantization make graph slower during inference.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from: using TF source code (GPU build), can provide docker to reproduce environment conditions if necessary
TensorFlow version: using r1.3 branch, version 1.3.1
Python version: 2.7
Bazel version (if compiling from source): 0.6.1
CUDA/cuDNN version: 8.0/6.0
GPU model and memory: GeForce GTX 1080 Ti, 11170 MB
Exact command to reproduce:
/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --in_graph=/quantization/VGG16/frozen_model.pb   --outputs="Validation_segmentation/Validation/decoder/Softmax" --out_graph=/quantization/VGG16/optimized_model.pb   --transforms='add_default_attributes strip_unused_nodes(type=float, shape="384,1248,3") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes'

Describe the problem
Hi, I compressed a graph using transform_graph tool but the resulting graph is actually slower during inference. I am compressing a graph similar to the one presented in this article: https://arxiv.org/pdf/1612.07695.pdf, which has VGG16 as an encoder in input and a classification decoder with a Softmax in output. Inference uses same python script for both graph (original and quantized) and make an average of 100 inferences. Original graph takes ~0.1s for inference, quantized graph takes 70s! If I perform quantization without quantize_nodes, inference takes ~0.3s.
I understand that this quantization is still in a work in progress and maybe was more aimed at improving inference on mobile devices, but I'm surprised that it is actually so much slower, so that's why I'm logging it as a bug here. (I posted this on stackoverflow but didn't get any answer...)
The graph takes ~500Mb, let me know if I should attach it to this ticket (or include an external link?)
Source code / logs
quantization_logs.txt
tf_env.txt
inference.py.txt