tf.identity() not copying varying inputs

When using tf.identity() it seems like a proper implementation to pass through the input tensor without copying. However, when the input is the value of a variable it might change later, leading to surprising behavior:
var = tf.Variable(1)
old = tf.identity(var.value())
with tf.control_dependencies([old]):
  with tf.control_dependencies([var.assign_add(1)]):
    new = tf.identity(var.value())

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([old, new]))  # Unexpected: [2, 2]
I would think this code should be equivalent to the following workaround that uses a second variable to remember the previous value:
var = tf.Variable(1)
old = tf.Variable(var.initialized_value())
with tf.control_dependencies([old.assign(var.value())]):
  with tf.control_dependencies([var.assign_add(1)]):
    new = tf.identity(var.value())

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([old, new]))  # Expected: [1, 2]
Can we make tf.identity() aware of whether its input is static or varying, to always return the value from the time it's executed?