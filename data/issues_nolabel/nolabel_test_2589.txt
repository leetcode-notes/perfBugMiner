Request for Schedule learning rate at arbitrary step(echo)

There is only tf.train.exponential_decay for decaying the learning rate at constant step.
However in ResNet-1001: "Identity Mappings in Deep Residual Networks", arXiv:1603.05027, 2016,
The learning rate starts from 0.1, and is divided by 10 at 32k and 48k iterations.
Following [1], for all CIFAR experiments we warm up then training by using a smaller learning rate of 0.01 at the beginning 400 iterations and go back to 0.1 after that.
It's hard to implement in tensor flow since tensor doesn’t support python comparison.
The learning rate can’t be set in if/else according to global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)