tf.slim should not use relu as a default activation function

It's hard to understand the reason why relu is chosen as a default activation function. It looks like the choice of default activation function is against common sense. If someone never read the actual code or forget about it, it's so easy to make a mistake.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1387
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L822