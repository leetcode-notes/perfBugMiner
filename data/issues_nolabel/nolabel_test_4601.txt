convert Inception v1 model .pb file into 8bit precision fail

I use the guide below to download a v3 model and convert it to 8 bit precision without no issue.
https://www.tensorflow.org/versions/master/how_tos/quantization/index.html
curl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz
tar xzf /tmp/inceptionv3.tgz -C /tmp/
bazel build tensorflow/contrib/quantization/tools:quantize_graph
bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph 
--input=/tmp/classify_image_graph_def.pb 
--output_node_names="softmax" --output=/tmp/quantized_graph.pb 
--mode=eightbit
However, i download a v1 model from below and try to convert to 8 bit precision fail.
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/
https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip
the model file is: "tensorflow_inception_graph.pb"
I get error:
File "/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py", line 319, in rewrite
for output_node_name in output_node_names]
KeyError: 'softmax'
it says key error: softmax. is this caused by v1 model use something other than softmax or it has different name? What should i input for --output_node_names parameter when I convert v1? Or how can i get the output node name? If i try to convert other model .pb file how to input this parameter?
Thanks