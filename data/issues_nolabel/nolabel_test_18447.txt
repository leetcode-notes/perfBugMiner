Numerically safe cross_entropy for tfd.Bernoulli?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.7
Python version: 3.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce:

import tensorflow as tf
from tensorflow import distributions as tfd

tf.enable_eager_execution()

def test_bernoulli_cross_entropy():
    logits = tf.random_normal(shape=[3, 4])
    labels = tf.cast(tf.random_normal(shape=[3, 4]) > 0.0, tf.float32)
    p = tfd.Bernoulli(logits=logits)
    q = tfd.Bernoulli(probs=labels)

    ce1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=p.logits, labels=q.probs)
    ce2 = q.cross_entropy(p)
    print(ce1)
    print(ce2)

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
v1.7.0-3-g024aecf414 1.7.0
Describe the problem
The Bernoulli in Tensorflow distributions does not seem to have a numerically safe cross entropy operation. See code above -- it should produce the same results as tf.nn.sigmoid_cross_entropy_with_logits, but produces NaN. If I set probs to be not exactly zero or one, then ce1 = ce2. So it seems to be a numerical stability issue.
Example output:
tf.Tensor(
[[0.88931024 1.4332782  0.47688866 0.48566538]
 [0.93064773 0.39325503 1.1308188  1.5703993 ]
 [0.09037975 0.1828385  0.39422417 0.839576  ]], shape=(3, 4), dtype=float32)
tf.Tensor(
[[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]], shape=(3, 4), dtype=float32)

Source code / logs
import tensorflow as tf
from tensorflow import distributions as tfd

tf.enable_eager_execution()

def test_bernoulli_cross_entropy():
    logits = tf.random_normal(shape=[3, 4])
    labels = tf.cast(tf.random_normal(shape=[3, 4]) > 0.0, tf.float32)
    p = tfd.Bernoulli(logits=logits)
    q = tfd.Bernoulli(probs=labels)

    ce1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=p.logits, labels=q.probs)
    ce2 = q.cross_entropy(p)
    print(ce1)
    print(ce2)