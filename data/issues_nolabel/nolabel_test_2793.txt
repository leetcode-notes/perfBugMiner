Clipping gradient w.r.t. inputs at each time step for RNN/LSTM

New feature request:
While training LSTMs, is it often useful to clip the derivates w.r.t the inputs into the LSTM at each time step (Alex Graves, Sec 2.1). This is different from clipping the overall gradient. Theano supports this feature with theano.gradient.grad_clip(tensor_var). Can we have a similar feature in tensorflow?