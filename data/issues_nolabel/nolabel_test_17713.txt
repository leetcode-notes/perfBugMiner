Manual Placement of Graph Nodes of High-level Optimizers and Loss functions

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.6.0-rc1
Python version: 2.7
Bazel version (if compiling from source): 0.11.0
GCC/Compiler version (if compiling from source): 5.4
CUDA/cuDNN version: 9.1/7.0
GPU model and memory: Tesla k80 (11441MiB)
Exact command to reproduce: python cifar10_train.py

Describe the problem
I want to use my own device placement algorithm to be used to place nodes across multiple devices my machine has. I looked into this : https://www.tensorflow.org/programmers_guide/using_gpu#manual_device_placement which suggests that if I figure out exact placement statically before executing the graph, I can use "with device" annotations to partition my graph across devices. But, if I am using some high-level optimizers and loss functions. Each of these operations are going to have a multi-node graph as well. Suppose I am running this example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10 and want to specify where each node of the complete tensor flow graph should go given a list of compute resources (cpu cores, gpu, tpu etc.) my machine has.
Is there any api or framework that allows you to do fine grained placement of graph nodes across multiple compute resources ? What's the best way to achieve this ?
Thanks a lot for any help!
Source code / logs
cifar10 example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10