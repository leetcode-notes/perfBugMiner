Added a bias term in Bahdanau attention alignments

The original paper by Bahdanau (https://arxiv.org/abs/1409.0473) lists a bias term (Eq. 18) and Tensorflow's normalized Bahdanau implementation uses one, so I don't see why we shouldn't use one in the original implementation.
Edit: Err, I was wrong, staring at papers too much all day, and quoted an equation from the paper that introduces Bahdanau energy normalization (http://arxiv.org/abs/1704.00784). Still, the original may have used it too as they state they "omit bias terms to make the equations less cluttered". My bad - the reviewer decides what to do with this I guess.