Dataset API fromGenerator Functionality

@mrry I was looking at the code for the fromGenerator function and I notice that you do iter(generator()) to create multiple parallel iterators over the same generator. Maybe I'm not too familiar with the semantics of a Python generator, but my impression was that it represents a continuation and it's not necessarily repeatable, meaning that you would have to cache all the elements you get in order to reproduce them. For example, let's say a generator is querying an online service and yielding a different tensor at each time point, dependent on the answer it gets from that service. This would not be repeatable without caching those tensors. So, in that case, how would your implementation behave? And in more general, what does the iter() function applied in this setting mean?
Regarding the caching of elements, I think that if there is a problem with the current implementation, then the right way to do this would be something along the lines of:

Create a dataset from a generator, as it's currently done, but also store a flag in it specifying it's a GeneratorDataset (could also be subclassing dataset to make things simpler).
When repeat is called on such a dataset, convert the generator to something like a Scala stream, which simply memoizes elements as you obtain them, but lazily evaluates the tail of your sequence (sort of like an iterator but with memoization) and then return a dataset that uses that stream as its source.

It may be totally unnecessary depending on the semantics of the iter() function, but based on a quick search I did, I couldn't find enough information for this setting.