Shape inference issues after running dynamic_rnn

I am trying to use dynamic_rnn in the encoder part of seq2seq library. I am using a placeholder for encoder input with dimensions [None, None] to reflect dynamic batch size and time steps.
The problem is that the output that I get after the dynamic_rnn run has unknown shape along time dimension which causes issues when I try to use it along with attention. I have been able to overcome it by using the bucket length to set_shape but now that I am trying to ditch bucketing, it seems a little non-trivial. I was trying to use the maximum of sequence lengths but I have not been able to successfully use the value obtained from tf.reduce_max(sequence_length). Any help/suggestions would be much appreciated. Also, I am using version 0.11.