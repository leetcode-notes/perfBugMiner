sampled_softmax souldn't be linear with respect to vocabulary size, but actually is

Hi,
tf.nn.sampled_softmax_loss API doc page tells us to get into Section 3 of Jean et al., 2014 (pdf) for more information about it.
And actually, they quite start sec. 3.1saying:

"With  the  proposed  approach,  the  computational complexity of training becomes constant with respect to the size of the target vocabulary"

Which, kept my attention.
I ran a benchmark using my custom RNN LM script (derived from tensorflow 0.11 ptb_word_lm.py.
Results shows that, only changing vocab_size increases computation time linearly. (the benchmark is using 'SmallConfig' with vocab_size of either 10k or 150k.
Environment info
Operating System: Ubuntu 14.04.
Installed version of CUDA and cuDNN:
$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root 546K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root   16 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0*
lrwxrwxrwx 1 root root   19 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44*
-rwxr-xr-x 1 root root 406K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44*
-rw-r--r-- 1 root root 757K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart_static.a

TensorFlow Version:
0.11RC2
Thx for your reading & feebacks
pltrdy