Different pixel values for the same image when read using DecodeJpeg on x86_64 and arm64

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
TensorFlow installed from (source or binary):
Source
TensorFlow version (use command below):
1.5.0
Python version:
3.4
Bazel version (if compiling from source):
0.11.0
CUDA/cuDNN version:
9.0/7.0.5
GPU model and memory:
N/A
Exact command to reproduce:
given below in the problem description

I used DecodeJpeg method to read in images during training, I used it with dct_method="".
In order to use the same operation on Android, I compiled TensorFlow with DecodeJpeg using bazel.
I compiled it for arm64.
The problem I'm facing is that, the pixel values from both the platforms are significantly different
In Python, I use the snippet below to read image,
with tf.gfile.FastGFile(r'path\to\image.jpg', 'rb') as image_file:
    image_bytes = image_file.read()
decode_jpeg_data = tf.placeholder(dtype=tf.string)
decode_jpeg = tf.image.decode_jpeg(decode_jpeg_data_f, channels=3)
image = sess.run(decode_jpeg , feed_dict={decode_jpeg_data: image_bytes })

I use this script to execute the model on Android.
I executed DecodeJpeg with dct_method set to ' ', INTEGER_FAST, INTEGER_ACCURATE, on both desktop as well as Android.
All of them gave pixel values which were significantly different from what I got on a desktop, for the same dct_method
For instance, for the same image, at (100,100,0) the value on x86_64 is 213, while it is 204 on arm64.
The problem is that, the prediction label sometimes flips across the two platforms, for the same image.
How can I get the same behavior on both platforms?
I had asked this on StackOverflow