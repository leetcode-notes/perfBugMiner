ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients

Hello everyone,
I have error when programming tensorflow:
ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ["<tf.Variable 'Variable:0' shape=(4, 2) dtype=float32_ref>", "<tf.Variable 'Variable_1:0' shape=(4, 2) dtype=float32_ref>", "<tf.Variable 'Variable_2:0' shape=(1, 3) dtype=float32_ref>", "<tf.Variable 'Variable_3:0' shape=(1, 2) dtype=float32_ref>", "<tf.Variable 'Variable_4:0' shape=(1, 3) dtype=float32_ref>", "<tf.Variable 'Variable_5:0' shape=(1, 3) dtype=float32_ref>"] and loss Tensor("Sum:0", dtype=float32).
My code is here
=============================
import tensorflow as tf
def weight_variable(shape):
initial = tf.truncated_normal(shape, stddev = 0.1)
return tf.Variable(initial)
def bias_variable(shape):
initial = tf.constant(0.1, shape = shape)
return tf.Variable(initial)
Model parameters
#W = tf.Variable([.3], dtype=tf.float32)
#b = tf.Variable([-.3], dtype=tf.float32)
#W=weight_variable([1])
#b=bias_variable([1])
indice= tf.constant([0,1])
segment_id= tf.constant([0,0,1,1])
W=weight_variable([4,2])
b=bias_variable([4,2])
b_=tf.Variable(tf.zeros([1,3]))
t0=tf.Variable(tf.zeros([1,2]))
t1=tf.Variable(tf.zeros([1,3]))
g=tf.Variable(tf.zeros([1,3]))
Model input and output
x = tf.placeholder(tf.float32)
#linear_model = W * x + b
#forward transform
linear_function = tf.add(tf.matmul(W,x),b)
linear_function_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[0]))
linear_function_col_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[1]))
min_1=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))
min_2=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))
#b_=tf.assign(b_,[[min_1,min_2,0]])
b_=tf.assign(b_,[[min_1,min_2,0.0]])
t0=tf.assign(t0,[[min_2,min_1]])
g = tf.nn.softmax(tf.scalar_mul(1000,b_),dim=-1)
#g = tf.nn.softmax(b_)
#inverse transform
linear_function_inv = tf.divide(tf.transpose(tf.transpose(t0)-tf.transpose(b)),W)
linear_function_inv_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv), indice[0]))
linear_function_inv_col_2= tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv ), indice[1]))
max_1=tf.reduce_max(tf.segment_min(linear_function_inv_col_1,segment_id))
max_2=tf.reduce_max(tf.segment_min(linear_function_inv_col_2,segment_id))
t1=tf.assign(t1,[[max_1,max_2,0.0]])
y = tf.placeholder(tf.float32)
loss
#loss = tf.reduce_sum(tf.square(linear_model-y)) # sum of the squares
#loss = tf.reduce_sum(tf.square(g)) # sum of the squares
#loss = tf.reduce_sum(tf.square(y-tf.matmul(g,tf.transpose(t1))))
loss = tf.reduce_sum(y-tf.matmul(g,tf.transpose(t1)))
optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
training data
#x_train = [1, 2, 3, 4]
x_train_array = tf.constant([0.58975124,0.22815752])
x_train=tf.diag(x_train_array)
#y_train = [0, -1, -2, -3]
y_train=tf.constant([[0.530]])
training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
sess.run(train, {x: x_train, y: y_train})
evaluate training accuracy
curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})
print("W: %s b: %s loss: %s"%(curr_W, curr_b, curr_loss))
Hope to get your help.
Thank you