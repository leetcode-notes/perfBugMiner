ResourceExhaustedError

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.2
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.1.0
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: 8/6
GPU model and memory: NVIDIA GTX 1080 TI, 11 GB
Exact command to reproduce:

# Disable linter warnings to maintain consistency with tutorial.
# pylint: disable=invalid-name

import argparse
import sys
from collections import namedtuple

import numpy as np
import tensorflow as tf
import tensorflow.contrib.slim as slim

from dataset import DataSet

FLAGS = None
HEIGHT = 276
WIDTH = 72
DEPTH = 3
INPUT_DIMENSION = HEIGHT * WIDTH * DEPTH
NUMBER_CLASSES = 2

import numpy as np
import tensorflow as tf

class DataSet(object):

    def __init__(self,
                 images,
                 labels,
                 dtype=tf.float32,
                 reshape=False):
        dtype = tf.as_dtype(dtype).base_dtype
        if dtype not in (tf.uint8, tf.float32):
            raise TypeError(
                'Invalid image dtype %r, expected uint8 or float32' % dtype)

        assert images.shape[0] == labels.shape[0], (
            'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))
        self._num_examples = images.shape[0]

        # Convert shape from [num examples, rows, columns, depth]
        # to [num examples, rows*columns] (assuming depth == 1)
        if reshape:
            assert images.shape[3] == 1
            images = images.reshape(images.shape[0],
                                    images.shape[1] * images.shape[2])

        self._images = images
        self._labels = labels
        self._epochs_completed = 0
        self._index_in_epoch = 0

    @property
    def images(self):
        return self._images

    @property
    def labels(self):
        return self._labels

    @property
    def num_examples(self):
        return self._num_examples

    @property
    def epochs_completed(self):
        return self._epochs_completed

    def next_batch(self, batch_size, shuffle=True):
        """Return the next `batch_size` examples from this data set."""
        start = self._index_in_epoch
        # Shuffle for the first epoch
        if self._epochs_completed == 0 and start == 0 and shuffle:
            perm0 = np.arange(self._num_examples)
            np.random.shuffle(perm0)
            self._images = self.images[perm0]
            self._labels = self.labels[perm0]
        # Go to the next epoch
        if start + batch_size > self._num_examples:
            # Finished epoch
            self._epochs_completed += 1
            # Get the rest examples in this epoch
            rest_num_examples = self._num_examples - start
            images_rest_part = self._images[start:self._num_examples]
            labels_rest_part = self._labels[start:self._num_examples]
            # Shuffle the data
            if shuffle:
                perm = np.arange(self._num_examples)
                np.random.shuffle(perm)
                self._images = self.images[perm]
                self._labels = self.labels[perm]
            # Start next epoch
            start = 0
            self._index_in_epoch = batch_size - rest_num_examples
            end = self._index_in_epoch
            images_new_part = self._images[start:end]
            labels_new_part = self._labels[start:end]
            return np.concatenate((images_rest_part, images_new_part), axis=0),\
                np.concatenate((labels_rest_part, labels_new_part), axis=0)
        else:
            self._index_in_epoch += batch_size
            end = self._index_in_epoch
            return self._images[start:end], self._labels[start:end]


def deepnn(x):
    x_image = tf.reshape(x, [-1, DEPTH, HEIGHT, WIDTH])
    is_training = tf.placeholder(tf.bool)

    with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                        data_format='NCHW', padding='SAME'):
        with slim.arg_scope([slim.conv2d, slim.fully_connected],
                            weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
                            biases_initializer=tf.constant_initializer(0.1)):
            with slim.arg_scope([slim.dropout],
                                is_training=is_training):
                net = slim.conv2d(x_image, 64, [5, 5], scope='conv1')
                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool1')
                net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')
                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool2')
                net = slim.flatten(net, scope="Flatten")
                net = slim.fully_connected(net, 384, scope='fc_1',
                                           weights_regularizer=slim.l2_regularizer(0.000005))
                net = slim.fully_connected(net, 192, scope='fc_2',
                                           weights_regularizer=slim.l2_regularizer(0.000005))
                net = slim.fully_connected(net, 2, activation_fn=None, scope='fc_out')

    return net, is_training


def import_images_and_labels():
    file_path = "/path/to/file/samples.npz"
    data = np.load(file_path)

    print(data['one_hot_labels'].shape)
    print(data['images'].shape)

    images = data['images'].astype(np.float32)
    labels = data['one_hot_labels'].astype(np.float32)

    number_training_samples = 17950
    train_data = DataSet(images=images[:number_training_samples, :],
                         labels=labels[:number_training_samples, :])
    validation_data = DataSet(images=images[16000:17950, :], labels=labels[16000:17950, :])
    test_data = DataSet(images=images[17950:-1, :], labels=labels[17950: -1, :])

    DataSets = namedtuple('DataSets', ['train', 'validation', 'test'])

    return DataSets(train=train_data,
                    validation=validation_data,
                    test=test_data)


def main(_):
    dataset = import_images_and_labels()

    # Create the model
    x = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])

    # Define loss and optimizer
    y_ = tf.placeholder(tf.float32, [None, NUMBER_CLASSES])

    # Build the graph for the deep net
    y_conv, is_training = deepnn(x)

    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,
                                                                           logits=y_conv))
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    with tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=1)) as sess:
        sess.run(tf.global_variables_initializer())

        for i in range(20000):
            batch = dataset.train.next_batch(50)

            if i % 100 == 0:
                train_accuracy = accuracy.eval(feed_dict={x: batch[0],
                                                          y_: batch[1],
                                                          is_training: False})
                print('step %d, training accuracy %g' % (i, train_accuracy))

            train_step.run(feed_dict={x: batch[0],
                                      y_: batch[1],
                                      is_training: True})

        print('test accuracy %g' % accuracy.eval(feed_dict={x: dataset.test.images,
                                                            y_: dataset.test.labels,
                                                            is_training: False}))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str,
                        default='/tmp/tensorflow/mnist/input_data',
                        help='Directory for storing input data')
    FLAGS, unparsed = parser.parse_known_args()
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

== cat /etc/issue ===============================================
Linux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION="16.04.2 LTS (Xenial Xerus)"
VERSION_ID="16.04"
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.11.0)
protobuf (3.2.0)
tensorflow (1.1.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH /usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue May  9 13:52:52 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Graphics Device     Off  | 0000:01:00.0      On |                  N/A |
| 26%   46C    P8    18W / 250W |    378MiB / 11171MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1514    G   /usr/lib/xorg/Xorg                             190MiB |
|    0      1904    G   kwin_x11                                        41MiB |
|    0      1910    G   /usr/bin/krunner                                 2MiB |
|    0      1915    G   /usr/bin/plasmashell                            94MiB |
|    0      2047    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    46MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7


Describe the problem
When loading the test images in order to determine the accuracy, an ResourceExhaustedError is thrown.
This network model works fine with a private theano based framework.
Source code / logs
...
...
step 19800, training accuracy 1
step 19900, training accuracy 1
2017-05-09 13:13:15.970193: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc)
ran out of memory trying to allocate 9.21GiB.  Current allocation summary follows.
2017-05-09 13:13:15.970227: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chu
nks: 1, Chunks in use: 0 256B allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B
client-requested in use in bin.
2017-05-09 13:13:15.970239: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970247: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970257: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970266: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970275: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):  Total Chu
nks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl
ient-requested in use in bin.
2017-05-09 13:13:15.970285: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):         T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970295: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):         T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970304: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):         T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970313: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):        T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970325: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):        T
otal Chunks: 1, Chunks in use: 0 288.0KiB allocated for chunks. 18.8KiB client-requested for chunks. 0B i
n use in bin. 0B client-requested in use in bin.
2017-05-09 13:13:15.970334: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):        T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970342: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970350: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970357: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970365: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):       T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970374: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):      T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970383: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):      T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970392: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):      T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970401: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):     T
otal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi
n. 0B client-requested in use in bin.
2017-05-09 13:13:15.970413: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):     T
otal Chunks: 1, Chunks in use: 0 7.61GiB allocated for chunks. 242.58MiB client-requested for chunks. 0B
in use in bin. 0B client-requested in use in bin.
2017-05-09 13:13:15.970423: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 9.21GiB was 25
6.00MiB, Chunk State:
2017-05-09 13:13:15.970436: I tensorflow/core/common_runtime/bfc_allocator.cc:666]   Size: 7.61GiB | Requ
ested Size: 242.58MiB | in_use: 0, prev:   Size: 465.75MiB | Requested Size: 465.75MiB | in_use: 1
2017-05-09 13:13:15.970444: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600000
 of size 1280
2017-05-09 13:13:15.970450: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600500
 of size 256
2017-05-09 13:13:15.970456: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600600
 of size 256
2017-05-09 13:13:15.970463: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600700
 of size 256
2017-05-09 13:13:15.970470: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600800
 of size 256
2017-05-09 13:13:15.970476: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600900
 of size 256
2017-05-09 13:13:15.970483: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600a00
 of size 256
2017-05-09 13:13:15.970490: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600b00
 of size 256
2017-05-09 13:13:15.970496: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600c00
 of size 256
2017-05-09 13:13:15.970504: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600d00
 of size 1536
2017-05-09 13:13:15.970510: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601300
 of size 256
2017-05-09 13:13:15.970517: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601400
 of size 256
2017-05-09 13:13:15.970524: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601500
2017-05-09 13:13:15.970531: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601800
 of size 256
2017-05-09 13:13:15.970538: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601900
 of size 256
2017-05-09 13:13:15.970544: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601a00
 of size 256
2017-05-09 13:13:15.970551: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601b00
 of size 256
2017-05-09 13:13:15.970557: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601c00
 of size 256
2017-05-09 13:13:15.970565: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601d00
 of size 19200
2017-05-09 13:13:15.970571: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606800
 of size 256
2017-05-09 13:13:15.970578: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606900
 of size 488374272
2017-05-09 13:13:15.970585: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6900
 of size 1536
2017-05-09 13:13:15.970592: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6f00
 of size 294912
2017-05-09 13:13:15.970599: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80ef00
 of size 768
2017-05-09 13:13:15.970606: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f200
 of size 1536
2017-05-09 13:13:15.970612: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f800
 of size 256
2017-05-09 13:13:15.970619: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f900
 of size 19200
2017-05-09 13:13:15.970625: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814400
 of size 256
2017-05-09 13:13:15.970632: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814500
 of size 1536
2017-05-09 13:13:15.970639: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814b00
 of size 17664
2017-05-09 13:13:15.970646: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819000
 of size 256
2017-05-09 13:13:15.970653: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819100
 of size 294912
2017-05-09 13:13:15.970659: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a861100
 of size 488079360
2017-05-09 13:13:15.970666: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x102479d9100
 of size 1536
2017-05-09 13:13:15.970673: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21700
 of size 768
2017-05-09 13:13:15.970680: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21a00
 of size 256
2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00
 of size 256
2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00
 of size 256
2017-05-09 13:13:15.970693: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21c00
 of size 256
2017-05-09 13:13:15.970700: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21d00
 of size 256
2017-05-09 13:13:15.970706: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21e00
 of size 256
2017-05-09 13:13:15.970713: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22000
 of size 256
2017-05-09 13:13:15.970720: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22100
 of size 256
2017-05-09 13:13:15.970726: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22200
 of size 256
2017-05-09 13:13:15.970733: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22300
 of size 19200
2017-05-09 13:13:15.970740: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a26e00
 of size 19200
2017-05-09 13:13:15.970747: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2b900
 of size 256
2017-05-09 13:13:15.970753: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2ba00
 of size 256
2017-05-09 13:13:15.970760: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2bb00
 of size 488374272
2017-05-09 13:13:15.970767: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10264bebb00
 of size 488374272
2017-05-09 13:13:15.970773: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dabb00
 of size 1536
2017-05-09 13:13:15.970780: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac100
 of size 1536
2017-05-09 13:13:15.970787: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac700
 of size 294912
2017-05-09 13:13:15.970793: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281df4700
 of size 294912
2017-05-09 13:13:15.970800: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3c700
 of size 768
2017-05-09 13:13:15.970807: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3ca00
 of size 768
2017-05-09 13:13:15.970814: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3cd00
 of size 1536
2017-05-09 13:13:15.970820: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d300
 of size 1536
2017-05-09 13:13:15.970827: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d900
 of size 256
2017-05-09 13:13:15.970834: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3da00
 of size 256
2017-05-09 13:13:15.970840: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3db00
 of size 19200
2017-05-09 13:13:15.970847: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e42600
 of size 488374272
2017-05-09 13:13:15.970855: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x102479d9700
of size 294912
2017-05-09 13:13:15.970861: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x10247a21f00
of size 256
2017-05-09 13:13:15.970868: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1029f002600
of size 8176048640
2017-05-09 13:13:15.970875: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use
 Chunks by size:
2017-05-09 13:13:15.970884: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 31 Chunks of size 256
totalling 7.8KiB
2017-05-09 13:13:15.970892: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 768 t
otalling 3.8KiB
2017-05-09 13:13:15.970899: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280
totalling 1.2KiB
2017-05-09 13:13:15.970908: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 9 Chunks of size 1536
totalling 13.5KiB
2017-05-09 13:13:15.970916: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 17664
 totalling 17.2KiB
2017-05-09 13:13:15.970924: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 19200
 totalling 93.8KiB
2017-05-09 13:13:15.970931: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 29491
2 totalling 1.12MiB
2017-05-09 13:13:15.970940: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 48807
9360 totalling 465.47MiB
2017-05-09 13:13:15.970947: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 48837
4272 totalling 1.82GiB
2017-05-09 13:13:15.970955: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use ch
unks: 2.27GiB
2017-05-09 13:13:15.970966: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:
Limit:                 10619240448
InUse:                  2442896640
MaxInUse:               2995339008
NumAllocs:                  803473
MaxAllocSize:            488374272
2017-05-09 13:13:15.970984: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **********************
**____________________________________________________________________________
2017-05-09 13:13:15.971003: W tensorflow/core/framework/op_kernel.cc:1152] Resource exhausted: OOM when a
llocating tensor with shape[1945,64,276,72]
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1039, in _do_ca
ll
    return fn(*args)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1021, in _run_f
n
    status, run_metadata)
  File "/usr/lib/python3.5/contextlib.py", line 66, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py", line 466, in
raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194
5,64,276,72]
         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", padding="SAME", strides=[1, 1
, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/gpu:0"](Reshape, conv2/weights/r
ead)]]
         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/
cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge
_20_Mean_1", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py", line 145, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py", line 136, in main
    is_training: False}))
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 569, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 3741, in _eval_u
sing_default_session
    return session.run(tensors, feed_dict)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 778, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 982, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1032, in _do_ru
n
    target_list, options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1052, in _do_ca
ll
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194
5,64,276,72]
         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", padding="SAME", strides=[1, 1
, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/gpu:0"](Reshape, conv2/weights/r
ead)]]
         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/
cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge
_20_Mean_1", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]

Caused by op 'conv2/convolution', defined at:
  File "/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py", line 145, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py", line 110, in main
    y_conv, is_training = deepnn(x)
  File "/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s
lim.py", line 56, in deepnn
    net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py", lin
e 181, in func_with_args
    return func(*args, **current_args)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py", line 9
18, in convolution
    outputs = layer.apply(inputs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py", line 320, in apply
    return self.__call__(inputs, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py", line 290, in __call__
    outputs = self.call(inputs, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py", line 156, in c
all
    data_format=utils.convert_data_format(self.data_format, self.rank + 2))
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py", line 661, in convolution
    op=op)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py", line 331, in with_space_
to_batch
    return op(input, num_spatial_dims, padding)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py", line 653, in op
    name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py", line 129, in _non_atrous
_convolution
    name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py", line 403, in conv2d
    data_format=data_format, name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py", line 768,
in apply_op
    op_def=op_def)
File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 2336, in create_
op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 1228, in __init_
_
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1945,64,276,72]
         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", padding="SAME", strides=[1, 1
, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/gpu:0"](Reshape, conv2/weights/r
ead)]]
         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/
cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge
_20_Mean_1", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]