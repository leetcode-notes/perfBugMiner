Generating NaN when computing gradient

Environment info
Operating System:
Red Hat Enterprise Linux Server release 6.6
Tensorflow version:
0.10.0rc0
Installed version of CUDA and cuDNN:
/usr/local/cuda/lib64/libcudart.so.7.5.23
I'm running a model with temporal attention strategy (https://arxiv.org/abs/1608.02927). I use tf.nn.seq2seq.sequence_loss_by_example() to compute the loss, and use adam gradient (lr:0.001) to minimize loss. The loss is not NaN, but gradients of all weigths became NaN values. If I use plain attention strategy, it won't have this NaN problem.
I even print out all hyperparameters, their values land in a sensible range until their gradients become NaN.
Hope someone can help me fix this issue. Thanks in advance.