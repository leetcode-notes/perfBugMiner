Cannot import graph_def for 8-bit Quantized cnn model

Following steps I did:

Train a CNN model on MNIST using tensorflow tutorials(Deep MNIST for Experts)(CONV+CONV+FC). Saved this model as binary protobuf(.pb).
Using 8-bit Quantization api, converted previously saved model to 8-bit model as binary proto(.pb).
Read 8-bit binary-proto to tensorflow, by first creating graph_def and importing graph def to tf.Session()

with sess.as_default() :
   with tf.Graph().as_default():
       graph_def = tf.GraphDef()
             with open(input_file_name, 'rb') as f:
                 proto_b = f.read()
                 graph_def.ParseFromString(proto_b)	
                 _ = tf.import_graph_def(graph_def, name="")
However, this gives error on tf.import_graph_def as
ValueError: graph_def is invalid at node u'concat_eightbit_reshape_concat/values_0': Input tensor 'concat/values_0:0' Cannot convert a tensor of type int32 to an input of type float32.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System: Ubuntu 16.04 + tensorflow 0.11.0rc2
Installed version of CUDA and cuDNN: No
If installed from binary pip package, provide:

A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl

What other attempted solutions have you tried?

I am able to correctly import quantized graph def for pre-trained inception model(.pb).
When I define Multi-layer perceptron model for MNIST, it can import quantized graph_def.
All the quantization headers are imported correctly as

from tensorflow.contrib.quantization import load_quantized_ops_so
from tensorflow.contrib.quantization.kernels import load_quantized_kernels_so
load_quantized_ops_so.Load()
load_quantized_kernels_so.Load()