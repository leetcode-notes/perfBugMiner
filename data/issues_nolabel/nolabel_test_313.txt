Allow learning rates of specific variables to be scaled

When repurposing a network, it can be useful to set layer-wise learning rates so that the final layer (which has random weights) does most of the learning initially. A useful pattern for applying learning rate scaling would be tf.stop_gradient(input, name=None), which can be seen as effectively scaling the learning rate by a factor of zero (obviously it also stops computation).