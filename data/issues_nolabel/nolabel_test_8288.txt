Dynamic RNN, Initialization, Control Dependencies

I am trying to write a custom RNN Cell with regularized weights and am encountering a problem caused by the treatment of control flow dependencies.
I initialize variables in the RNNCell as follows:
with tf.variable_scope(scope):
w = tf.get_variable(..., reg=tf.layers.l2_reg())
and use the cell in a dynamic_rnn call.
At the end of graph construction, I retrieve all the regularized losses (using tf.GraphKeys.REGULARIZATION_LOSSES).
When I add the regularization losses to my cross entropy loss, I run into the following error: "InvalidArgumentError: The node 'Sum/input' has inputs from different frames." It seems that, because the variables are only created conditionally, they can't be used together with other tensors.
Could you please suggest a workaround or provide a mechanism for adding regularization to an RNN?