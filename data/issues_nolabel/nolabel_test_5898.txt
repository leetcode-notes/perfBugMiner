pool efficiency on cpu

What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I have searched about cpu usage and try everything I can. But it's still pool efficiency usage, only use 30% of my cpu
I thought it maybe the train-dataset read speed limitation?
Environment info
Operating System:
centos6.5 with latest version of tensorflow
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
full code here
https://github.com/tobegit3hub/deep_recommend_system/blob/master/a8a_classifier.py
# Read TFRecords files for training
filename_queue = tf.train.string_input_producer(
    tf.train.match_filenames_once("data/a8a_train.libsvm.tfrecords"),
    num_epochs=epoch_number)
serialized_example = read_and_decode(filename_queue)
batch_serialized_example = tf.train.shuffle_batch(
    [serialized_example],
    batch_size=batch_size,
    num_threads=thread_number,
    capacity=capacity,
    min_after_dequeue=min_after_dequeue)

What other attempted solutions have you tried?
try the specify the thread number when init session , the shuffle_batch num_threads etc.
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
here is a tensorboard screenshot