Add `Dataset.from_variable_length` to accept numpy arrays with varying length

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS x
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0
Python version: Python 3.5.2 :: Continuum Analytics, Inc.
Bazel version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

import numpy as np
from tensorflow.contrib.data import Dataset
a=[np.arange(3), np.arange(5)]
Dataset.from_tensor_slices(np.asarray(a))

Describe the problem
Dataset has a nice .padded_batch feature which allows padding batches to ensure all tensors have the same size, which is very nice for the common use-case of having sequences of different lengths.
However, it seems impossible to create such a dataset from a list of numpy array with different lengths. See command above.
It would be very nice to have a Dataset.from_variable_length_tensor_slices or such that could take such inputs.
I know I'm supposed to use a TextLineDataset or something similar, and then use a series of map functions, but I think tensorflow should support doing all of the pre-processing outside of tensorflow instead of forcing it into a delayed execution graph paradigm, which is harder to reason about and debug. Also, my data is stored in JSON files, and requires complex pre-processing, so the TextLineDataset/map approach doesn't cut it.