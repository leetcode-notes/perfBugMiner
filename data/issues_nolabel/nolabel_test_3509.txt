Check failed,  cuCtxSetCurrent(context). training cifar10 multi-gpu example on 2 gpus.

I'm having trouble getting the multiple GPU example to work. Here is the full output:
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                            
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                             
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally                                                                                                                
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally                                                                                                               
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally                                                                                                               
>> Downloading cifar-10-binary.tar.gz 100.0%                                                                                                                                            
Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:02:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)
F tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)

Environment info
Operating System: CentOS release 6.7 (Final). Using a docker image available here: gideonitemd/hal-tf.
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
cudnn/7.0/lib64/libcudnn.so
cudnn/7.0/lib64/libcudnn.so.7.0.64
cudnn/7.0/lib64/libcudnn.so.7.0
cudnn/7.0/lib64/libcudnn_static.a
/usr/lib/libcuda.so
/usr/lib/libcuda.so.1
/usr/lib/libcuda.so.352.39
(should be libcuda v7.5)
Running python -c "import tensorflow; print(tensorflow.__version__)" yields 0.8.0
Steps to reproduce
Run this (the output of the docker_run_gpu.sh script):
docker run -it -v /usr/lib64/libcuda.so:/usr/lib64/libcuda.so -v /usr/lib64/libcuda.so.1:/usr/lib64/libcuda.so.1 -v /usr/lib64/libcuda.so.352.39:/usr/lib64/libcuda.so.352.39 -v /lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko:/lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko -v /lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin,:/lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin, --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia1:/dev/nvidia1 --device /dev/nvidia2:/dev/nvidia2 --device /dev/nvidia3:/dev/nvidia3 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm --env LD_LIBRARY_PATH=/cbio/shared/software/cudnn/7.0/lib64:/usr/local/cuda-7.5/lib64:/usr/local/cuda-7.5/lib:/usr/local/cuda-7.5/targets/x86_64-linux/lib/:/opt/mpich2/gcc/eth/lib:/opt/gnu/gcc/4.8.1/lib64:/opt/gnu/gcc/4.8.1/lib:/opt/gnu/gmp/lib:/opt/gnu/mpc/lib:/opt/gnu/mpfr/lib:/usr/lib64/ --env CUDA_VISIBLE_DEVICES=0,1 -v /cbio/ski/fuchs/projects/tissue-microarray-resnet/:/mnt/data -v /cbio/ski/fuchs/home/dresdnerg/software/tensorflow/tensorflow/models/image/cifar10:/mnt/code -it gideonitemd/tmrn python cifar10_multi_gpu_train.py --num_gpus 2

What have you tried?

Messing with CUDA_VISIBLE_DEVICES: empty, =0, =0,1. All failed.
Looking at other issues with the same error, none address this specific problem since it works fine on a single GPU.
nvidia-smi shows no other jobs accessing the GPU
docker ps shows no zombie images

Logs or other output that would be helpful
(If logs are large, please upload as attachment).