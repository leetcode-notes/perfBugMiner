tf.train.batch does not preserve the order of data and miss some data.

I use tf.train.batch to produce data in tensorflow but get unexpected data order as following:
# Create a queue that produces the filenames to read.
filename_queue = tf.train.string_input_producer(string_tensor=filenames, num_epochs=1, shuffle=False)

......

num_preprocess_threads = 1 
images_x, images_y, label_batch = tf.train.batch(
            [image[0], image[1], label],
            batch_size=1,
            num_threads=num_preprocess_threads,
            capacity=1)

the label produced by tf.train.batch: [ 6.03125     5.2734375   4.03125   5.84375     5.15625   ....]. A total of 33 labels.
the label of original data: [ 6.03125     5.2734375   4.546875    4.03125     6.1875  5.8438 ...]. A total of 68 labels.
It seems that the generated data differs from the original data in the order and is less. Why?