problem with optimize_for_inference

hi,
I'm using tensorflow 1.3.0 installed via pip with python 2.7.12 (Ubuntu 16.04, cuda 8, nvidia 1060)
when i try to optimize a custom model trained with tensorflow 1.3.0 with
python -m tensorflow.python.tools.optimize_for_inference --input saved_model.pb --output opt_model.pb --input_names=in --output_names=out
i get the following error:
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/home/********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/home/**********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py", line 83, in main
    input_graph_def.ParseFromString(data)
  File "/home/***********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/message.py", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py", line 1069, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py", line 633, in DecodeField
    if value._InternalParse(buffer, pos, new_pos) != new_pos:
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py", line 612, in DecodeRepeatedField
    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py", line 743, in DecodeMap
    if submsg._InternalParse(buffer, pos, new_pos) != new_pos:
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py", line 1095, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py", line 799, in _SkipGroup
    new_pos = SkipField(buffer, pos, end, tag_bytes)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py", line 814, in _SkipFixed32
    raise _DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message.

i already did a ' export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python'
without doing it i got
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/home/*********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/home/*********/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/home/*********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py", line 83, in main
    input_graph_def.ParseFromString(data)
google.protobuf.message.DecodeError: Error parsing message

I'm saving the model like:
builder = saved_model_builder.SavedModelBuilder("model"))
builder.add_meta_graph_and_variables(sess,['serve'],signature_def_map= {"model": tf.saved_model.signature_def_utils.predict_signature_def(inputs= {"in" : X }, outputs= {"out": pred_Y })})
builder.save()

i had the same issue with 1.2.1, after this i did the upgrade to 1.3.0 via pip install --upgrade and a retrain.
i have no issues loading and using the model with tensorflow-rust in my test application, but to add some context i got the error
Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef 
at  org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:392) at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:96)

when trying to load the model with tensorflow on android. first i thought this would be because of some unsupported ops, so i went the optimize_for_inference way. but i guess it looks like a protobuf issue
i would appreciate any hint to solve this issue.
with best regards