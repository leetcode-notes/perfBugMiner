How can I use BatchNorm in a multi-GPU model

I want to use batch normalization in the cifar10 example in a multi-GPU structure. All variables are stored in CPU, I build 2 Queues one for training batch and one for testing batch;  5 models( 4 for training and another for testing),
GPU_tower codes like this

Problem is: when I add tf.contrib.batch_norm layers in my NN model . How can I  reuse those batchnorm variables ? I set reuse = True and pass namescope but get valueError. Is there a simple way to make it work? I don't want to modify a lot.
I know there are some high level framework such as slim with the inception model with a built-in batch-norm
but I can hardly do personal implementations (I need to add many instructions in slim.conv2d and tf.contrib.conv2d, so it's more convenient to use tf.nn.conv2d),
Could I just follow the inception Sync method but without high level frame like slim ?
Thanks a lot!