Serialization error in freeze_graph and/or optimize_for_inference_lib

I haven't found any mention of this anywhere online.
After applying a workaround to #8404 another error turns up:
E/TensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/optimized_model.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: read/bn4/BatchNorm/cond/AssignMovingAvg_1/decay = Const[_class=["loc:@BatchNorm_3/moving_variance"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0.999>](read/bn4/BatchNorm/cond/Switch:1)
I have no ideas on what this error message even tries to tell me nor how to resolve the issue.