Luong attention fails when used with scale=True and dtype=tf.float16

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): b'v1.6.0-rc1-1857-g67e2efa' 1.6.0
Python version: 3.5.2
Bazel version (if compiling from source): 0.10.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: 9.0/7.0
GPU model and memory: not relevant
Exact command to reproduce:

import tensorflow as tf
dtype = tf.float16

with tf.variable_scope("name", dtype=dtype):
    cell = tf.nn.rnn_cell.LSTMCell(128)

    encoder_outputs = tf.placeholder(dtype, shape=[64, None, 256])
    input_lengths = tf.placeholder(tf.int32, shape=[64])
    tgt_lengths = tf.placeholder(tf.int32, shape=[64])
    input_vectors = tf.placeholder(dtype, shape=[64, None, 128])

    attention_mechanism = tf.contrib.seq2seq.LuongAttention(
        num_units=128,
        memory=encoder_outputs,
        scale=True,
        memory_sequence_length=input_lengths,
        probability_fn=tf.nn.softmax,
        dtype=dtype,
    )
    attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism)

    helper = tf.contrib.seq2seq.TrainingHelper(
        inputs=input_vectors,
        sequence_length=tgt_lengths,
    )

    decoder = tf.contrib.seq2seq.BasicDecoder(
        cell=attn_cell,
        helper=helper,
        initial_state=attn_cell.zero_state(64, dtype),
    )

    tf.contrib.seq2seq.dynamic_decode(decoder=decoder)

Describe the problem
Luong attention fails when using with scale=True and dtype=tf.float16. Changing lines 341-342 of attention_wrapper.py to:
g = variable_scope.get_variable(
    "attention_g", dtype=dtype, shape=(),
    initializer=init_ops.ones_initializer(),
)

seems to solve the problem.
Source code / logs
Traceback:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-4dec9cd8e3b2> in <module>()
     31     )
     32 
---> 33     tf.contrib.seq2seq.dynamic_decode(decoder=decoder)

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)
    307         ],
    308         parallel_iterations=parallel_iterations,
--> 309         swap_memory=swap_memory)
    310 
    311     final_outputs_ta = res[1]

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)
   3203     if loop_context.outer_context is None:
   3204       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
-> 3205     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
   3206     if maximum_iterations is not None:
   3207       return result[1]

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)
   2941       with ops.get_default_graph()._lock:  # pylint: disable=protected-access
   2942         original_body_result, exit_vars = self._BuildLoop(
-> 2943             pred, body, original_loop_vars, loop_vars, shape_invariants)
   2944     finally:
   2945       self.Exit()

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2878         flat_sequence=vars_for_body_with_tensor_arrays)
   2879     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-> 2880     body_result = body(*packed_vars_for_body)
   2881     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
   2882     if not nest.is_sequence(body_result):

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)
    252       """
    253       (next_outputs, decoder_state, next_inputs,
--> 254        decoder_finished) = decoder.step(time, inputs, state)
    255       if decoder.tracks_own_finished:
    256         next_finished = decoder_finished

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)
    135     """
    136     with ops.name_scope(name, "BasicDecoderStep", (time, inputs, state)):
--> 137       cell_outputs, cell_state = self._cell(inputs, state)
    138       if self._output_layer is not None:
    139         cell_outputs = self._output_layer(cell_outputs)

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)
    230         setattr(self, scope_attrname, scope)
    231       with scope:
--> 232         return super(RNNCell, self).__call__(inputs, state)
    233 
    234   def _rnn_get_variable(self, getter, *args, **kwargs):

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    712 
    713         if not in_deferred_mode:
--> 714           outputs = self.call(inputs, *args, **kwargs)
    715           if outputs is None:
    716             raise ValueError('A layer\'s `call` method should return a Tensor '

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)
   1409       attention, alignments, next_attention_state = _compute_attention(
   1410           attention_mechanism, cell_output, previous_attention_state[i],
-> 1411           self._attention_layers[i] if self._attention_layers else None)
   1412       alignment_history = previous_alignment_history[i].write(
   1413           state.time, alignments) if self._alignment_history else ()

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _compute_attention(attention_mechanism, cell_output, attention_state, attention_layer)
   1046   """Computes the attention and alignments for a given attention_mechanism."""
   1047   alignments, next_attention_state = attention_mechanism(
-> 1048       cell_output, state=attention_state)
   1049 
   1050   # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in __call__(self, query, state)
    427     """
    428     with variable_scope.variable_scope(None, "luong_attention", [query]):
--> 429       score = _luong_score(query, self._keys, self._scale)
    430     alignments = self._probability_fn(score, state)
    431     next_state = alignments

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _luong_score(query, keys, scale)
    340     # Scalar used in weight scaling
    341     g = variable_scope.get_variable(
--> 342         "attention_g", dtype=dtype, initializer=1.)
    343     score = g * score
    344   return score

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)
   1315       partitioner=partitioner, validate_shape=validate_shape,
   1316       use_resource=use_resource, custom_getter=custom_getter,
-> 1317       constraint=constraint)
   1318 get_variable_or_local_docstring = (
   1319     """%s

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)
   1064         if init_dtype != dtype:
   1065           raise ValueError("Initializer type '%s' and explicit dtype '%s' "
-> 1066                            "don't match." % (init_dtype, dtype))
   1067       if initializer is None:
   1068         initializer = self._initializer

ValueError: Initializer type '<dtype: 'float32'>' and explicit dtype '<dtype: 'float16'>' don't match.