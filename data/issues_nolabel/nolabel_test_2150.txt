rnn_cell.py improvements

Motivation
Current implementation of rnn_cell.py does not support custom initialization on a gate and input-to-hidden/hidden-to-hidden level (like setting forgetgate bias to 0 while leaving updategate bias at 1, etc.). Further when debugging the gates in TensorBoard, the matrix is represented as one large matrix, which makes it difficult to see whats happening inside a specific matrix (e.g. hidden-to-hidden) of a specific gate (e.g. forgetgate).
In recurrent neural networks (RNNs) GRU and LSTM uses various gates with separate weights, for both hidden-to-hidden and input-to-hidden, to computing steps in a recurrent sequence.
To optimize computational speed these gates, and their separate weights, are often stacked and computed simultaneous at every step.
In TensorFlows rnn_cell.py the GRUCell and BasicLSTMCell are implemented using linear to handle weigths and computation hereof.
However, the implementation of linear does not initialize separate matrices for each gate, but initializes the gates, and their input-to-hidden/hidden-to-hidden matrices, as one big matrix for weights and bias.
Proposal
Implementing separate initialization of gates, and their input-to-hidden/hidden-to-hidden matrices, and concatenating these gates. This allows custom initializatio, TensorBoard information on hid_in/hid_hid/bias for every gate and still retains the advantage of weights in a large matrix.
Implementation
With minimal rewriting of the current structure in rnn_cell.py I implemented a lasagne-gate like structure, made a new linear function and made some minor changes to GRUCell.
All of these changes should, with minor modifications, work for BasicLSTM as well.
Do notice that code below is for my own purpose, it is not rigorously tested yet.
First, the gate to hold initialization for every weight matrix/bias in a gate (notice it also handles LSTM's by having W_cell)
class Gate(object):
  """Gate to handle to handle initialization"""  

  def __init__(self, W_in=init_ops.random_normal_initializer(stddev=0.1),
               W_hid=init_ops.random_normal_initializer(stddev=0.1),
               W_cell=init_ops.random_normal_initializer(stddev=0.1),
               b=init_ops.constant_initializer(0.),
               activation=None):
    self.W_in = W_in
    self.W_hid = W_hid
    # Don't store a cell weight vector when cell is None
    if W_cell is not None:
        self.W_cell = W_cell
    if b is not None:
      self.b = b
    # For the activation, if None is supplied, use identity
    if activation is None:
        self.activation = control_flow_ops.identity
    else:
        self.activation = activation
A modified GRU cell to handle weigths (took only minimal modification to init and call)
class GRUCell(rnn_cell.RNNCell):
  """Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""

  def __init__(self, num_units, input_size=None,
               resetgate=Gate(W_cell=None, activation=sigmoid),
               updategate=Gate(W_cell=None, activation=sigmoid),
               candidategate=Gate(W_cell=None, activation=tanh)):
    self._num_units = num_units
    self._input_size = num_units if input_size is None else input_size
    self._resetgate = resetgate
    self._updategate = updategate
    self._candidategate = candidategate

  @property
  def input_size(self):
    return self._input_size

  @property
  def output_size(self):
    return self._num_units

  @property
  def state_size(self):
    return self._num_units

  def __call__(self, inputs, state, scope=None):
    """Gated recurrent unit (GRU) with nunits cells."""
    with vs.variable_scope(scope or type(self).__name__):  # "GRUCell"
      with vs.variable_scope("Gates"):  # Reset gate and update gate.
        # We start with bias of 1.0 to not reset and not update.
        r, u = array_ops.split(1, 2, Modified_linear([inputs, state],
          [(self._num_units, "Reset", self._resetgate),
           (self._num_units, "Update", self._updategate)]))
        r, u = self._resetgate.activation(r), self._updategate.activation(u)
      with vs.variable_scope("Candidate"):
        c = Modified_linear([inputs, r * state],
          (self._num_units, "Candidate", self._candidategate))
        c = self._candidategate.activation(c)
      new_h = u * state + (1 - u) * c
    return new_h, new_h
I found linear required the largest amount of rewriting, however I have tried to keep the original structure and functionality intact.
def Modified_linear(args, output, scope=None):
  """Modified linear takes args and output.
     Args is same as in linear, but output is a tuple consisting of:
     output_size, name of gate, gate object (with all initializations)
  """
  if args is None or (isinstance(args, (list, tuple)) and not args):
    raise ValueError("`args` must be specified")
  if not isinstance(args, (list, tuple)):
    args = [args]
  if not isinstance(output, list):
    output = [output]
  shapes = [a.get_shape().as_list() for a in args]
  for shape in shapes:
    if len(shape) != 2:
      raise ValueError("Linear is expecting 2D arguments: %s" % str(shapes))
    if not shape[1]:
      raise ValueError("Linear expects shape[1] of arguments: %s" % str(shapes))

  matrices = []
  biases = []
  with vs.variable_scope(scope or "Linear"):
    for output_size, name, gate in output: # loops over every gate
      with vs.variable_scope(name):
        W_in = vs.get_variable("W_in", [args[0].get_shape()[1], output_size],
          initializer=gate.W_in)
        W_hid = vs.get_variable("W_hid", [args[1].get_shape()[1], output_size],
          initializer=gate.W_hid)
        if hasattr(gate, 'b'):
          b = vs.get_variable("Bias", [output_size],
            initializer=gate.b)
          biases.append(b)
        if hasattr(gate, "W_cell"):
          pass
          # do some LSTM stuff ...
        else:
          matrix = array_ops.concat(0, [W_in, W_hid]) # concats all matrices
        matrices.append(matrix)

  total_matrix = array_ops.concat(1, matrices) # concats across gates
  res = math_ops.matmul(array_ops.concat(1, args), total_matrix) # computes the results

  if biases is not []:
    total_bias = array_ops.concat(0, biases) # concats across gates biases
    if total_matrix.get_shape()[1] != total_bias.get_shape()[0]:
      raise ValueError('Must have same output dimensions for W and b')
    res += total_bias
  return res
Questions

Would this be of interest for a PR to rnn_cell.py? (given further development of code and BasicLSTMCell implementation)
General comments/thoughts would be much appreciated