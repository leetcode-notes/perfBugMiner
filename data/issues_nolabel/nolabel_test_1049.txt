How to set the max_grad_norm with AdamOptimizer?

New to the TensorFlow.
I am working closed with RNN. To avoid overfitting, I am trying to control the max_grad_norm when the gradient passes through.
Here is the code I worked with.
    # Define optimizer with norm limitation
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred_y, y)) # Softmax loss
tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam Optimizer
optimizer.apply_gradients(zip(grads, tvars))

I kept receiving the error that
TypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> of <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)
All suggestions are more than welcome.
Thanks