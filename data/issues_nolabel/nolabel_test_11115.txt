Unecessary type checking for shape_invariants in tf.while_loop()

Apologies in advance if this is not the right place to post this or I did something wrong, I am new to GitHhub issues and TensorFlow.
Describe the problem
In tf.while_loop, when passing in shape_invariants, it is not (easily) possible to specify a shape invariant for a state variable belonging to a BasicLSTMCell. This is because the tf.while_loop makes a nest.assert_same_structure(loop_vars, shape_invariants) call, and uses the default parameter type_check=True. what this means, however, is that there is no way to manually pass a nested tuple in to specify an invariant for the state. For example, if the shape invariant for the LSTM state in the shape_invariants tuple is
tuple(tf.TensorShape((None, size)) for size in lstm_cell.state_size)
then tf.while_loop fails with the exception
TypeError: The two structures don't have the same sequence type. First structure has type <class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'>, while second structure has type <type 'tuple'>.
However, LSTMStateTuple is simply a named tuple, so why does this not work? You can circumvent this restriction with the following code:
tf.contrib.rnn.LSTMStateTuple(*tuple(tf.TensorShape((None, size)) for size in lstm_cell.state_size))
But this seems like a hack and just feels wrong. I think that either type checking should be turned off for the purposes of shape_invariants, or some more intelligent type checking should be applied. Would this make sense?