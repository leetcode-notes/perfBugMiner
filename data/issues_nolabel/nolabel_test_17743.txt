Training through an Estimator is much slower than writing a for loop when using a Dataset

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Happens with stock code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu Server 17.10.1
TensorFlow installed from (source or binary):
Source
TensorFlow version (use command below):
b'v1.6.0-0-gd2e24b6039' 1.6.0
Python version:
3.6
Bazel version (if compiling from source):
0.11.0
GCC/Compiler version (if compiling from source):
gcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010
CUDA/cuDNN version:
9.1/7.0.5
GPU model and memory:
not applicable
Exact command to reproduce:
not applicable

I have a dataset that consists of one feature and one label, both are int64, there are 7166 steps of batches consisting of 128 and run for 10 epochs. I have tested this using two approaches, the first uses 100 tfrecord files that are read through the tf.data.Dataset API, the second reads the tfrecord data into memory then creates a tf.data.Dataset utilizing from_tensor_slices.
Obviously the in-memory version runs much faster than the read-from-disk method.
When I train my model using a custom while loop, 10 epochs runs 20 seconds faster than running it using the tf.estimator.Estimator.train API. This 20 seconds is a blanket 20 seconds, whether I am running it using the in-memory dataset or the read-from-disk method.
I thought, the overhead could be from storing checkpoints, etc. so I supplied a RunConfig that disables checkpoints and sets the various save/log step counts to very high numbers. This helped, but only a little bit, it went from a blanket 20 seconds to a blanket 17 seconds. 17 seconds doesn't seem like much, but if I am running for a large number of epochs, or if I use a much larger dataset, those seconds can turn into hours.
Is this something inherent with tf.estimator.Estimator or could I be hitting a problem?