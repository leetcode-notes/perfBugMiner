Unnecessary label division at tf.nn.nce_loss?

As far as I know, NCE Loss is a sampling-based loss that converts large-scale multiclass loss into a sum of binary loss for sampled classes. Each binary classification infers a probability where the given context and word is from the proxy corpus (real distribution) or noise distribution. Therefore, I guess each binary classification of word should be hard binary classification with label 0.0 or 1.0.
However, the documentation and the implementation of tf.nn.nce_loss (https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L1044) indicates that the target probability is assigned to 1 / num_true to make target probabilities sum to 1. Implementation of tf.nn.nce_loss uses the helper function _compute_sampled_logits in same file, which always returns true label divided by the number of true examples (Line 1044).
Is this division necessary for NCE Loss? Isn't the label of <1.0 for positive examples generates the unnecessary opposite-direction loss (1-y)log(1-y')? Is there any other reason that I missed for this label division?