Why the leaky_relu is betterï¼Ÿ

def lrelu(x, leak=0.2, name="lrelu"):
with tf.variable_scope(name):
f1 = 0.5 * (1 + leak)
f2 = 0.5 * (1 - leak)
return f1 * x + f2 * abs(x)
I don't know why is better than tf.maximum(0.2*x,x)
Could someone tell me why?
I really want to know.