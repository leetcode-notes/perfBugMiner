Wrong Logistic Loss

In tensorflow/python/ops/nn.py, the logistic loss is described as
x - x * z + log(1 + exp(-x))
but this is not what's commonly known as the logistic loss.
I see that you're trying to apply a trick here to avoid overflow, but it should be
x - x * z + log(exp(0 - (x - x * z)) + exp(-x))
instead of what you have.
Of course, the correct version is now prone to underflow, so it would make most sense to add a native robust log-sum-exp function to tensorflow and then rely on that to avoid over- and underflow.