Crossentropy loss function with weights by sample and by category and outcome

In my loss function I would like to weight each sample differently and in each sample, each category should be weighted differently as well depending on the outcome. Meaning if in a cross entropy the one_hot is correctly specified, a different weight needs to be applied than when the output is incorrect. So I would need two weights per category. A tensor with rank 3. One dimension for the samples, a second dimension for the amount of classes, and a third dimension that differentiates between correct and incorrect match.
I have seen that with sparse_softmax_cross_entropy it is possible to pass in a weight, that serves as a coefficient for positive examples. This is a good start, but I would need to pass in a tensor instead, to treat each sample differently. weighted_cross_entropy_with_logits seems to work in a very similar way but doesn't offer that functionality.
Is this a feature that could be added?