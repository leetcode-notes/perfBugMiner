Add adaptive softmax implemtation in nn_impl.py

Add a implementation of AdaptiveSoftmax, see Efficient softmax approximation for GPUs for detail.
The adaptive softmax is a faster way to train a softmax classifier over a huge number of classes, and can be used for both training and prediction. For example, it can be used for training a language model with a very huge vocabulary, and the trained languaed model can be used in speech recognition, text generation, and machine translation very efficiently .
It has been tested for language modeling on PTB (Penn Treebank) and GBW (Google Billion Word) corpus, and achieved a better result than the Torch implementation.
On GBW corpus, we achived a perplexcity of 43.24 after 5 epochs, taking about two days to train on 2 GPUs with synchronous gradient updates. Detail experiment result and usage demo can be found here: TencentAILab/tf-adaptive-softmax-lstm-lm.
Further more, it has been used in the ASR system developed by Tencent AI Lab, and achieved about 20x speed up than the full sotfmax in the second pass for rescoing.