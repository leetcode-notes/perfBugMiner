Is it possible to implement Sparse Cross Entropy or Sparse Softmax Cross Entropy with Smooth Threshold to deal with large loss?

I use sparse_softmax_cross_entropy_with_logits in Seq2Seq task with large vocabulary.
But sometime I got a large loss, such as 1000. It's too large for optimization. So I think to add a smooth threshold in sparse_softmax_cross_entropy_with_logits may deal with this problem.
sm = tf.nn.softmax(logit)
sm = sm + threshold
ce = sparse_cross_entropy(sm, target)
BTW, I clip logit for smoothness now. But I'm not sure if it's a good idea.
logit = clip(logit, -threshold, threshold)
Does anyone have better solution to deal with large loss or will implement Sparse Cross Entropy?
Thanks so much!