distributed tensorflow does not use GPU 1,2,3 on server B

Environment info
Operating System: Ubuntu 14.04 desktop
Installed version of CUDA and cuDNN: 7.5,  5.0.5
tensorflow 0.9.0.rc0 is installed from source.

I'm using distributed tensorflow with 8 Titan-x GPUs in two servers.
4 GPUs are in one server.
GU=gpu utilization
These are what I tested, changing ps and workers.
Cifar-10, ResNet. batchsize=32.


serverA (1 ps, 1 worker),  serverB (None) ==> Mem allocated on (serverA: GPU 0: GU>30%)


serverA (1 ps, 4 workers),  serverB (None)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%)


serverA (1 ps, 1 workers),  serverB (1 ps, 1 worker)  ==> Mem allocated on (serverA: GPU 0,1: GU>30%), (serverB: GPU 0: GU ~8%).


serverA (1 ps, 1 workers),  serverB (1 ps, 2 workers)  ==> Mem allocated on (serverA: GPU 0,1,2: GU>30%), (serverB: GPU 0: GU ~8%).


serverA (1 ps, 1 workers),  serverB (1 ps, 3 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%), (serverB: GPU 0: GU ~8%).


serverA (1 ps, 4 workers),  serverB (1 ps, 4 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>50%), (serverB: GPU 0: GU ~8%).


It looks dist tensorflow does not use GPU on serverB.
next is "nvidia-smi" in case of 6)
((serverA)) Mem allocated on GPU 0,1,2,3

((serverB)) Mem allocated on GPU 0 (last one is 0)

I use replica_device_setter to allocate workers to GPU.
if FLAGS.job == "ps":
    server.join()
elif FLAGS.job == "worker":
    # Assigns ops to the local worker by default.
    with tf.device(tf.train.replica_device_setter(worker_device="/gpu:%d" % (FLAGS.task_id%4), cluster=cluster)):

please help me.