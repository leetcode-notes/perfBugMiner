Distributed tensorflow - Stuck at "CreateSession still waiting for response from worker"

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
1.0 && 1.3
Python version:
2.7
CUDA/cuDNN version:
no

Describe the problem
Run the code on k8s with the spec 1 ps + 20 workers. there are some workers print the log all the time.
I tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:12
I tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:17

I met two situations:

worker 1 wait for worker 12 and 17, but worker 12 and 17 could start training without these logs.
all of other works wait for the worker 12 and 17.

I could telnet the no-response worker within the docker vm in both situations, so it's wired. Is there a bug?
Some have also encountered this problem. see Stackoverflow.
Source code / logs
import datetime
import json
import numpy as np
import os
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_integer("max_epochs", 10000000, "Number of steps to run trainer.")
flags.DEFINE_string("checkpoint_path", "./checkpoint/",
                    "The checkpoint directory")
flags.DEFINE_string("output_path", "./tensorboard/",
                    "indicates training output")
flags.DEFINE_integer("checkpoint_period", 1,
                     "Number of epochs to save checkpoint.")
flags.DEFINE_float("learning_rate", 0.01, "Initial learning rate.")
FLAGS = flags.FLAGS


def main():
    # Create train data
    train_X = np.linspace(-1, 1, 100)
    train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10
    learning_rate = FLAGS.learning_rate
    start_training_time = datetime.datetime.now()

    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    # Exampmle: {"cluster": {"ps": ["127.0.0.1:3001"], "worker": ["127.0.0.1:3002", "127.0.0.1:3003"]}, "task": {"index": 0, "type": "worker"}}
    env = json.loads(os.environ.get("TF_CONFIG", "{}"))
    task_data = env.get("task", None)
    cluster_spec = env["cluster"]
    task_type = task_data["type"]
    task_index = task_data["index"]

    cluster = tf.train.ClusterSpec(cluster_spec)
    server = tf.train.Server(cluster,
                             job_name=task_type,
                             task_index=task_index)

    if task_type == "ps":
        server.join()
    elif task_type == "worker":
        with tf.device(tf.train.replica_device_setter(
                worker_device="/job:{}/task:{}".format(task_type, task_index),
                cluster=cluster)):

            # Define the model
            keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1])
            keys = tf.identity(keys_placeholder)
            X = tf.placeholder("float", shape=[None, 1])
            Y = tf.placeholder("float", shape=[None, 1])
            w = tf.Variable(0.0, name="weight")
            b = tf.Variable(0.0, name="bias")
            global_step = tf.Variable(0, name="global_step", trainable=False)
            loss = tf.reduce_sum(tf.square(Y - tf.multiply(X, w) - b))
            train_op = optimizer.minimize(loss, global_step=global_step)
            predict_op = tf.multiply(X, w) + b
            tf.summary.scalar("loss", loss)
            summary_op = tf.summary.merge_all()
            init_op = tf.global_variables_initializer()
            saver = tf.train.Saver()
            #saver = tf.train.Saver(sharded=True)


            sv = tf.train.Supervisor(is_chief=(task_index == 0),
                                     logdir=FLAGS.checkpoint_path,
                                     init_op=init_op,
                                     #summary_op=summary_op,
                                     summary_op=None,
                                     saver=saver,
                                     global_step=global_step,
                                     save_model_secs=60)

            try:
                with sv.managed_session(server.target) as sess:
                    print("Save tensorboard files into: {}".format(FLAGS.output_path))
                    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)

                    print("Run training with epoch number: {}".format(
                        FLAGS.max_epochs))
                    for i in range(FLAGS.max_epochs):
                        for (x, y) in zip(train_X, train_Y):
                            x = np.array([[x]])
                            y = np.array([[y]])
                            sess.run(train_op, feed_dict={X: x, Y: y})

                        if i % FLAGS.checkpoint_period == 0:
                            x = np.array([[train_X[0]]])
                            y = np.array([[train_Y[0]]])
                            summary_value, loss_value, step = sess.run(
                                [summary_op, loss, global_step],
                                feed_dict={X: x,
                                           Y: y})
                            print("Epoch: {}, loss: {}".format(i, loss_value))
                            if task_index == 0:
                                writer.add_summary(summary_value, step)

                    writer.close()

                    end_training_time = datetime.datetime.now()
                    print("[{}] End of distributed training.".format(
                        end_training_time - start_training_time))


            except Exception as e:
                print(e)


if __name__ == "__main__":
    main()