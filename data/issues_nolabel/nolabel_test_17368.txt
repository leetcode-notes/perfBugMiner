The best way to pass the LSTM state between batches

Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:
System information
The problem is independent of the system information.
I am using Tensorflow 1.5
Describe the problem
I think I will be insane. I am trying to find the best way to pass the LSTM state between batches. I have searched everything but I could not find a solution for the current implementation. Imagine I have something like:
cells = [rnn.LSTMCell(size) for size in [256,256]
cells = rnn.MultiRNNCell(cells, state_is_tuple=True)
init_state = cells.zero_state(tf.shape(x_hot)[0], dtype=tf.float32)
net, new_state = tf.nn.dynamic_rnn(cells, x_hot, initial_state=init_state ,dtype=tf.float32)

Now I would like to pass the new_state in each batch efficiently, so without storing it back to memory and then re-feed to tf using feed_dict. To be more precise, all the solutions I found use sess.run to evaluate new_state and  feed-dict to pass it into init_state. Is there any way to do so without having the bottleneck of using feed-dict?
I think I should use tf.assign in some way but the doc is incomplete and I could not find any workaround.
I am writing this problem here since It is unsolved everywhere else and I think this should be in the doc since it is a common case scenario.
I want to thank everybody that will ask in advance.
Cheers,
Francesco Saverio