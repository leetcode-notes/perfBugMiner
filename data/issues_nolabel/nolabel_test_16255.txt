tf.scatter_update Error

Hi,
I use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient.
Because of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine.
However, I am getting error:
LookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate).
Here are the lines of the code that I update AO and AS that gives the error:
self.players[k-1].AS = tf.scatter_update(self.players[k-1].AS,
self.curTime + leadTimeIn,
tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )
self.players[k+1].AO = tf.scatter_update(self.players[k+1].AO,
self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime],
self.players[k].actionValue(self.curTime, self.playType))
, name='handle_scat_j')
Since both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here?
Is it something that you can fix it, or is there any reason behind this behavior?
BTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory.
Thanks,
Afshin