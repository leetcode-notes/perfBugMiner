CUBLAS_STATUS_ALLOC_FAILED when attempting to train linear regressor

System information

This is my code.
Windows 10 x64
installed tensorflow using pip
tensorflow-gpu v1.4.0
Python 3.6.0
CUDA v8.0, Cudunn v6.0
EVGA GTX 980 SC ATX 4GB

Describe the problem
Whenever I try to train a linear regression network with this dataset, it gives me a CUDA error saying CUBLAS_STATUS_ALLOC_FAILED. I suspect it's not because I'm using too much memory, as when it does occasionally work, it always maxes out my gpu memory usage, no matter what size the network is.
I've included my script and my data file that i ran it with, and a log of the error occurring.
error.log
Here is my source:
predictlaptimes.txt
(just pretend it's a python file)
And here's my data:
carnumbers.txt
pretend it's a csv file.
Github won't let me upload .py or .csv files for some reason.

Looking online I found a solution by changing the allow_growth option to True. I added this line to my code;
regressor._session_options.gpu_options.allow_growth = True

And now it runs without any problems.