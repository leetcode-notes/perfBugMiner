tf.Variable uses about twice the memory (on the GPU)

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): binary with pip using Anaconda
TensorFlow version (use command below): tensorflow-gpu 1.8.0
Python version: 3.6.5
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: V9.0.176
GPU model and memory: Quadro M2000M, 4G
Exact command to reproduce: Please see problem description. Thank you.

Describe the problem
This is almost the same to #13433 but on GPU. I think this is a bug/feature request. If not, please let me know. I use the following python code for testing both tf.Variable and tf.get_variable. The variable v should be 800MB.
import tensorflow as tf

sess_conf = tf.ConfigProto()
sess_conf.gpu_options.allow_growth = True

# Test the following for Tensor and Variable
v = tf.Variable(tf.random_uniform((1024*1024, 100), dtype=tf.float64))
# v = tf.get_variable('v', shape=(1024*1024, 100), initializer=tf.random_normal_initializer, dtype=tf.float64, trainable=False)

init = tf.global_variables_initializer()
with tf.Session(config=sess_conf) as sess:
    sess.run(init)

And I use nvidia-smi dmon -c 10 -s m to monitor the memory usage. For both cases got:
# gpu    fb  bar1
# Idx    MB    MB
    0    42   227
    0    42   227
    0    42   227
    0    42   227
    0   102   227
    0  2163   227
    0  2163   227
    0  2163   227
    0  2163   227
    0  2163   227

Could you please let me know which part is wrong? Or is this an expected behavior? Thank you.