Tensorflow Lite exhibits longer inference time when build with Android NN API on Google Pixel 1

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
Source
TensorFlow version (use command below):
'v1.3.0-rc1-6055-gfdf34a8', '1.4.0'
Python version:
2.7.12
Bazel version (if compiling from source):
0.8.0
GCC/Compiler version (if compiling from source):
GCC 5.4.0-6ubuntu1~16.04.5
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Describe the problem
When enabling the NN API usage (edit interpreter.cc) one would expect that due to the HW acceleration, inferece times would be shorter. However, exactly the opposite happens. I tried this using the included demo app, using mobilenet_quant_v1_224.tflite and a custom (not quantized) model that I converted to tflite. In the mobilenet case, inference takes around 80ms without NNAPI, and ~100ms with NNAPI . My custom model, which sadly I cannot share , takes 40ms without , and ~90ms with NNAPI.
Please note that I have also tested this with FAST_SINGLE_ANSWER  and SUSTAINED_SPEED  NNAPI preference settings. There was no significant change in inference times.
My Pixel Build number is: OPM1.171019.011
Source code / logs
--- a/tensorflow/contrib/lite/interpreter.cc
+++ b/tensorflow/contrib/lite/interpreter.cc
@@ -51,7 +51,7 @@ Interpreter::Interpreter(ErrorReporter* error_reporter)
tensors_.reserve(kSlotsToReserve);
nodes_and_registration_.reserve(kSlotsToReserve);
next_allocate_node_id_ = 0;

UseNNAPI(false);


UseNNAPI(true);