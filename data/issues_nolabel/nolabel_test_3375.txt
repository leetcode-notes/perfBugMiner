wrong use of sequence_loss_by_example in ptb_word_lm.py?

Hi, I noticed "sequence_loss_by_example" with default of "average_across_timesteps":
def sequence_loss_by_example(logits, targets, weights,
average_across_timesteps=True,
softmax_loss_function=None, name=None):
but in pub_word_lm.py:
loss = tf.nn.seq2seq.sequence_loss_by_example(
[logits],
[tf.reshape(self._targets, [-1])],
[tf.ones([batch_size * num_steps])])
self._cost = cost = tf.reduce_sum(loss) / batch_size
weights sum of batch_size*num_steps was used in "sequence_loss_by_example" to normalize the score, so there is no need to divide loss by batch_size again, right? cost was used for gradient later.