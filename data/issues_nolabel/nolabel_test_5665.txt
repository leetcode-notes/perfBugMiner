Is there a bug in embedding_attention_seq2seq?

Hi, all,
I think the code of embedding_attention_seq2seq is confusing. And I can't run this code.
if output_projection is None:
      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)
      output_size = num_decoder_symbols

...

      x = linear([inp] + attns, input_size, True)
      # Run the RNN.
      cell_output, state = cell(x, state)

...

with variable_scope.variable_scope("AttnOutputProjection"):
        output = linear([cell_output] + attns, output_size, True)

I don't know what's the meaning of "AttnOutputProjection". The attention information has been used in "x = linear([inp] + attns, input_size, True)", why adding it again?
And meanwhile what is the meaning of "[cell_output] + attns", "cell_output" is equal with num_symbols, so why add attens?
In my experiment, the num_decoder_symbols is 10000 and the hidden size is 32, and I get a matrix 10032*10000 at "AttnOutputProjection". It will be out of memory.
10032 is a confusing number, I don't know the physical meaning of this number.
So I don't know if it's a bug. If it's a bug, I'm really pleasure to make a PR.
Thanks so much.