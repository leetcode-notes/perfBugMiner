Data on GPU not garbage collected

System information

No custom code written
Linux Ubuntu 16.04)
from source
TF 1.6
Python 2.7

I've also posted the question on stack overflow (https://stackoverflow.com/questions/49399495/data-on-gpu-not-garbage-collected). However this could potentially be a bug so I decided to post an issue anyway.
For our current implementation, the return value of a sess.run() will be stored in a dictionary. We do that to implement statefulness of RNN.
However, it turns out that storing data that way prevents tensorflow from garbage-collecting the data allocated on GPU. GPU memory usage simply keeps growing and it eventually blows up.
The return value from a sess.run() should be an numpy.ndarray as far as I'm concerned. And the data of an numpy.ndarray should live on good old CPU, shouldn't it?
Then why would storing the return value prevent GC? I don't think this is a reference counting problem.
Our current theory is that the returned numpy.ndarray actually has some reference to the GPU memory. Hope someone could verify/debunk this.
We actually tried commenting out the data storing part. Then the GPU memory becomes constant.
Thanks!!!