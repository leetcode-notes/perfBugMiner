Fix folding unfused batchnorm bug

Folding batch normalization which has parameters like is_training=False, fuse=False, scale=False raises an error because its batch_mean_tensor is None. I suggest using moving_mean_tensor instead.