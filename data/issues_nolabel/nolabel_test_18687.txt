Absence of 'tanh()' operation in the computation of attention vector

Tanh() operation is missed in the computation of attention vector (https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py), which is mentioned useful in the definition of Attetntion vector(https://www.tensorflow.org/tutorials/seq2seq).
OS : Ubuntu 14.04.4 LTS
TensorFlow installed from: (https://files.pythonhosted.org/packages/f7/52/4f78cc674775bd36a28223fd63d98260eb7130128298f7c70edbdcb34075/tensorflow-1.4.1-cp34-cp34m-manylinux1_x86_64.whl)   via pip
TensorFlow version: tensorflow-gpu 1.4.1
CUDA/cuDNN version: 8.0
GPU model and memory: Tesla K80
The problem is found when I compare the attention vector value, computed with Numpy and network's weights, to those retrieved from Attention Vector tensor. These two vectors are the same when I did not apply tanh(), meaning a tanh() opertion is missed with regards to the Attention vector's definition.