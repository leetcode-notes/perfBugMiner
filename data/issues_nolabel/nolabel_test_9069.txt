Does Tensorflow support learning of embeddings for output classes in multi-class classification?

Hi,
I came across this research paper released by YouTube, on how they use deep learning neural networks for recommendations. It's located here: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf
In the paper, the candidate generation neural network model outputs a softmax with 256 dimensions, which acts as an "output embedding" of each of the 1M video classes.
How is this possible to implement in Tensorflow, for example? Isn't softmax supposed to be only 1-Dimensional. If the model outputs an "embedding" like this, as they say it does, how would the training data's labels be formatted as 256-dimensional? In other words, how do they compute the 256-dimensional vector for each of the videos in their training dataset?
Also, is it possible to create an output embedding layer when the labels in the training dataset are one-hot encodings of a particular class. In other words, can the output layer automatically learn embeddings for the output class?
Thank you so much for your time and help, guys! I have also asked this question on StackOverflow here: http://stackoverflow.com/questions/43297567/how-to-create-a-multi-dimensional-softmax-output-in-tensorflow
Tensorflow details: Windows, version 1.01, binary (via pip)