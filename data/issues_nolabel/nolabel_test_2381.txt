understanding the current state of dynamic_rnn vs buckets vs scan for seq2seq

Bear with me as I am not an expert in the practical or theoretical details.
Perhaps because tensorflow has developed so rapidly, I find it a little difficult to find the recommended procedure for RNNs with varying length, at least from my lay end-user perspective. It is unclear to me on May 15, 2016 what tool within tensorflow is best suited to this task.
It seems that originally RNNs of varying sequence length were handled by "bucketing". This means building a "separate" model for each of several bucket lengths so that inputs could be padded to the same length without wasting too much gpu memory. There is not presently a "simple" example with bucketing, but to those unfamiliar, I would look to the seq2seq model for clarity. Also the model_with_buckets function
Sometime between March and May a number of (maybe?) different tools were added that could possibly be used to avoid this bucketing trick without wasting memory.

rnn.py/dynamic_rnn
functional_ops, though within this code it's not clear without investigation if map_fn or tf.scan (!) would be better suited.
control_flow_ops

In spite of this, the seq2seq model still uses bucketing, and I struggle to find any examples using these newer methods.
Creating detailed documentation is ultimately part of the endgoal, but for now I thought it would be useful to at least make a mention of what the recommended approach is.
Thanks.
relevant issue 208 
relevant reddit with broken link to example