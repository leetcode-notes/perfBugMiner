Non chief worker is throwing exception while saving the model

System information

Have I written custom code - Yes
OS Platform CentOS - 7.2.1511
TensorFlow installed from - Binary
TensorFlow version - 1.3.0
Python version - 2.7
Bazel version - N/A
CUDA/cuDNN version - N/A
GPU model and memory - N/A

I am having three nodes Tensorflow cluster with one parameter server, two workers, first one is chief worker.
All three servers are running on the single machine but on different ports.
Source Code
Sharing my code snippet below,
sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,
                                 summary_op = my_summary_op, init_fn = restore_fn)
 with sv.prepare_or_wait_for_session(server.target) as sess:
            for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):
                #Log info at each epoch:
                if step % num_batches_per_epoch == 0:
                    logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)
                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])
                    logging.info('Current Learning Rate: %s', learning_rate_value)
                    logging.info('Current Streaming Training Accuracy: %s', accuracy_value)
					
                #Log the summaries every 10 step.
                if step % FLAGS.steps_update_frequency == 0 and step != 0:
                    loss, gs = train_step(sess, train_op, sv.global_step)
                    summaries = sess.run(my_summary_op)
                    
                    sv.summary_computed(sess, summaries)
                    print "**** SAVE THE MODEL ****"
                    sv.saver.save(sess,sv.save_path,global_step=sv.global_step)

                else:
                    loss, _ = train_step(sess, train_op, sv.global_step)

            #We log the final training loss and accuracy
            logging.info('Final Loss: %s', loss)
            logging.info('Final Training Accuracy: %s', sess.run(accuracy))

            #Save the model on completing the training
            logging.info('Finished training! Saving model to disk now.')
            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)

Describe the problem
Parameter server and workers have been started fine and training is running on the both the workers.
Plz note that I am using tensorflow Supervisor API to run my training.
Problem starts at non-chief worker when I try to save summaries and checkpoint after certain number of training steps.
I am using the single script for running parameter server and workers.
I don't understand why is chief worker not reporting any issue while saving summaries and checkpoint and only non-chief worker is complaining about it.
Am I doing anything wrong in my code ?
As per my understanding, every worker will run training on it's own data and store the checkpoints periodically.
However, I have read somewhere that it's the responsibility of only chief worker to save summaries and checkpoints.
Is that true and that's the reason saving model is failing for me at non-worker node?
If yes, then my question is what is the use of other worker here if it doesn't store the model, don't we loose pottentially a better model at other non-chief worker ?
Error Logs
Posting error logs received at non-chief worker while saving summaries.Similarly some other exception was thrown while saving the model.
Traceback (most recent call last):
  File "./DTF_train_image_classifier.py", line 469, in <module>
    tf.app.run()
  File "/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "./DTF_train_image_classifier.py", line 433, in main
    sv.summary_computed(sess, summaries)
  File "/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py", line 852, in summary_computed
    raise RuntimeError("Writing a summary requires a summary writer.")
RuntimeError: Writing a summary requires a summary writer.

Plz note that error is thrown at below two lines,
sv.summary_computed(sess, summaries)
sv.saver.save(sess,sv.save_path,global_step=sv.global_step)