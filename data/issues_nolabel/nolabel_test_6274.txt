seq2seq checkpoints not working

I am seeing abnormal behavior when I train seq2seq...there are 3 checkpoint files being generated at each checkpoint i.e.:

seq2seq.ckpt-300.data-00000-of-00001
seq2seq.ckpt-300.index
seq2seq.ckpt-300.meta

Training seems to progress normally with no errors but when I pause training and try to resume or test from the last checkpoint, a model with fresh parameters is created and my training is rendered useless.
UPDATE:
Initializing tf.train.Saver() with write_version=1 to revert back to the deprecated version appears to have fixed the problem. Now I see that only two files are created at each checkpoint i.e.:

seq2seq.ckpt-300
seq2seq.ckpt-300.meta

Obviously this would suggest something is going on with the new write version...any ideas?
UPDATE:
The problem is referenced and solved here: http://stackoverflow.com/questions/40469553/tensorflow-loading-model-with-saver-v2