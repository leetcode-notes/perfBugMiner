Distributed variable initialization never reaches some nodes (affects MonitoredTrainingSession too)

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Multiple affected in different ways, including Linux Ubuntu 16.04.03, Mac OS X 10.12.6, Windows 10 and Bash on Windows 10 running Ubuntu 16.04.03.
TensorFlow installed from (source or binary): binary, followed the pip install instructions
TensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0
Python version: 3.5.2, 3.6.0
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: See instructions below.

Describe the problem
In a distributed environment with 2 nodes (a chief 'foo', a non-chief 'bar') variable initialization performed by foo never reaches bar in some particular distributed combinations. If using MonitoredTrainingSession, this leads to the session in bar never starting (it keeps trying forever every 30 seconds). Further research showed that no matter what I do (including restarting sessions, delays and anything I could think of) the session in 'bar' is unable to see the initialization of the variables that 'foo' confirms as initialized.
The source code below can be used to reproduce the problem depending on the hosts used for the 'foo' and 'bar' jobs. In particular, the following has been observed and reproduced multiple times:

When foo and bar are in the same host and OS, the problem never happens.
When ran foo in a Linux host and bar in a Mac OS X host the problem always happened.
However, if the roles are reversed (Linux runs bar, Mac OS X runs foo) the problem never happens.
Making foo_variables below an empty list also avoids the problem entirely.

Additionally I also tested this in another couple of platforms in the same host.

When ran foo in Windows 10 and bar in bash on Windows 10 running Ubuntu Linux 16.04, the problem always happened.
When reversing it and making Linux run foo and Windows 10 run bar the problem disappeared.
In this case both OS run in the same host and communicate through localhost sockets. Other tests suggest there's no problem in this socket communication.

Source code / logs
#!/usr/bin/env python

import sys
import tensorflow as tf

with tf.variable_scope('foo'), tf.device('job:foo/task:0'):
  foo_variables = [tf.get_variable('W', shape=(10, 5), dtype=tf.float32)]
  foo_init_vars = tf.variables_initializer(foo_variables)
  foo_pending_vars = tf.report_uninitialized_variables(foo_variables)
  foo_pending_vars.mark_used()


with tf.variable_scope('bar'), tf.device('job:bar/task:0'):
  # Expect more stuff and ops in bar in a real use case. This is just an example.
  bar_pending_vars = tf.report_uninitialized_variables(foo_variables)
  bar_pending_vars.mark_used()


cluster_spec = tf.train.ClusterSpec({
  "foo": ["<insert_ip_here>:55700"],
  "bar": ["<insert_ip_here>:55701"],
})


def foo_job():
  server = tf.train.Server(cluster_spec, job_name='foo', task_index=0)
  with tf.train.MonitoredTrainingSession(server.target, is_chief=True) as session:

    # This always runs fine.
    session.run(foo_init_vars)
    print("Foo -- Variables not initialized: ", session.run(foo_pending_vars))
    server.join()


def bar_job():
  server = tf.train.Server(cluster_spec, job_name='bar', task_index=0)
  with tf.train.MonitoredTrainingSession(server.target, is_chief=False) as session:
    # On failure, this never gets executed...
    print("**** Session started! ****")

    vars_left = session.run(bar_pending_vars)
    if len(vars_left) == 0:
      print("Bar -- Variables initialized!")
      return

    print("Bar -- Variables not initialized: ", vars_left)
    server.join()


if __name__ == '__main__':
  if len(sys.argv) < 2:
    print("Usage: %s {foo|bar}" % sys.argv[0])
    exit(1)

  if sys.argv[1] == 'foo':
    foo_job()
  else:
    bar_job()
@mrry, this looks like something you might have some intuition about. Any ideas of what might be going on?
This is affecting my distributed system pretty badly. I'd be happy to do more experiments to help diagnosing the problem.