in the test dataset the use of different batch_size will get different results. ,why?

When I used kears and tensorflow training after the checkpoint, after stopping the process, when I was found in the test, the use of different batch_size will get different results. The When batch_size = 1 when the error rate is high, but when the batch_size = 128, the error rate is not so high. The The The Why is this? The So what about my online service?