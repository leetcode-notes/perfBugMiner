Changing optimizer during the training gives weird results.

I am trying to change the var_list provided to the minimize() function after some iterations. I am trying to implement a two step finetuning, where for first "n" iterations, I am training the last layer of the network and after that finetuning the whole network. So for first "n" iterations i am providing variables of last layer in var_list and after "n" iterations i am providing all variables in the network.
It seems like whole network is being reinitialised when I change the optimizer after "n" iterations.