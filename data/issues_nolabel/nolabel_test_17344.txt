Does the size of embedding have a huge influence on the training speed?

In my text classification task, I use vectors to represent the words. After I enlarge the size of vocabulary (word dimension is still 128), from 200000 to 1000000, the training step time arise from 0.1s to 0.4s. Here is the word embedding tensor creatation:
tf.get_variable('embedding', shape, dtype=dtype, return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True)
And the sentences are made of word ids and vectors will be looked up from the embedding tensor above.
inputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)
So what confused me is that why the size of vocabulary has a significant impact on the training speed. Is that normal ?