Eager: eager mode considerably slower than standard TensorFlow for large matrix multiplications

We have been benchmarking eager mode versus standard TensorFlow for large square matrix multiplications, specifically the time to run
m = tf.matmul(A, B) (in eager mode)
versus
m = sess.run(self.c, feed_dict={self.A:A, self.B:B})
in non-eager mode.
We find that while runtimes are comparable for small matrices, eager mode is considerably slower for repeated multiplications of large matrices (eg, of dimension 15,000). The first multiplication is fast, but subsequent multiplications take much longer, even after resetting the computation graph. Is this expected behavior? We are running everything on a GPU.