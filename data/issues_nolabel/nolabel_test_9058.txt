AdadeltaOptimizer numeric issue

Hi,
I tested it with following script, and expected the model update to be -sqrt(0+1)/sqrt(1+1)*1 = -0.7071. However, the output is -0.866. Input gradient is 1, and confirmed with GradientDescentOptimizer. I also verified in rho=1/epsilon=1 the output is -1 as expected, and rho=1/epsilon=0 the output is nan as expected. Reading the implementation of adadelta didn't tell me why the output is -0.866 for rho=0/epsilon=1. What am I missing?
def test_tf():
    data  = [0.5]
    label = [0]
    dim   = 1
    batch_size = 1
    import tensorflow as tf
    tf.reset_default_graph()

    x = tf.placeholder(tf.float32, [batch_size, dim])
    l = tf.placeholder(tf.int32, [batch_size, dim])
    W = tf.Variable(tf.zeros((dim,)), name='W')
    logits = x + W
    print(logits)
    loss = tf.losses.mean_squared_error(logits, l)
    #optimizer = tf.train.GradientDescentOptimizer(1)
    optimizer = tf.train.AdadeltaOptimizer(1, rho=0, epsilon=1)
    train = optimizer.minimize(loss)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        input_map = {x:[data], l:[label]}
        print('loss', loss.eval(input_map))
        sess.run(train, input_map)
        print('\n'.join([' {}\n'.format(p.eval()) for p in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]))
    sess.close()

test_tf()