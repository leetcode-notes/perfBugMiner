softmax_cross_entropy_with_logits to take integer labels / avoid giant dense one-hot matrix

Currently to compute this loss for a classifier you need to generate a big dense matrix of one-hot vectors to pass to softmax_cross_entropy_with_logits, as in the example at http://tensorflow.org/tutorials/mnist/tf/index.html#loss
This is kinda fiddly to do, but also rather wasteful of memory especially in the case of a larger multiclass softmax. Could we have a version of this function which accepts a list of indices, e.g. like Theano's: http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#tensor.nnet.categorical_crossentropy ?