Distributed Tensorflow stucks in multiple loss functions

Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I have implemented the code based on the official Distributed Tensorflow example and my non-distributed implementation
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
TensorFlow installed from (source or binary):
binary, using "pip install tensorflow-gpu"
TensorFlow version (use command below):
Tensorflow-GPU 1.7
Python version:
3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
CUDA_9.0.176 / cudnn-9.0
GPU model and memory:
2X NVIDIA GeForce GTX 980 Ti / 6GB
Exact command to reproduce:
see the description of the problem

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the problem
I am implementing our distributed-application based on the official example. We were using between-graph replication and synchronous training (tf.train.SyncReplicasOptimizer
). The model we trained is a variational autoencoder plus GAN. In each iteration, it updates the coefficients with two loss functions -- one for the generator and one for the discriminator. The first loss function for the generator runs well, but the second one for the discriminator doesn't return. I tried swapping the loss functions, but the second one still stucks.
I tried using the asynchronous training, this works. But I prefer the synchronous training which seems perform better in terms of throughput.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.