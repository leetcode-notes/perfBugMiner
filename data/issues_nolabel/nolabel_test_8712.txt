major memory allocation/copying overhead in Android Inference Library

There seems to be a major overhead in TensorFlowInferenceInterface.feed(...)
When running the feed-run-fetch cycle, I noticed that feeding took an unreasonable amount of time.
So I looked at the source code. I realized that I could pass a Buffer instead of an Array, because it would have been wrapped anyway. But either way a new Tensor object is created in Tensor.create(...) which includes first allocating the memory and then copying over from the passed Buffer.
Although this seems to be okay, when doing some Method Tracing, I discovered that when doing one cycle of feeding, running and fetching, it spends 99.99% with feeding, specifically 99.99% of that with creating the Tensor and 99.99% of that with copying over from the Buffer.
The chart is really packed with FloatBuffer.put(...)
Why isn't it possible to use the just newly created Buffer directly?
Or why isn't the final Buffer filled directly from the Array?