Multiple Towers Not Applying Updates

Environment info
All 4 machines: 4.1.13-100.fc21.x86_64
Setting 1:
1 machine with 1x 960 GTX (one of the workers from below)
Setting 2:
1 machine with cpu (parameter server)
3 machines with 1x 960 GTX (worker)
tensorflow 0.9.0, cudNN v4
Problem
I can train a mnist example on both settings while using the same input pipeline as in the following scripts.
The following two scripts are almost identical but one is made to work on Setting 1 while the other one is made for Setting 2 to work in a distributed, parameter sharing, data parallelism manner. The scripts operates on a small set with coin images with 2 classes.
Single Machine Script (Setting 1): https://gist.github.com/ischlag/96b10519a45727bd17fe0cce01c1bd15
Distributed Script (Setting 2): https://gist.github.com/ischlag/d9fc4429971ce7c1957798de30c56372
The Single machine script works as expected and can generalize well:
'''
Session started!
Partial-Epoch Avg Error:  0.692565  AvgMsPerBatch: 0.56 ms
Partial-Epoch Avg Error:  0.686972  AvgMsPerBatch: 0.49 ms
Partial-Epoch Avg Error:  0.671962  AvgMsPerBatch: 0.46 ms
Partial-Epoch Avg Error:  0.645295  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.59795  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.607252  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.548216  AvgMsPerBatch: 0.49 ms
Partial-Epoch Avg Error:  0.5107  AvgMsPerBatch: 0.49 ms
Partial-Epoch Avg Error:  0.492883  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.466268  AvgMsPerBatch: 0.48 ms
Partial-Epoch Avg Error:  0.431923  AvgMsPerBatch: 0.49 ms
Partial-Epoch Avg Error:  0.407919  AvgMsPerBatch: 0.48 ms
Partial-Epoch Avg Error:  0.387163  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.340534  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.349155  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.327694  AvgMsPerBatch: 0.47 ms
Partial-Epoch Avg Error:  0.244313  AvgMsPerBatch: 0.46 ms
Partial-Epoch Avg Error:  0.256759  AvgMsPerBatch: 0.46 ms
Partial-Epoch Avg Error:  0.206276  AvgMsPerBatch: 0.46 ms
Partial-Epoch Avg Error:  0.184809  AvgMsPerBatch: 0.46 ms
Partial-Epoch Avg Error:  0.187335  AvgMsPerBatch: 0.46 ms
'''
The distributed script is quite slower and fails to update the parameters.
'''
Session started!
Partial-Epoch Avg Error:  0.69339  AvgMsPerBatch: 3.25 ms
Partial-Epoch Avg Error:  0.692113  AvgMsPerBatch: 3.27 ms
Partial-Epoch Avg Error:  0.693958  AvgMsPerBatch: 3.27 ms
Partial-Epoch Avg Error:  0.688354  AvgMsPerBatch: 3.26 ms
Partial-Epoch Avg Error:  0.692994  AvgMsPerBatch: 3.25 ms
Partial-Epoch Avg Error:  0.692903  AvgMsPerBatch: 3.24 ms
Partial-Epoch Avg Error:  0.691708  AvgMsPerBatch: 3.29 ms
Partial-Epoch Avg Error:  0.691477  AvgMsPerBatch: 3.35 ms
Partial-Epoch Avg Error:  0.69129  AvgMsPerBatch: 3.37 ms
Partial-Epoch Avg Error:  0.691391  AvgMsPerBatch: 3.35 ms
Partial-Epoch Avg Error:  0.691415  AvgMsPerBatch: 3.30 ms
Partial-Epoch Avg Error:  0.69209  AvgMsPerBatch: 3.30 ms
Partial-Epoch Avg Error:  0.691746  AvgMsPerBatch: 3.32 ms
Partial-Epoch Avg Error:  0.690423  AvgMsPerBatch: 3.31 ms
Partial-Epoch Avg Error:  0.692738  AvgMsPerBatch: 3.30 ms
'''
The same distributed script works well with the mnist dataset in a distributed manner. All the lines necessary to run it with mnist are there but commented out.
Why is my distributed script not working in this case?