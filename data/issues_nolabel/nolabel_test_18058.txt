Large multinomial sampling on GPU causes OOM

Sampling m samples from tf.multinomial with support n allocates an unnecessarily large n x m tensor when running on a GPU. For large n and m, this causes out of memory errors. It should be possible to sample from a multinomial distribution without allocating such a large tensor (output tensor is only b x m, where b is the batch size).
Example and traceback:
sample_indices = tf.multinomial(tf.ones([1, 1e6]), 10000, output_dtype=tf.int32)  # sample_indices has shape [1 x 1e4]
with tf.Session() as sess:
    output = sess.run([sample_indices])

Traceback:

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,10000,1000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
[[Node: multinomial/Multinomial = Multinomial[T=DT_FLOAT, output_dtype=DT_INT32, seed=0, seed2=0,
_device="/job:localhost/replica:0/task:0/device:GPU:0"](ones, multinomial/Multinomial/num_samples)]]


System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Springdale LInux 7.4 (Redhat)
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.6.0
Python version: 2.7.5
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: CUDA 9.0, cuDNN 7.0.5
GPU model and memory: TITAN X (Pascal) 12GB
Exact command to reproduce: (see above)