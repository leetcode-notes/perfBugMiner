tf.gradients casts away imaginary part of result when differentiating by a real value

This is continued from #2255:
Differentiating a complex expression by a real value with tf.gradients should yield a complex value, but this doesn't seem to be the way that tf.gradients works at the moment.
If I run
import tensorflow as tf
import numpy as np

s = tf.Session()
x = tf.placeholder(tf.float32)

z = tf.complex(0., x)

print('full', s.run(tf.gradients(z, [x]), feed_dict={x: 1}))
print('real', s.run(tf.gradients(tf.real(z), [x]), feed_dict={x: 1}))
print('imag', s.run(tf.gradients(tf.imag(z), [x]), feed_dict={x: 1}))

The output is
full [0.0]
real [0.0]
imag [1.0]

while I would have expected
full [0.0 + 1.0j]
real [0.0]
imag [1.0]

I suspect that tf.gradients always casts the final result to the type of the variable that we differentiate with respect to, which makes sense in many circumstances, but leads to incorrect behaviour in this case.