stop gradients for weights in tf.losses

In the case that the weights given to tf.losses.* depend in some way on the model parameters,
the derivative of that loss also calculated with respect to the weights.
(Stupid) minimal example:
import tensorflow as tf
x = tf.constant(0)
w = tf.get_variable(name="W", shape=(), initializer=tf.zeros_initializer())
L = tf.losses.mean_squared_error(x, x, weights=w)
tf.train.AdamOptimizer().compute_gradients(L)
results in
[(<tf.Tensor 'gradients/mean_squared_error/Mul_grad/tuple/control_dependency_1:0' shape=() dtype=float32>,
  <tf.Variable 'W:0' shape=() dtype=float32_ref>)]

I would expect the weights to be considered constant for the calculation of a loss. In case you agree with me, I can make a PR that adds stop_gradient around the weights parameter.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
TensorFlow installed from (source or binary): N/A
TensorFlow version (use command below): N/A
Python version:  N/A
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A