segmentation fault during data loading of machine translation example on amazon ec2

Hello,
I am running tensorflow on amazon ec2 with AMI id ami-cf5028a5 constructed as follows:
https://gist.github.com/erikbern/78ba519b97b440e10640. The instance is a g2.2xlarge.
The only modification to the AMI was to move /models/rnn/translate from the ~/tensorflow dir, to the installation directory (/usr/local/lib/python2.7/dist-packages/tensorflow), so that the libs in translate.py are properly imported.
The command I run is:
python tensorflow/tensorflow/models/rnn/translate/translate.py --data_dir /mnt/translate/data --train_dir /mnt/translate/train --size=256 --num_layers=2 --steps_per_checkpoint=50

Output looks like this:
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927400448
I tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 8
Creating 2 layers of 256 units.
Created model with fresh parameters.
Reading development and training data (limit: 0).
  reading data line 100000
  reading data line 200000
  reading data line 300000
[...]
  reading data line 15000000
Segmentation fault (core dumped)

When I set --max_train_data_size 100000 everything is okay.