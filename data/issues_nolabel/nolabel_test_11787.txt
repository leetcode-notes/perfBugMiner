Unexpected behavior in tf.scatter_update

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.12.3
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.2.0
Python version: 2.7.10
Bazel version (if compiling from source):
CUDA/cuDNN version: CPU
GPU model and memory: CPU
Exact command to reproduce:

>>> import tensorflow as tf
>>> sess = tf.Session()
>>> dim = 10
>>> probs = tf.Variable(tf.ones(dim), trainable=False)
>>> dist = tf.contrib.distributions.Categorical(probs=probs)
>>> mask = tf.Variable(tf.ones(dim), trainable=False)
>>> b = dist.sample([2])
>>> mask = tf.scatter_update(mask, b, [3,3])
>>> sess.run(tf.global_variables_initializer())
>>> print(mask.eval(session=sess))
[ 1.  1.  3.  3.  1.  1.  1.  1.  1.  1.]  # first call
>>> c = dist.sample([3])
>>> mask = tf.scatter_update(mask, c, [5,5,5])
>>> print(mask.eval(session=sess))
[ 3.  1.  3.  5.  3.  5.  5.  1.  1.  1.]  # second call

Describe the problem
I am trying to update mask at indices sampled by dist, and I need past updates to persist as I add more updates to mask. That is, I would expect the second call to be [ 1. 1. 3. 5. 1. 5. 5. 1. 1. 1.]. Since tf.scatter_update mutates mask, I would expect the updates to persist, but it seems like the the updates are made anew every time I call mask.eval().
Furthermore, perhaps I'm missing something, but the output of the second call is further unexpected in the sense that I see three 3's in mask, even when I never assigned three 3's to mask, only two. Is there an explanation for this?
If this is not a bug, then could we add a feature to allow for consistent and persistent updates?
Source code / logs
Please see above.