model parameters contains a lot of 'nan' at certain stage of training using AdamOptimizer

I am training a deep neural network using a simple and well defined case. The training goes very well and reach a high accuracy (~95%), and then, suddenly, just one step iteration, the accuracy is drop to almost zero. I then printed out all the model parameters, and there are lot of 'nan', something like this
....0.076455489, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.08646702....
Here is the information about my system.


Installation. I installed tensor flow from source code on AWS ubuntu g2 instance (GPU instance). During the installation, I use customized configuration so that the tensor flow can run on g2 instance with computing capacity 3.0.


DNN structure: I have a 30 hidden fully connected layers. Each layer has 32 nodes. To make the DNN more stable, I have added short-cuts for every three layers (similar to a recent publication from Microsoft Research "Deep Residual Learning for Image Recognition", by Kaiming He et al.)


Optimizer: AdamOptimizer.
I first found the problem with AdamOptimzer. However, I just realized that the same problem also happens to GradientDescentOptimizer.


Procedure to produce the bug.


I have a saved model, which has good accuracy, and I restart the training by reading the saved model, and the problem shows up for the first iteration.
I wonder if this is a known issue or not. I would be happy to provide my code and input data if this is necessary. I need to create a simple case with simplified code as I am working on a production code that I cannot post it here.