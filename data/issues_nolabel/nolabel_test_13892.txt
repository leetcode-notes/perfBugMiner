Inconsistent Result of SyncReplicaOptimizer

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Sierra 10.12.6
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0
Python version: Python 3.5.2 |Anaconda custom (x86_64)
Bazel version (if compiling from source):
CUDA/cuDNN version: Not used/
GPU model and memory: Not used/
Exact command to reproduce: python synchronous_sgd.py (see below)

Describe the problem
Training a trivial model of 2-layer fully connected MNIST, with one parameter server thread and one worker thread to reproduce this issue.
The file is linked here. We run python synchronized_sgd.py and python async_sgd.py one after one in the same terminal so that they receive same random results to recreate the bug.
The only difference in the two files below is: async comment out 10 trivial lines from sync. (Please diff)
https://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py
https://github.com/heyucongtom/PGRD/blob/master/async_sgd.py
I make sure both trainer receive the exactly same data for each batch, and I also fixed the random seed. As a results, both model shall get exactly the same output. However, they don't.
The problem is, after the first step, the two models are in sync. At exactly the second run of the train_op, this train_op of the sync replica doesn't update the model, nor does it update the global step, resulting in output:
Worker 0: training step 0 done (global step: 0)
On trainer 0, iteration 0 ps it reaches 0.078900 accuracy
Worker 0: training step 1 done (global step: 1)
On trainer 0, iteration 1 ps it reaches 0.319800 accuracy
Worker 0: training step 2 done (global step: 1)
On trainer 0, iteration 1 ps it reaches 0.319800 accuracy
Worker 0: training step 3 done (global step: 2)
On trainer 0, iteration 2 ps it reaches 0.455800 accuracy
Worker 0: training step 4 done (global step: 3)
On trainer 0, iteration 3 ps it reaches 0.477400 accuracy
Worker 0: training step 5 done (global step: 4)
On trainer 0, iteration 4 ps it reaches 0.478100 accuracy

As a comparison, let's take the output of the simple async version. With exactly
Worker 0: training step 0 done (global step: 0)
On trainer 0, iteration 0 ps it reaches 0.078900 accuracy
Worker 0: training step 1 done (global step: 1)
On trainer 0, iteration 1 ps it reaches 0.319800 accuracy <After the first training step, the accuracy is the same, which is expected.>
Worker 0: training step 2 done (global step: 2)
On trainer 0, iteration 2 ps it reaches 0.279100 accuracy <Something different happening.>
Worker 0: training step 3 done (global step: 3)
On trainer 0, iteration 3 ps it reaches 0.427200 accuracy
Worker 0: training step 4 done (global step: 4)
On trainer 0, iteration 4 ps it reaches 0.567100 accuracy
Worker 0: training step 5 done (global step: 5)
On trainer 0, iteration 5 ps it reaches 0.617500 accuracy
Worker 0: training step 6 done (global step: 6)
On trainer 0, iteration 6 ps it reaches 0.561400 accuracy

I read through the source code of SyncReplicaOptimizer and find out that the train_op returned by that optimizer is a Assign operation, which could be only executed after the grads were applied and global steps enqueued. So sync and async with only one process should be exactly the same.
This behavior is mysterious to me now. Not sure if I got anything wrong.