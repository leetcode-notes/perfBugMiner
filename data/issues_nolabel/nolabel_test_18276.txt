tf.train.latest_checkpoint always return the same checkpoint for GCS paths (pywrap_tensorflow cache bug?)

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.3 (confirmed on Ubuntu 16.04.4 LTS as well)
TensorFlow installed from (source or binary): binary (confirmed from 1.6 source as well)
TensorFlow version (use command below): ('v1.6.0-0-gd2e24b6039', '1.6.0')
Python version: 2.7.13
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A (confirmed with CUDA 9.1 cuDNN 7.1)
GPU model and memory: N/A (confirmed with P100 16GB)
Exact command to reproduce:

Open 2 terminals, 1 and 2
From terminal 1
cat <<EOT > checkpoint
model_checkpoint_path: "model.ckpt-4"
all_model_checkpoint_paths: "model.ckpt-0"
all_model_checkpoint_paths: "model.ckpt-1"
all_model_checkpoint_paths: "model.ckpt-2"
all_model_checkpoint_paths: "model.ckpt-3"
all_model_checkpoint_paths: "model.ckpt-4"
EOT
gsutil cp checkpoint gs://path_to_checkpoint_dir/

From terminal 2
python
import tensorflow as tf
print tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')

From terminal 1
cat <<EOT > checkpoint
model_checkpoint_path: "model.ckpt-7"
all_model_checkpoint_paths: "model.ckpt-3"
all_model_checkpoint_paths: "model.ckpt-4"
all_model_checkpoint_paths: "model.ckpt-5"
all_model_checkpoint_paths: "model.ckpt-6"
all_model_checkpoint_paths: "model.ckpt-7"
EOT
gsutil cp checkpoint gs://path_to_checkpoint_dir/

From terminal 2 (python shell still opened, do not reimport tensorflow)
print tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')

tensorflow always returns the same cached copy of the checkpoint file.
Describe the problem
tf.train.latest_checkpoint and tf.train.get_checkpoint_state always returns a cached version of the latest checkpoint file on GCS, ignoring new checkpoints written to the checkpoint file. This is extremely problematic when training a model.
I traced the problem to pywrap_tensorflow so my guess is it's coming from the underlying native code. I would be happy with a way to reset the cache or an input bool to just not use it at all.
Source code / logs
>>> import tensorflow as tf
>>> print tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')
model_checkpoint_path: "gs://path_to_checkpoint_dir/model.ckpt-4"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-0"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-1"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-2"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-3"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-4"

>>> print tf.train.get_checkpoint_state('gs://path_to_checkpoint_dir')
model_checkpoint_path: "gs://path_to_checkpoint_dir/model.ckpt-4"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-0"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-1"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-2"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-3"
all_model_checkpoint_paths: "gs://path_to_checkpoint_dir/model.ckpt-4"