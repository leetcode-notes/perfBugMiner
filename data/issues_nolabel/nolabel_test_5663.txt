slim.batch_norm used with slim.conv2d problem

Dear All,
I have met some problems when using the batch norm layer of slim. I trained the same model structure but with different ways to use batch_norm layer, shown below. It seems the output of the two ways to use batch_norm is different to me. I have look inside the code to see. It seems the two way no different at all. But the result is actually not the same. Anyone met it before?


net = slim.conv2d(net, 64, [4, 4], 2, normalizer_fn=None, activation_fn=None, biases_initializer=None, reuse=reuse)
output1 = slim.batch_norm(net)


output2 = slim.conv2d(net, 64 ,[4, 4], 2, normalizer_fn=slim.batch_norm, activation_fn=None, reuse=reuse)