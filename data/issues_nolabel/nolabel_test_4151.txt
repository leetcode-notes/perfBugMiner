Memory leak when continuously run assign op

Environment info
Operating System:
Ubuntu 16.04
Installed version of CUDA and cuDNN:
8.0 RC, 5.1 (on GTX 1080)
If installed from source, provide
HEAD: a8512e2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
Problem:
In my application, I need to change value of some variable, and run the minimize in a loop, thus I have to run assign op continuously, which may add new op to graph every time, thus the program become very slow and memory explode.
sess = tf.Session()
a = tf.Variable(np.ones((5, 10000, 10000, 3)))
sess.run(tf.initialize_all_variables())

t0 = time.time()
for i in range(10000):
    sess.run(tf.assign(a,np.ones((5, 10000, 10000, 3)) ))
    t1 = time.time()
    print(t1-t0)
    t0 = t1

So is there a method to change the value of Variables without adding an op to graph? Or a way to remove it after(I have a big graph defined before the loop, I don't want to reset all of them and define again with reset_default_graph)?
And since the value assigned to the variable is different all the time, I cannot define the op before the for loop.