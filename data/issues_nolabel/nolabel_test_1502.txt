BatchNorm kernel?

Hello,
Adding batch norm to a model results in a significant slowdown (using tf.nn.batch_normalize).
I was wondering why a dedicated kernel was removed in favour of a symbolic approach?
(seemingly unrelated commit in question: 03869e6)
Was this a design decisions to not want these "one off" style optimizations/kernels?
Having a faster kernel, (like cudnn #714, or something like what torch does) would be great if its not against tensorflows design. I will look into it if it is not.
Thank you.