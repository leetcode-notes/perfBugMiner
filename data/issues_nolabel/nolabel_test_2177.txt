tf.argmax Should Not Permit Backprop Through It

Hey Tensorflow,
Lately, I have been using the argmax function but I have always placed a tf.stop_gradient before using it. However, when I remove the stop_gradient, tensorflow still works fine.
Maybe I'm misunderstanding something, but argmax is not a differentiable function. How is backprop still working when you remove it? Shouldn't an error be thrown when you pass argmax without any stop_gradient?
If it is possible to differentiate argmax, then I would greatly appreciate any resource showing how this is done. Thanks TF!