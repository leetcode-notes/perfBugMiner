Truncated backdrop with max pooling over time

It seems like the way truncated backprop is done, at least in the tutorial on RNNs, is to

set up the data in batches where the time dimension is capped at the max num_steps
run forward/backward passes making sure to use the correct initial state (e.g final state of the previous batch if was part of the same observation but prior in time or the true initial state if the current batch starts at true t=0)

This works fine if each hidden state contributes directly to the loss but this approach fails if you want to do something like max pooling across time.  You can carry around the current max (like the correct initial state for truncated backprop) but it seems like the gradient won't flow from the loss, to the max'd elements.
To summarize, truncated backprop won't work in situations when a given hidden state at time t's contribution to the loss is unknown without knowing the hidden states for all t.
Would love to be wrong.