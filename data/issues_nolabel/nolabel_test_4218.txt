seq2seq: Cannot parse tensor from proto: dtype: DT_FLOAT

My platform is ubuntu 14.04 with tensorflow version of 0.10, CPU version.
I am using functions in seq2seq.py to build a model. The code works fine when I use the embedding_rnn_seq2seq function, but when I use embedding_attention_seq2seq, it has error like this:
AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'
To fix this, I turned the state_is_tuple to false, but now it has another problem:
W tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

E tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_29 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
W tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

E tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
W tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

E tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
E tensorflow/core/client/tensor_c_api.cc:485] Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py", line 730, in _do_call
    return fn(*args)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py", line 712, in _run_fn
    status, run_metadata)
  File "/usr/lib/python3.4/contextlib.py", line 66, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py", line 450, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 29, in <module>
    chatbot.main()
  File "/mnt/d/DeepQA/chatbot/chatbot.py", line 208, in main
    self.sess.run(tf.initialize_all_variables())
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py", line 382, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py", line 655, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py", line 723, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py", line 743, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT
tensor_shape {
  dim {
    size: 52772
  }
  dim {
    size: 52767
  }
}
float_val: 0

         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
Caused by op 'zeros_28', defined at:
  File "main.py", line 29, in <module>
    chatbot.main()
  File "/mnt/d/DeepQA/chatbot/chatbot.py", line 169, in main
    self.model = Model(self.args, self.textData)
  File "/mnt/d/DeepQA/chatbot/model.py", line 58, in __init__
    self.buildNetwork()
  File "/mnt/d/DeepQA/chatbot/model.py", line 118, in buildNetwork
    self.optOp = opt.minimize(self.lossFct)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py", line 198, in minimize
    name=name)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py", line 300, in apply_gradients
    self._create_slots(var_list)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/adam.py", line 118, in _create_slots
    self._zeros_slot(v, "m", self._name)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py", line 494, in _zeros_slot
    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/slot_creator.py", line 106, in create_zeros_slot
    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py", line 1131, in zeros
    output = constant(0, shape=shape, dtype=dtype, name=name)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py", line 167, in constant
    attrs={"value": tensor_value, "dtype": dtype_value}, name=name).outputs[0]
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py", line 2310, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py", line 1232, in __init__
    self._traceback = _extract_stack()

Code is as follows:
# Creation of the rnn cell
encoDecoCell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hiddenSize, state_is_tuple=False)  # Or GRUCell, LSTMCell(args.hiddenSize)
#encoDecoCell = tf.nn.rnn_cell.DropoutWrapper(encoDecoCell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!)
encoDecoCell = tf.nn.rnn_cell.MultiRNNCell([encoDecoCell] * self.args.numLayers, state_is_tuple=False)

# Network input (placeholders)

self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim

self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)
self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]
self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]

# Define the network
# Here we use an embedding model, it takes integer as input and convert them into word vector for
# better word representation
decoderOutputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(
    self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength
    self.decoderInputs,  # For training, we force the correct output (feed_previous=False)
    encoDecoCell,
    self.textData.getVocabularySize(),
    self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class
    embedding_size=self.args.embeddingSize,  # Dimension of each word
    output_projection=None,  # Eventually
    feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)
)