TF-Keras dont see TF variables

It is a feature or a bug?
For example, I have 2 models:
typical imports:
import numpy as np
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.contrib.keras import layers as klayers
from tensorflow.contrib.keras import activations as kacts
from tensorflow.contrib.keras import models as kmodels
from tensorflow.contrib.keras import optimizers as kopts
from tensorflow.contrib.keras import losses as kloss
from tensorflow.contrib.keras import backend as kback

Keras model
# keras model
features = klayers.Input(shape=(28, 28, 1))
x = klayers.Conv2D(64, 3, strides=(2, 2))(features)
x = klayers.MaxPool2D()(x)
x = klayers.Conv2D(32, 3, strides=(2, 2))(x)
x = klayers.Flatten()(x)
prelogits = klayers.Dense(128, activation=kacts.elu)(x)
pred = klayers.Dense(10, activation=kacts.softmax)(prelogits)

and TF one:
features = klayers.Input(shape=(28, 28, 1))
def convolution_network(
        states, n_filters=None, kernels=None, strides=None,
        activation_fn=tf.nn.elu, use_bn=False, dropout=-1):
    n_filters = n_filters or [64, 32]
    kernels = kernels or [3, 3]
    strides = strides or [2, 2]
    x = states
    for n_filter, kernel, stride in zip(n_filters, kernels, strides):
        x = tf.layers.conv2d(x, n_filter, kernel, stride, activation=None)
        if use_bn:
            x = tf.layers.batch_normalization(x, training=is_training)
        x = tf.layers.max_pooling2d(x, 2, 2)
        x = activation_fn(x)
        if dropout > 0:
            x = tf.layers.dropout(x, rate=dropout, training=is_training)
    x = tf.contrib.layers.flatten(x)
    return x

def simple_model(features):
    features = convolution_network(features)
    prelogits = tf.layers.dense(features, 128, activation=tf.nn.elu)
    logits = tf.layers.dense(prelogits, 10, activation=tf.nn.softmax)
    
    return logits

pred = tf.contrib.keras.layers.Lambda(simple_model)(features)

Both of them then made with model = kmodels.Model(inputs=features, outputs=pred)
Nevertheless, when I use model.summary() , Kesar model give me correct output:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 64)        640       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 2, 2, 32)          18464     
_________________________________________________________________
flatten_1 (Flatten)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1290      
=================================================================
Total params: 36,906
Trainable params: 36,906
Non-trainable params: 0
_________________________________________________________________

but TF one, give only:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 28, 28, 1)         0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 10)                0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________

Is it correct, that TF-Keras cannot see TF variables and optimize them? So we still have separated Tensorflow an Keras frameworks?
Why cannot I use TF as Keras backend?
TF-versions: from 1.1.0 to 1.2.0rc1