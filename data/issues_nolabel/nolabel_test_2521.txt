Retrain.py with multiple GPUs

I am trying to run retrain.py on an AWS GPU instance with 4 K520's. When running retrain.py and watching the GPU activity using nvidia-smi, it seems like all of the calculations are happening on one GPU. The output looks like:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      2833    C   python                                        3801MiB |
|    1      2833    C   python                                          37MiB |
|    2      2833    C   python                                          37MiB |
|    3      2833    C   python                                          37MiB |
+-----------------------------------------------------------------------------+


However, when I run cifar10_multi_gpu_train.py and set --num_gpus to 4, the computations are distributed evenly as follows:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     14226    C   python                                        3841MiB |
|    1     14226    C   python                                        3841MiB |
|    2     14226    C   python                                        3841MiB |
|    3     14226    C   python                                        3841MiB |
+-----------------------------------------------------------------------------+

Is there a way to run retrain.py on multiple GPUs like on the cifar10_multi_gpu_train.py? If not, is there an easy way to use my own custom images for the cifar10_multi_gpu_train.py?