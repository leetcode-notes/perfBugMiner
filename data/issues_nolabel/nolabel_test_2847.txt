[WIP] Gradient reversal op

"Unsupervised Domain Adaptation by Backpropagation" introduces a very simple but effective "gradient reversal layer" that is the identity for the forward computation and flips the gradient during backpropagation. This PR implements this as a user op called FlipGradientOp. The gradient registration is here (with a small MNIST experiment in that repo that uses the op).
I created the PR to gauge if there is any interest in getting something like this into core or at least contrib. As you can see, the operation itself is dead simple and very general. The same effect can be achieved by processing gradients before application, but the op implementation means that everything works naturally as a result of autodiff. I may have missed a way to achieve this functionality with existing operations - if so, let me know!
The example implementation here of FlipGradientOp effectively multiplies the gradient by -1. It may be worth generalizing this as a ScaleGradientOp that scales the incoming gradient by an arbitrary scalar tensor.
I'm happy to get this in mergeable shape if the tensorflow team approves.