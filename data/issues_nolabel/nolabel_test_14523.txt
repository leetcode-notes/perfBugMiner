[Fetaure Request] Layer normalization for NCHW/NHWC with fast gpu kernel

tf.contrib.layers.layer_norm is slow and only supports NHWC layout. It is beneficial to have a fast gpu kernel for layer normalization that supports both NCHW and NHWC. I think layer normalization is quite useful when the minibatch is extremely small or comprises only one sample (in which case batch norm/renorm doesn't work) and when local receptive fields is desired (in which case instance norm is bad).