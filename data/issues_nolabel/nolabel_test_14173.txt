using batchnorm in conv2d discard the bias

Hi!
I observed that whenever I applied batch normalization to conv2d, the bias variable are not created!
version : Tensorflow 1.3
import tensorflow

image = tf.placeholder(tf.float32, [None, 14, 14, 1024])

tf.contrib.layers.conv2d(image,
                                 num_outputs=128,
                                 kernel_size=[3,3],
                                 normalizer_fn=tf.layers.batch_normalization,
                                 normalizer_params={"training": False, "reuse": False},
                                 activation_fn=tf.nn.relu,
                                 reuse=False,
                                 scope="conv1")
                                 
tf.contrib.layers.conv2d(image,
                                 num_outputs=128,
                                 kernel_size=[3,3],
                                 #normalizer_fn=tf.layers.batch_normalization,
                                 #normalizer_params={"training": False, "reuse": False},
                                 #activation_fn=tf.nn.relu,
                                 reuse=False,
                                 scope="conv2")
                                 
for v in tf.trainable_variables():
	print(v)

output:
<tf.Variable 'conv1/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>
<tf.Variable 'conv1/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>
<tf.Variable 'conv1/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>
<tf.Variable 'conv2/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>
<tf.Variable 'conv2/biases:0' shape=(128,) dtype=float32_ref>
no bias for conv1 !
Question: Is it a bug or a feature :)
[edit] I also observed the same behavior with mlp.
Thank you very much!
Florian