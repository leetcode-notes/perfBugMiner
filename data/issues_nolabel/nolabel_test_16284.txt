[bug?] Tensorflow accepts CUDA_VISIBLE_DEVICES but still allocates memory on multiple GPUs

System information

I have written a custom script, but effect is also visible for just import tensorflow as tf; sess = tf.Session():
Windows 7 Professional:
TensorFlow installed from binary (pip, in Anaconda environment):
TensorFlow version 1.4.0:
Python version 3.5.4:
CUDA/cuDNN version 8.0/6.0:
4 GeForce GTX 1080Ti, 11GB:

Problem description
I am facing the following issue:
If I set
CUDA_VISIBLE_DEVICES=1
and start my python script, everything works as expected, only GPU 1 is used/only memory from GPU 1 is allocated. GPU1 hosts the Desktop Window Manager.
If I set
CUDA_VISIBLE_DEVICES=0 # or 2 or 3
before running my python script
sess = tf.Session()
faithfully reports only one available GPU (with the expected PCI bus id), see attached file ipython.txt.
However, nvidia_smi.exe shows that memory on all remaining GPUs except for GPU1 is allocated. GPU-Util shows that expected GPU is used for actual computation, see attached file nvidia_smi_output.txt.
I see this effect both for my actual tensorflow script as well for simple interactive python with
import tensorflow as tf
sess = tf.Session()

Unfortunately, the dual boot Ubuntu is not working at the moment, but once it is running again, I can try to check whether a similar effect presents itself there.
Could this be a bug, possibly related to Windows? Or a driver or hardware issue?
Attached files:
ipython.txt: ipython script and output.
nvidia_smi_output.txt: output of nvidia-smi.exe