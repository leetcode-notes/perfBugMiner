sigmoid_cross_entropy_loss_with_logits: ReLU input is not finite

W tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values
     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message="ReluGrad input is not finite.", _device="/job:localhost/replica:0/task:0/cpu:0"](nl_1_hs_100_lr_0p1/add)]]
W tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values
     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message="ReluGrad input is not finite.", _device="/job:localhost/replica:0/task:0/cpu:0"](nl_1_hs_100_lr_0p1/add)]]
     [[Node: _send_nl_1_hs_100_lr_0p1/div_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=6247430638048453470, tensor_name="nl_1_hs_100_lr_0p1/div_1:0", _device="/job:localhost/replica:0/task:0/cpu:0"](nl_1_hs_100_lr_0p1/div_1)]]
Traceback (most recent call last):
  File "rnn.py", line 80, in <module>
    training=True)
  File "rnn.py", line 18, in run_epoch
    model.seq_targets: seq_targets
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 444, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values
     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message="ReluGrad input is not finite.", _device="/job:localhost/replica:0/task:0/cpu:0"](nl_1_hs_100_lr_0p1/add)]]
Caused by op u'nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics', defined at:
  File "rnn.py", line 70, in <module>
    train_model = Model(set_config(config, "train"))
  File "/Users/yoav/projects/music_rnn/model.py", line 61, in __init__
    .minimize(self.loss)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 186, in minimize
    aggregation_method=aggregation_method)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 232, in compute_gradients
    aggregation_method=aggregation_method)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 445, in gradients
    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py", line 126, in _ReluGrad
    t = _VerifyTensor(op.inputs[0], op.name, "ReluGrad input is not finite.")
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py", line 119, in _VerifyTensor
    verify_input = array_ops.check_numerics(t, message=msg)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 48, in check_numerics
    name=name)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 664, in apply_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1043, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'nl_1_hs_100_lr_0p1/logistic_loss/Relu', defined at:
  File "rnn.py", line 70, in <module>
    train_model = Model(set_config(config, "train"))
  File "/Users/yoav/projects/music_rnn/model.py", line 58, in __init__
    losses = tf.nn.sigmoid_cross_entropy_with_logits(outputs, targets_concat)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn.py", line 273, in sigmoid_cross_entropy_with_logits
    return math_ops.add(nn_ops.relu(logits) - logits * targets,
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 547, in relu
    return _op_def_lib.apply_op("Relu", features=features, name=name)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py", line 664, in apply_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1043, in __init__
    self._traceback = _extract_stack()

When training a model using tf.nn.sigmoid_cross_entropy_loss_with_logits and RMSPropOptimizer, I am running into a ReLu gradient error. Since the initial learning rate for this instance of the model is quite high and this is a RNN, I am pretty confident that this is an exploding gradient issue with my model. However, the ReLU operation that my model is exploding on is part of an optimization to "ensure stability and avoid overflow", but it seems to be having the opposite effect.