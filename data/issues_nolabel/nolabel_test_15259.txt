Using tensorflow to compute gradients w.r.t. spectral decomposition error

Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug or a feature request.
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.1
TensorFlow installed from (source or binary): pip install tensorflow
TensorFlow version (use command below): v1.2.0-5-g435cdfc 1.2.1
Python version: 3.5.2
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: see code

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Hi,
I'm having issues with evaluating the gradients of eigenvalues/eigenvectors with respect to the underlying matrix. Tensorflow evaluates the gradients, however when compared against analytical derivations the gradients are generally inconsistent (I understand numerical can be unstable). I'd like to understand is there's a bug in the gradients or if a different methodology was employed to the one cited below.
We are using tf.self_adjoint_eig to evaluate the spectral decomposition for the tensorflow variable input. We have initialised this with a symmetric matrix to satisfy the self adjoint operator property. We wish to take the derivative of individual eigenvalues or eigenvector with respect to the original matrix (note for eigenvectors we take one element from one eigenvector, e.g. element 2 in eigenvector 1 to avoid the issue of gradient aggregation for now).
The methodology for evaluating analytical gradients of eigenvalues and vectors of a symmetric real matrix can be found in the paper "On differentiating Eigenvalues and Eigenvectors" by Magnus (1985), Theorem 1 eqn (6) + (7). We evaluated using our input matrix the gradients under this paper and compared it to tensorflow evaluated gradients and the gradients from finite difference approximation. For the eigenvalues, the gradients are similar (identical on diagonal entries, off by a factor of 2 on off-diagonal elements), however the eigenvectors are off by quite a bit outside the diagonal entries. To start, define a matrix A as (excuse the matrix output formatting from Python)
A=[[-3 -2  4]
[-2  1  1]
[ 4  1  5]]
using np.linalg.eig and tf.self_adjoint_eig on A (I simply initialised a variable with A and computed the gradient for the tf implementation)
Tensorflow eigenvalues: [-5.43071561  1.76904987  6.66166575]
Python eigenvalues: [-5.43071561  6.66166575  1.76904987]
I now wish to evaluate the gradient of eigenvalue 1 (-5.43071561) w.r.t. A.
Analytical gradient:
[[ 0.75896178  0.28555906 -0.31842553]
[ 0.28555906  0.10744148 -0.11980748]
[-0.31842553 -0.11980748  0.13359674]]
Tensorflow gradient:
[[[ 0.75896178,  0.        ,  0.        ],
[ 0.57111812,  0.10744148,  0.        ],
[-0.63685107, -0.23961495,  0.13359674]]]
The diagonal entries are the same but the off-diagonal entries are clearly off by a factor of 2.  Now we try and evaluate gradients for the eigenvectors.
Tensorflow eigenvectors:
[[-0.87118413 -0.31452619 -0.37697678]
[-0.32778267  0.94426587 -0.0303395 ]
[ 0.36550888  0.09713517 -0.92572567]]
Python eigenvectors:
[[ 0.87118413  0.37697678 -0.31452619]
[ 0.32778267  0.0303395   0.94426587]
[-0.36550888  0.92572567  0.09713517]]
We try and find the gradient of the eigenvector 1 element 2 (+/-0.32778267). We expect the tensorflow gradient to the equivalent to the analytical gradient (after taking into account the sign difference).
Analytical gradient:
[[ 0.03511309 -0.10795607 -0.01312188]
[ 0.01321128 -0.04061843 -0.0049371 ]
[-0.01473184  0.04529341  0.00550534]]
Tensorflow gradient:
[[[-0.03511309,  0.        ,  0.        ],
[ 0.09474478,  0.04061843,  0.        ],
[ 0.02785372, -0.04035631, -0.00550534]]]
Besides the entries being different on the off-diagonal, one issue is that tensorflow only returns the lower triangle of the gradient. Despite A being symmetric, the contribution to the gradient is not symmetric as shown in the analytical evaluation above. Thank you for reading!
TL:DR, I'd like to understand where the gradient computations are coming from and why they differ substantially to the results we have been using in our research.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Here is a script that reproduces the above.
eigen_decomp_examplev2.py.zip