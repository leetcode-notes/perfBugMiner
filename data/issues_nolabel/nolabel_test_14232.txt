[Feature Request] Automatic ClusterSpec Propagation for multiple hosts

Currently, when launching a distributed TensorFlow job, user need to manually input all the worker hosts' IP and port number. This is not too convenient and does not scale well. It would be really nice to have the native TensorFlow functionality that workers can automatically register themselves on master service without knowing all the host IP and port beforehand. Not sure if there is existing solution to solve this problem. But I used some of the building blocks (ClusterSpec Propagation and ClusterResolver) to enable this feature on my client code. Please let me know if it is a good approach to do this, I'd love to contribute if there's interest in this functionality.
The solution I have involve the following steps:

Master service start server with user specified port and wait for all workers to register
Worker register themselves by sending ClusterSpec to Master service and wait for response
Master service waits until numbers of workers registered matches requested worker number
Master service merges ClusterSpecs and propagates to all registered workers
Master service and workers continues