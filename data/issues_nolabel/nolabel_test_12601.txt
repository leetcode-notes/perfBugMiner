Produce mask when padding batches

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 17.04
TensorFlow installed from (source or binary): binary via pip
TensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0
Python version: 3.5.3
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: See source code below

Describe the problem
Currently, the Dataset API (as well as the older queue-based API) allows for batches to be padded, but there is no way to determine which values were inserted for padding in the resulting output tensors unless care is taken to pick a padding value that will never occur in the original dataset. It would be useful if this could be indicated via a separate mask tensor that indicates this rather than having the user compute it manually (if it's even possible for their particular data). E.g., this is useful for sequential tagging (like part-of-speech tagging), where each example is a multi-word sentence and each label is actually a sequence of labels. Computing the correct losses requires knowledge of which values were just included for padding.
Currently, I do this using Fuel, which, for a given batch, gives me one array for the input data, one for the label, and then one mask each for the input and the label. The masks are binary and of the same shape as the input/label array, respectively. I then feed these into TensorFlow via the feed_dict mechanism.
Here's an example from the programmer's guide:
dataset = tf.contrib.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(4, padded_shapes=[None])

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]
print(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],
                               #      [5, 5, 5, 5, 5, 0, 0],
                               #      [6, 6, 6, 6, 6, 6, 0],
                               #      [7, 7, 7, 7, 7, 7, 7]]

With a mask, this might look like:
dataset = tf.contrib.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(4, padded_shapes=[None], mask=True)

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

print(sess.run(next_element))  # ==> ([[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]],
                               #      [[0, 0, 0], [1, 0, 0], [1, 1, 0], [1, 1, 1]])
print(sess.run(next_element))  # ==> ([[4, 4, 4, 4, 0, 0, 0],
                               #       [5, 5, 5, 5, 5, 0, 0],
                               #       [6, 6, 6, 6, 6, 6, 0],
                               #       [7, 7, 7, 7, 7, 7, 7]],
                               #      [[1, 1, 1, 1, 0, 0, 0],
                               #       [1, 1, 1, 1, 1, 0, 0],
                               #       [1, 1, 1, 1, 1, 1, 0],
                               #       [1, 1, 1, 1, 1, 1, 1]])

(Obviously, this particular example is simple to compute manually, but imagine the case where there is no obvious padding value to pick that wouldn't occur in the data.)