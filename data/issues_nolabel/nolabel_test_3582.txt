Loss not decreasing, GTX 1070, cuDNN 5.0 for RC 8.0, Cuda 8.0 RC

I've spent about 12 hours monkeying around before throwing in the towel here. I've checked the other similar threads on this and done the steps there. This includes #3068 and #3507.
Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN:
Cuda 8.0 RC
cuDNN 5.0 RC
Output of libcudn*:
/usr/local/cuda/lib64/libcudadevrt.a       /usr/local/cuda/lib64/libcudnn.so
/usr/local/cuda/lib64/libcudart.so         /usr/local/cuda/lib64/libcudnn.so.5
/usr/local/cuda/lib64/libcudart.so.8.0     /usr/local/cuda/lib64/libcudnn.so.5.0.5
/usr/local/cuda/lib64/libcudart.so.8.0.27  /usr/local/cuda/lib64/libcudnn_static.a
/usr/local/cuda/lib64/libcudart_static.a
Built from source

The commit hash: 5161e4c
The output of bazel version:
Build label: 0.3.0
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 10 11:38:23 2016 (1465558703)
Build timestamp: 1465558703
Build timestamp as int: 1465558703

Notes
I've built and thrown out, built and thrown out, cleaned the slate and reinstalled in about as many permutations as I can think. However, with training, the loss freezes. I simplified my network all the way down to a shallow autoencoder where there is only one image in the training set, and still the loss freezes. This worked completely fine using the CPU for similar code from a regular old pip distro. I'm using the MSE loss on a 224*224 image scaled between 0 and 1 to try to keep things as regular as possible.
Something odd to note is that no matter what my loss starts at, it always tends to 11893.53125. Ironically, if it starts below there, it goes up to it. Example of several 40 epoch runs: 11893.52930, 11893.53223, 11893.52930, 11893.52930, 11893.52930.
My theory was that it was outputting 0s -- assuming an even distribution, the summed MSE loss would be 224x224x.5^2 = 12544, which is pretty close to our 11893.52930. Since regular images won't have an even pixel distribution, this makes sense. In practice, after testing the network, it outputs all 1s.
A side note is that when running the trainer test, the lambda value was not always exactly 2 -- every few iterations it was off by a fraction.
I'm really scratching my head here -- any folks with combat experience see any telltale signs of a solution? Thank you very much in advance for your time.