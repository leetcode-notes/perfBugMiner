fp16 inference is slower than fp32 on Nvidia Jetson TX2

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): r1.5
Python version: 3.5.2
Bazel version (if compiling from source): 0.9.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: Cuda 8.0 and cuDNN 7.0
GPU model and memory: Jetson TX2

The problem
I created a fully convolutional float 16 (half precision) neural network in tensorflow. When I run this network with some inputs, the inference time is slower than when I run the same network in float 32 (full precision) mode.
I should also note that the following variables are set:
os.environ['TF_FP16_CONV_USE_FP32_COMPUTE'] = '0' os.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE'] = '0'
As Nvidia Jetson TX2 support FP16 operations, I expected an inference time not worse than when I use FP32, but surprisingly it is about 1.5 times worse! (36 miliseconds vs 22 miliseconds). I guess it is becuase of the overhead of internal type conversion in the tensorflow core between float16 and float32!
Is it a problem with Tensorflow or TX2?