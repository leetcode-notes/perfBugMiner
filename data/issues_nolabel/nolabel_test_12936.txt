change cnn_mnist example to use Adam optimizer; added a 'loss' summary

I remember that other (non-estimator) versions of this example used the Adam optimizer, which has nicer convergence -- could we use it in this example?
Also, I added a summary for loss, so that it would show up on TensorBoard.
(I'd also like to add support for passing in num_steps, model_dir, etc. as command-line args with defaults, but I'll make that a separate PR unless you'd like it bundled with this one).