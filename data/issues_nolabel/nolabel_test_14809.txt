Batch Normalization layer is unusable

Despite the numerous submitted issues, tf.layers.batch_normalization still feels completely unusable. The major problems are:


It does not allow for input tensors with varying shapes. It is complete nonsense to have a fixed batch size. It should be allowed for the batch dimension to be vary.


One needs to manually update the running mean and variance. This is very uncomfortable and a very common pitfall for many beginners, while it would take just a couple of lines to do the update internally based on the value of the training parameter.


I have recently seen too many custom implementations of a batch normalization layer because of the above problems and it will definitely be very useful if these problems are fixed ASAP.
I am using tensorflow-gpu, version 1.4