variable_scope behave differently for master code and release 0.12.1

For master code, git rev-parse HEAD
ec7929b
The problem is one of my code used to work for release 0.12.1 fail for master code, which use adagrad as optimizer.
ValueError: Variable OptimizeLoss/w_h/Adagrad/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
I traced this and find below two simple codes can reproduce, the second one works for both 0.12.1 release and master, but the first one only works for 0.12.1 and fail master code.
I wonder what's the diff here ?  code 2 style is suggested ? Why code 1 will fail ? Also for code 1 if I use gradient desc instead of adgrad will not fail. Similar error can occur when using lstm cell.
code 1:
  loss, accuracy = model.build_graph(X, y)  

  tf.get_variable_scope().reuse_variables()    

  eval_loss, eval_accuracy = model.build_graph(eval_X, eval_y)    

code 2:
with tf.variable_scope("mlp") as scope:    

    loss, accuracy = model.build_graph(X, y)    

    scope.reuse_variables()    

    eval_loss, eval_accuracy = model.build_graph(eval_X, eval_y)