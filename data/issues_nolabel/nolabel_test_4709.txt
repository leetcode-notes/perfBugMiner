Is tensorflow consuming much more memory than torch?

I am trying to replicate the stacked hourglass architecture which is implemented in torch, https://github.com/anewell/pose-hg-train/. However, I am unable to train it using settings in the opts.lua file which set the train batch size as 6, number of train iteration as 8000, test batch size as 1 and number of test iteration as 1000. I understand that I can reduce the batch size or iteration number to fit the network. However, I just want to know if tensorflow is consuming more memory than torch, if the same architecture and settings can be implemented in tensorflow.