Zeros function not correctly determining shape.

I want to have dynamic batch sizes so I defined my input tensors as follows.
input_tensor = tf.placeholder(tf.float32, (None, TIME_STEPS, 128), 'input_tensor')
I then calculate the batch size.
batch_size = tf.shape(input_tensor)[0]
I can use this to determine the initial state for my RNNs as follows...
initial_state = state = tf.zeros((batch_size, encoder_multi_rnn.state_size), tf.float32)
...
forward_outputs[t], state = encoder_multi_rnn(getTimeStep(input_tensor, t), state)

This seems to work fine when I evaluate tf.shape(initial_state) in my sess.run() it prints what appears to be the correct value.  However, when do the following it prints (?, ?).  I think the second dimension should not be a ?
    state = tf.zeros((batch_size, encoder_multi_rnn.state_size), tf.float32)
    print(state.get_shape())

This is not causing an issue and my code compiles fine.  However this is causing an issue when I try to feed the output of the last time step of my sequence generating decoder back into the input of the MultiRNN.  On the first time step I feed it zeros since no output has yet to be produced.
    rnn_input = tf.reduce_sum(w * decoder_input, 1)
    last_out = decoder_outputs[t - 1] if t else tf.zeros((batch_size, 128))
    rnn_input = tf.concat(1, (rnn_input, last_out))

I get this error when concat is called.  ValueError: Linear expects shape[1] of arguments: [[None, None], [None, 1024]]
When I print last_out.get_shape() I get (?, ?) again.  In this case I think it should be (?, 128) since the last dimension is clearly defined.