Allow full deallocation of GPU memory

When using the TF C++ library inside an application that also uses GPUs for other tasks (not implemented in TF), it would be useful to be able to deallocate all the GPU memory TF has allocated once the session is closed, and no further TF calls are expected for the time being. gpu_options.allow_growth keeps TF's allocated pool small, but it still can grow to several GB. Even after the session is deleted, the pool doesn't shrink. To free it up, the whole application must be restarted, if I'm not mistaken.
Being able to destroy the ProcessState singleton seems to solve it without breaking anything. However, its destructor is protected. Alternatively, getting the Allocator for each GPU from ProcessState and manually destroying them does the trick, but renders TF unusable for all future operations because ProcessState still thinks the Allocators exist and doesn't recreate them when they are required again.
I think making the ProcessState destructor public (or adding a public method to invoke similar code) would be the best solution, but maybe I'm missing an obvious solution that already exists?