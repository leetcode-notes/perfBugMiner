Is it possible to calculate two kinds of gradient separately in tensorflow

w_1   = tf.get_variable("w_1", shape)   
w_2   = tf.get_variable("w_2", shape) 
output = tf.mul(w_1, w_2)
.....
.....
optimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)

As we know, when we run "optimizer", tensorflow will caculate gradient and update w_1 & w_2.
But what i want to do is, first, I want to treat w_1 as a constant, I just want to caculate gradient and update only w_2. Second, treat w_2 as a constant and caculate gradient and update only w_1. I want to take turns to do these things.
Actually, I have seen this  #before: #834. But I use BasicLSTMCell module.  I try this code: print (tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)), it shows there are four kinds of parameter in my neural network, which means besides w_1 and w_2, there are other two parameters in  BasicLSTMCell.
So, if I use such as var_list=[w_1], the other two parameters in BasicLSTMCell  can not be optimized, How can I do it?