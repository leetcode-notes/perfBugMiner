seq2seq.py: encoder_inputs from embedding_attention_seq2seq to static_rnn wrong?

The encoder_inputs to the embedding_attention_seq2seq function is stated as "A list of 1D int32 Tensors of shape [batch_size]."
Which is a shape of [input_size, batch_size]
This same input is then passed to core_rnn.static_rnn directly where static_rnn expects an input shape of [batch_size, input_size] instead.
with variable_scope.variable_scope(
      scope or "embedding_attention_seq2seq", dtype=dtype) as scope:
    dtype = scope.dtype
    # Encoder.
    encoder_cell = core_rnn_cell.EmbeddingWrapper(
        cell,
        embedding_classes=num_encoder_symbols,
        embedding_size=embedding_size)
    encoder_outputs, encoder_state = core_rnn.static_rnn(
        encoder_cell, encoder_inputs, dtype=dtype)

This seems like an error or am I unclear?
Thanks