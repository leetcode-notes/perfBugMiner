TensorRT create exception when engine is created from sub-process

Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the problem
TensorRT works well when called in a single process however it breaks when trt.utils.uff_to_trt_engine () is called from sub-process.
Source code / logs
Here is the summary of error and how to reproduce it.
Working TensorRT example Python code with a single process :
import pycuda.driver as cuda
import pycuda.autoinit
import argparse
import numpy as np
import time
import tensorrt as trt
from tensorrt.parsers import uffparser
uff_model = open('resnet_v2_50_dc.uff', 'rb').read()
parser = uffparser.create_uff_parser()
parser.register_input("input", (3, 224, 224), 0)
parser.register_output("resnet_v2_50/predictions/Reshape_1")
trt_logger = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)
engine = trt.utils.uff_to_trt_engine(logger=trt_logger,
stream=uff_model,
parser=parser,
max_batch_size=4,
max_workspace_size= 1 << 30,
datatype=trt.infer.DataType.FLOAT)
Non-working TensorRT  example Python code  where  trt.utils.uff_to_trt_engine () is called from sub-process :
import pycuda.driver as cuda
import pycuda.autoinit
import argparse
import numpy as np
import time
import tensorrt as trt
from tensorrt.parsers import uffparser
import multiprocessing
from multiprocessing import sharedctypes, Queue
def inference_process():
uff_model = open('resnet_v2_50_dc.uff', 'rb').read()
parser = uffparser.create_uff_parser()
parser.register_input("input", (3, 224, 224), 0)
parser.register_output("resnet_v2_50/predictions/Reshape_1")

trt_logger = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)
engine = trt.utils.uff_to_trt_engine(logger=trt_logger,
                                     stream=uff_model,
                                     parser=parser,
                                     max_batch_size=4,
                                     max_workspace_size= 1 << 30,
                                     datatype=trt.infer.DataType.FLOAT)

inference_p = multiprocessing.Process(target=inference_process, args=( ))
inference_p.start()
Console Error Message :
[TensorRT] ERROR: cudnnLayerUtils.cpp (288) - Cuda Error in smVersion: 3
terminate called after throwing an instance of 'nvinfer1::CudaError'
what():  std::exception