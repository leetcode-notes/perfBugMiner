Can aggregate all gradients from local wokers on local GPUs before push to parameter server ？

Can we use Graph mechnism to build the situation that all workers will aggregate all gradients from their replicated sub-graph， before pushing gradients to all pservers？
If OK， can show us some examples？
BTW：


with r1.0.1 version, inception training with distributed nodes will crash for several old APIs，and some more unpredictable reason.

TypeError: init() got an unexpected keyword argument 'replica_id' .



some ps error comes if we use non-default DNS IP when multiple NICS exist.