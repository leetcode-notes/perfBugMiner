Optimizing valid TensorFlow graph yields "graph_def is invalid" ValueError

tensorflow.python.tools.optimize_for_inference_lib.optimize_for_inference is producing an invalid graph definition. So far as I can tell this is not user error; optimizing a valid graph definition should produce a valid graph definition, so this appears to be a bug.
The following code demonstrates the problem. You will need the input graph definition model.txt.gz. Running the code loads the graph definition, verifies it is valid (by importing it and printing the number of nodes) then calls optimize_for_inference. We then attempt to verify the resulting graph definition but get the error
ValueError: graph_def is invalid at node u'valid/valid_fed/model/rnn/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/concat/axis': More inputs specified ('valid/valid_fed/model/rnn/rnn/while/Switch:1') than the op expects..

The error originates from
tensorflow/python/framework/importer.py, line 362, in import_graph_def

The attached model definition has been manually altered to reduce the size of the (frozen) parameters but the same error occurs with the unmodified original. Attempts to reproduce this problem with a simpler graph failed. Simpler graphs can be optimized successfully. I don't know what it is about this graph that causes the failure.
import gzip

import tensorflow as tf
from google.protobuf import text_format
from tensorflow.python.tools import optimize_for_inference_lib


def verify(graph_def):
    with tf.Graph().as_default():
        tf.import_graph_def(graph_def, name="")
        print(len(tf.get_default_graph().as_graph_def().node))


def read_graph_def(path):
    graph_def = tf.GraphDef()

    with gzip.open(path, "rb") as input_file:
        text_format.Merge(input_file.read(), graph_def)

    return graph_def


def optimize(input_graph_def):
    output_graph_def = optimize_for_inference_lib.optimize_for_inference(
        input_graph_def, ["valid/valid_fed/model/input/x"],
        ["valid/valid_fed/model/output/y"], tf.int32.as_datatype_enum)
    return output_graph_def


def main():
    input_graph_def = read_graph_def("model.txt.gz")
    verify(input_graph_def)
    output_graph_def = optimize(input_graph_def)
    verify(output_graph_def)


if __name__ == "__main__":
    main()

Environment details:

Linux Ubuntu 14.04
TensorFlow installed from source
TensorFlow version: ('v1.1.0-0-g1ec6ed5', '1.1.0')
Bazel version: 0.4.5
No GPU