Flatten all gradients in an MLP to a tensor of rank 1 (i.e. 1D array)

Suppose the following Keras model:
model = Sequential()
model.add(Dense(512, activation='sigmoid', input_shape=(784,)))
model.add(Dense(10, activation='softmax'))

Obviously we can calculate the gradients by:
grads = K.gradients(loss, params)
which just calls:
tf.gradients(loss, variables, colocate_gradients_with_ops=True)
This returns a list of tensors containing:

a tensor with 512x784 elements (input to hidden connections)
a tensor with the biases of the 512 units in the hidden layer
a tensor with 10x512 elements (hidden to output connections)
a tensor with the biases of the 10 output units

I would like to ask if there's a simple way to "flatten" grads to a single tensor of rank 1 (i.e. 1D array) with (512x784)+512+(10x512)+10elements, without looping over the layers and corresponding biases.
Thanks