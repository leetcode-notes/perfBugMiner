control_dependencies of ExponentialMovingAverage in cifar10_multi_gpu_train.py

There are two ExponentialMovingAverage in cifar10_multi_gpu_train.py.

For model parameters https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L220
Should the apply operation of ExponentialMovingAverage be called after the update of parameters?

with tf.control_dependencies([apply_gradient_op]):
    train_op = variable_averages.apply(tf.trainable_variables())

For loss
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L105-L106
Are these two lines redundant? Loss value won't change within a single run.

P.S. The document of ExponentialMovingAverage (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L172-L177) says

maintain_averages_op = ema.apply([var0, var1])
  # Create an op that will update the moving averages after each training
  # step.  This is what we will use in place of the usual training op.
with tf.control_dependencies([opt_op]):
    training_op = tf.group(maintain_averages_op)```

If we want to update the moving averages after the training step, should we call ema.apply within the context of tf.control_dependencies([opt_op])?