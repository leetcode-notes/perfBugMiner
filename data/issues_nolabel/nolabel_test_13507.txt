AttributeError in distributed training

Im am running tensorflow gpu '0.12.1' installed in a virtualenv on Debian 9.1 with cuda 8 and cudnn 5.1.
I tried to run the tutorial from https://www.tensorflow.org/versions/r1.2/deploy/distributed
I started 2 servers and 2 workers like in the tutorial. The servers started as expected.
I run this command to start a worker:
python cluster_trainer.py \
  --ps_hosts=131.188.30.144:2222,131.188.30.142:2222 \
  --worker_hosts=131.188.30.134:2222,131.188.30.135:2222 \
  --job_name=worker --task_index=1
The works exited with the error message:
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1050 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.392
pciBusID 0000:02:00.0
Total memory: 3.94GiB
Free memory: 3.87GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:02:00.0)
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> 131.188.30.144:2222, 1 -> 131.188.30.142:2222}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> 131.188.30.135:2222}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:2222
['131.188.30.144:2222', '131.188.30.142:2222'] ['131.188.30.134:2222', '131.188.30.135:2222'] worker 0
Traceback (most recent call last):
  File "cluster_trainer.py", line 85, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File "cluster_trainer.py", line 36, in main
    loss, global_step=global_step)
  File "/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py", line 269, in minimize
    grad_loss=grad_loss)
  File "/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py", line 320, in compute_gradients
    self._assert_valid_dtypes([loss])
  File "/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py", line 460, in _assert_valid_dtypes
    dtype = t.dtype.base_dtype
AttributeError: 'ellipsis' object has no attribute 'dtype'