Feature request: RMSProp without momentum variables

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): v1.3.0-24-g658866597 1.3.0
Python version: 3.6.3
Bazel version (if compiling from source): 0.6.1
CUDA/cuDNN version: None
GPU model and memory: None
Exact command to reproduce:

Describe the problem
The current RMSPropOptimizer always allocates momentum variables, even though the default (and probably 99% of people) never make use of it. In fact, if one wishes to combine momentum and adaptive gradient descent, he/she will most likely instantiate an AdamOptimizer instead. The extra variables waste precious GPU/CPU memory as well as disk space (when saved as checkpoints) while providing minimal utility.
Suggestion: introduce a new version of ApplyRMSProp operations that doesn't use momentum variables at all, and dynamically choose which implementation to use in RMSPropOptimizer constructor depending on whether the momentum argument is constant zero.
Alternative solution: change the current ApplyRMSProp operations so that it doesn't use momentum variables, and direct the minority users who currently need momentum with RMSProp to Adam instead.