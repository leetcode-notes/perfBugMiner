sharing variables but matrices are transposed even though src & dst tensors appear to have same shape

System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04


TensorFlow installed from (source or binary):
binary (pip install tensorflow-gpu==1.2.1)


TensorFlow version (use command below):
v1.2.0-5-g435cdfc, 1.2.1


Python version:
Python 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)
Type "copyright", "credits" or "license" for more information.
IPython 5.3.0 -- An enhanced Interactive Python.


Bazel version (if compiling from source):


CUDA/cuDNN version:
Cuda V8.0.61, CuDNN 5.1


GPU model and memory:
GeForce GTX 1080, 8GB


Describe the problem
At first I thought I was doing something wrong (initially posted on stackoverflow), but now I think this is a bug. Full text from SO pasted below.
I want to share variables between an autoencoder and a decoder. But the weights matrix from z to the first fully connected flat layer is transposed, even though at every step of the graph construction I dump the previous tensor scopename and shape to the console, and in both cases ('autoencoder' and 'decoder') the scopenames and shapes are identical.
To be specific the error is

Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).

But in both cases I'm creating a dense layer from (?, 128) to units=65536 with the same code:
print('dense from {} to {}'.format(t, post_z_flat_dim))
t = tf.layers.dense(inputs=t, units=post_z_flat_dim, ...)

which gives the output

autoencoder: dense from Tensor("z/Merge:0", shape=(?, 128), dtype=float32) to 65536


decoder: dense from Tensor("z_1:0", shape=(?, 128), dtype=float32) to 65536

The only difference is, for the decoder t is a placeholder whereas for autoencoder it's the result of a sequence of operations (but still of shape [None, 128])
Relevant code is below followed by the output.
class Model:
    def __init__(self,
                 usage, # Usage enum
                 reuse=None, # instance of another model to reuse sess, graph and variables from
                 ):

        if reuse == None:
            config = tf.ConfigProto()
            config.gpu_options.allow_growth=True    
            self.graph = tf.Graph()
            self.sess = tf.Session(graph=self.graph, config=config)
            root_scope = self.name
        else:
            self.graph = reuse.graph
            self.sess = reuse.sess
            root_scope = reuse.name

        with self.graph.as_default():
            with tf.variable_scope(root_scope, reuse=(reuse != None)):
                
                if usage >= Usage.autoencoder:
                    self.x = tf.placeholder(tf.float32, (None,) + tuple(img_shape), name='x')
                    t = self.x        
                    print('> x', t.shape, t.name)

                    # ENCODER
                    for i, filter_depth in enumerate(filters):
                        name = 'encoder{}'.format(i)
                        with tf.variable_scope(name):         
                            t = tf.layers.conv2d(...)                
                            if usage >= Usage.trainer: t = tf.layers.dropout(inputs=t, rate=self.dropout_conv_amt_T, training=True)
                            print('> conv', t.shape, t.name)
                            
                 
                # Z    
                if usage >= Usage.autoencoder:
                    t = tf.contrib.layers.flatten(t)
                    print('> pre z flat', t.shape, t.name)

                    if usage >= Usage.trainer:
                        t = tf.layers.dropout(inputs=t, rate=self.dropout_fc_amt_T, training=True)
                    
                    def vae_z(t):
                        self.mu = tf.layers.dense(inputs=t, units=z_dim, activation=None)
                        self.log_sigma = tf.layers.dense(inputs=t, units=z_dim, activation=None)                        
                        epsilon = tf.random_normal(tf.shape(self.log_sigma), name='epsilon')
                        t = self.mu + epsilon * tf.exp(self.log_sigma)
                        return t
                    
                    def nonvae_z(t):
                        return tf.layers.dense(inputs=t, units=z_dim, activation=None)
                    
                    t = tf.cond(self.use_vae_T, lambda: vae_z(t), lambda: nonvae_z(t), name='z')
                    self.z = t
                else:
                    t = tf.placeholder(tf.float32, [None, z_dim], name='z')
                    self.z = t

                print('> z', t.shape, t.name)
                
                # TODO is there a better way to calculate desired flat shape post z?
                post_z_img_dim = self.img_shape[0] // (2**len(filters))
                post_z_img_shape = [-1, post_z_img_dim, post_z_img_dim, filters[-1]]
                post_z_flat_dim = filters[-1] * post_z_img_dim * post_z_img_dim
                print('dense from {} to {}'.format(t, post_z_flat_dim))
                t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)
                print('> post z flat', t.shape, t.name)
                
                t = tf.reshape(t, shape=tf.constant(post_z_img_shape))
                print('> post z img', t.shape, t.name)


                # DECODER   

Output when I use the above. Note the scopenames and shapes in decoder1 (which is created without variable sharing), and it's identical to autoencoder. But decoder2 (which tries to share variables with autoencoder) fails.
autoencoder = Model(usage=Usage.autoencoder, reuse=None)
--------------------------------------------------------------------------------
__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3
> x (?, 256, 256, 3) x:0
> conv (?, 128, 128, 64) encoder0/conv/Relu:0
> conv (?, 64, 64, 128) encoder1/conv/Relu:0
> conv (?, 32, 32, 128) encoder2/conv/Relu:0
> conv (?, 16, 16, 256) encoder3/conv/Relu:0
> pre z flat (?, 65536) Flatten/Reshape:0
> z (?, 128) z/Merge:0
dense from Tensor("z/Merge:0", shape=(?, 128), dtype=float32) to 65536
> post z flat (?, 65536) dense/Relu:0
> post z img (?, 16, 16, 256) Reshape:0
> deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0
> deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0
> deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0
> deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0
> y (?, 256, 256, 3) final/conv/Sigmoid:0



decoder1 = Model(usage=Usage.decoder, reuse=None)
--------------------------------------------------------------------------------
__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3
> z (?, 128) z:0
dense from Tensor("z:0", shape=(?, 128), dtype=float32) to 65536
> post z flat (?, 65536) dense/Relu:0
> post z img (?, 16, 16, 256) Reshape:0
> deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0
> deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0
> deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0
> deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0
> y (?, 256, 256, 3) final/conv/Sigmoid:0



decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)
--------------------------------------------------------------------------------
__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3
> z (?, 128) z_1:0
dense from Tensor("z_1:0", shape=(?, 128), dtype=float32) to 65536
Traceback (most recent call last):

  File "<ipython-input-4-6f2915008bb4>", line 1, in <module>
    decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)

  File "/home/memo/Dropbox/research/py/apps/webcam-pix2pix-tensorflow/models/cnnvae.py", line 152, in __init__
    t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py", line 218, in dense
    return layer.apply(inputs)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py", line 320, in apply
    return self.__call__(inputs, **kwargs)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py", line 286, in __call__
    self.build(input_shapes[0])

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py", line 123, in build
    trainable=True)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py", line 1049, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py", line 948, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py", line 349, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py", line 275, in variable_getter
    variable_getter=functools.partial(getter, **kwargs))

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py", line 228, in _add_variable
    trainable=trainable and self.trainable)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py", line 341, in _true_getter
    use_resource=use_resource)

  File "/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py", line 658, in _get_single_variable
    found_var.get_shape()))

ValueError: Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).