ValidationMonitor + readers lead to: Coordinator didn't stop cleanly: Attempted to use a closed Session.

What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://andyljones.tumblr.com/post/133267887103/tensorflow-placeholder-queue-errors
Above not totally relevant, but couldn't find much.
Environment info
Operating System:
Relatively recently updated Arch.
Linux terrapin 4.6.5-4-ck #1 SMP PREEMPT Mon Aug 8 20:47:19 EDT 2016 x86_64 GNU/Linux
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
$ ls -l /usr/local/cuda/lib/libcud*
-rw-r--r-- 1 root root 189170 Sep 17 01:09 /usr/local/cuda/lib/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 311596 Sep 17 01:09 /usr/local/cuda/lib/libcudart.so.7.5.18
-rw-r--r-- 1 root root 558020 Sep 17 01:09 /usr/local/cuda/lib/libcudart_static.a

If installed from binary pip package, provide:

A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".

$ python -c "import tensorflow; print(tensorflow.__version__)"
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
0.10.0

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Would be a bit of a hassle, and I think I already found the bug so...
I've got a fairly simple model, it is fed by a string_input_queue() and then batched up by shuffle_batch() for training and batch() for evaluation. I'm using an estimator.Estimator() for the model and then calling fit() on the estimator with a ValidationMonitor.
I get a bunch of errors in the output when trying this. Here are the relevant parts:
WARNING:tensorflow:Coordinator didn't stop cleanly: Attempted to use a closed Session.
WARNING:tensorflow:Given features: {'clips': <tf.Tensor 'input_test-11/batch:0' shape=(100, 3840) dtype=float32>}, required signatures: {'clips': TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(100), Dimension(3840)]), is_sparse=False)}.
WARNING:tensorflow:Given targets: Tensor("input_test-11/batch:1", shape=(100,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(100)]), is_sparse=False).
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:03:00.0)
W tensorflow/core/kernels/queue_base.cc:294] _9_input_test-11/input_producer: Skipping cancelled enqueue attempt with queue not closed
WARNING:tensorflow:Coordinator didn't stop cleanly: Enqueue operation was cancelled
     [[Node: input_test-11/input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=["loc:@input_test-11/input_producer"], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](input_test-11/input_producer, input_test-11/input_producer/Identity)]]
Caused by op 'input_test-11/input_producer/input_producer_EnqueueMany', defined at:
  File "./train.py", line 320, in <module>
    tf.app.run()

Without the ValidationMonitor, everything is fine, but with it, most of these errors happen on every validate, and sometimes it throws the exception shown.
Of particular relevance is the error Coordinator didn't stop cleanly: Attempted to use a closed Session. which pretty clearly points out the problem.
You can see the exact situation the error is complaining about here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L757
There's even a comment saying that exactly that is about to be done, although I don't know why. If I move the session.close() after the coord.join(...) call, then all my problems go away.
Seems like a pretty clear problem with a pretty clear fix. Let me know if you need more details.