tf.biderectional_dynamic_rnn get stuck when running the graph

I built a 1-layer bidirectional RNN with 128 hidden nodes using the output = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seqlens, tf.float32, is_tuple=True, time_major=True) interface. I run the network with input data size of 57x285x4608 ([time_step x batch_size x num_feature]) but get stuck in the _outputs = sess.run(outputs, feeds). The system does not indicate any resource exhausted. When I reduce the time_step to 31, the network runs successfully. When I only reduce the number of 3rd dimension to 512, it still fails to work. It seems there is some constraints on the input sequence length.
Any idea on this problem?
I run this program on Nvidia DGX Server with 4 Tesla P100 GPUs. The OS is ubuntu 14.04.