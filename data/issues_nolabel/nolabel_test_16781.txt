Feature request: Save optimizer variables under separate name scope

Tensorflow version b'v1.3.0-rc1-3011-gd86448938' 1.3.0
Have I written custom code N/A
OS Platform and Distribution N/A
TensorFlow installed from N/A
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A
The variables initialized by the Adam optimizer is not created under its own name scope, but inherits the name of trainable variables, e.g. model/q_networks/online/conv2d/bias/Adam_1 (the Adam optimizer was not creater under the name scopes model, q_networks or online).
This creates an issue if you want to use another optimizer for a restored model, because you can't use the name scope to avoid restoring the variables related to the optimizer.
Here's a workaround:
        build_model()  # Build model under namescope 'model', without optimizer
        temp = set(tf.global_variables())
        if save or restore:
            saver = tf.train.Saver(
                var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model'))
        sess.run(tf.global_variables_initializer())
        if restore:
            saver.restore(sess, model_path)
        trainer = tf.train.AdamOptimizer(1e-4)
        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))  # Initialize Adam variables

It would be simpler (to figure out) if the optimizer variables had their own scope.