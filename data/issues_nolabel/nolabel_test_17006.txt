Batch normalization acting weird?!

System Information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7.1
TensorFlow version (use command below): 1.3.0
Python version: 3.6
Exact command to reproduce: See code below
Bazel version: Not building from source
CUDA/cuDNN version: Version 6
GPU model and memory: Nvidia Quadro k2200 with 4GB

Describe the problem
Hello everyone,
so theres the method tf.layers.batch_normalization and i am trying to understand what is Happening behind the curtains and in how far it is manually reproducible and in conformance of the paper by Ioffe&Szegedy. After running a small script which can only Train the two Parameters Gamma and beta to minimize the loss it appears that there is not normalization taking place(activation reduces by the mean over all activations of one Batch divided by the square-root of the variance of all activations of one batch). It is more or less always considering the Initial activation and simply multiplies this with Gamma plus beta. And that is especially interesting if the Batch size is reduced to one(not even remotely sure what is Happening then; yet it seems like this does not affect the computation at all)...
Thank you for your time and consideration!
Source code / logs
import numpy as np
import tensorflow as tf

test_img = np.array([[[[50],[100]],
                   [[150],[2000]]],
                   [[[0],[300]],
                   [[140],[5000]]],
                   [[[0],[300]],
                   [[140],[5500]]],
                   [[[0],[200]],
                   [[1400],[5000]]],
                   [[[0],[300]],
                   [[140],[5000]]]], np.float32)
gt_img = np.array([[[[60],[130]],
                [[180],[225]]],
                [[[60],[130]],
                [[180],[225]]],
                [[[600],[130]],
                [[1800],[225]]],
                [[[60],[100]],
                [[180],[205]]],
                [[[60],[100]],
                [[180],[205]]]], np.float32)
test_img_op = tf.convert_to_tensor(test_img, tf.float32)
norm_op = tf.layers.batch_normalization(test_img_op, momentum=0)

loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = gt_img,
                                                             logits = norm_op))
count = 0
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer_obj = tf.train.AdamOptimizer(0.01).minimize(loss_op)
with tf.Session() as sess:
    sess.run(tf.group(tf.global_variables_initializer(), 
                      tf.local_variables_initializer()))
    print(test_img)
    while True:
        count += 1
        if count < 100:
            new_img, op, lossy, trainable = sess.run([norm_op, 
                                                      optimizer_obj, 
                                                      loss_op, 
                                                      tf.trainable_variables()])
            print(trainable)
            print(new_img)
        else:
            break