Inference results depend on order of images in training batch

I've trained the same network two times on the same dataset of 5 images. For the first time, the images in a batch for each step were in the same order. For the second time, the batch was shuffled before every training step. Both models overfit. Both models were tested on shuffled images from training dataset.
The firs model shows 100% accuracy.
Prediction  Labels
[6 1 4 3 7] [6 1 4 3 7]
[3 4 1 7 6] [3 4 1 7 6]
[6 1 4 3 7] [6 1 4 3 7]
[4 3 7 6 1] [4 3 7 6 1]
[4 7 6 3 1] [4 7 6 3 1]
[1 3 7 6 4] [1 3 7 6 4]
[3 1 6 7 4] [3 1 6 7 4]

logs_not_shuffled.txt
The second model shows accuracy close to a random guess.
Prediction  Labels
[1 4 3 6 7] [7 4 6 3 1]
[3 6 4 1 7] [7 4 3 1 6]
[1 6 3 7 4] [1 6 4 3 7]
[4 7 3 6 1] [1 6 4 3 7]
[3 6 7 4 1] [4 1 3 7 6]
[1 3 7 4 6] [6 3 4 7 1]
[1 4 7 3 6] [3 7 1 4 6]

logs_shuffled.txt
Related issues - none
Environment:

Ubuntu 14.04 64-bit
CUDA 8.0
cuDNN 5.1

 -rw-r--r-- 1 root root   558720 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Jan 11 17:43 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Jan 11 18:10 /usr/local/cuda-8.0/lib64/libcudnn_static.a


tensorflow 0.12.1 from pip with GPU support

Dataset
5 images from MNIST
The attached archive
contains code for reproducible example.

runner.py is responsible for preparing image batches and maintains the queue.
network.py contains the code of simple neural network(2 convolutional layers with relu activation, softmax output layer)
train_not_shuffled.py trains and tests the network using non-shuffled batch
train_shuffled.py trains and tests the network using shuffled batch

example.gz
I've retrained the models several times, tried different datasets and network architectures. I've visualized graph to make sure inference uses the same variables. I've also visualized weights, they look slightly different for two cases but don't contain any anomalies that could explain such behavior.