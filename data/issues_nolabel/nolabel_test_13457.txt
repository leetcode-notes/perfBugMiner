Two issues on tf.nn.ctc_loss

Environments: tf version 1.3, CPU version; python 3.5/3.6; Win10/Ubuntu 16.04.
To begin with, we start from code:
import tensorflow as tf
num_classes, batch_size, seq_len = 3, 1, 2
labels = tf.SparseTensor(indices=[[0,0]], values=[0], dense_shape=[1,1])
inputs = tf.zeros([seq_len, batch_size, num_classes])
loss = tf.nn.ctc_loss(labels, inputs, [seq_len])
print(tf.InteractiveSession().run(loss))
tf.nn.ctc_loss behaves as expected, and print the correct answer: 1.09861231
Issue one: How to calculate the ctc loss of a sequence with all blanks? The tf.nn.ctc_loss API requires that values < num_labels, so we have no way to achieve it? If I do change the values in the above example to num_classes - 1 (the reserved blank ID), tf.nn.ctc_loss has no complain, and returns the wrong answer: 0.81093025! The correct answer is 2*log(3). The code to reproduce issue one is as below:
import tensorflow as tf
num_classes, batch_size, seq_len = 3, 1, 2
labels = tf.SparseTensor(indices=[[0,0]], values=[2], dense_shape=[1,1])
inputs = tf.zeros([seq_len, batch_size, num_classes])
loss = tf.nn.ctc_loss(labels, inputs, [seq_len])
print(tf.InteractiveSession().run(loss))
Issue two: Let's change the sequence length to 1 as below
import tensorflow as tf
num_classes, batch_size, seq_len = 3, 1, 1
labels = tf.SparseTensor(indices=[[0,0]], values=[2], dense_shape=[1,1])
inputs = tf.zeros([seq_len, batch_size, num_classes])
loss = tf.nn.ctc_loss(labels, inputs, [seq_len])
print(tf.InteractiveSession().run(loss))
and run the code again. This code gives the correct answer, log(3), in Ubuntu, but crashes in Win10 with message: Kernel died, restarting.