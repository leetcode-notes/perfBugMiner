bidirectional_rnn return state

Hi I am wondering why the bi-directional rnn doesn't return a final state? From a code point of view it would be very easy to return. I.e line 301 in rnn.py
output_fw, _ = rnn(cell_fw, inputs, initial_state_fw, dtype,
sequence_length)
One could just return _ (named as final_state or whatever) with the concatenated outputs at the end. Is there any theoretical reason why you would never want the final output state? I was planning on feeding the final forward state as the initial state of the forward and backward rnn's on the next pass. The reason I am wanting to do this is the sequences I am working on are much longer than the number of steps I can unroll (seqs are about 15k on average, I'm unrolling 1000 steps) so continuing state is pretty important. It seems weird that the bidirectional rnn would even have the ability to input an initial state but you can't return the output state (what state would you feed in if not the output of the last pass?)
Let me know what you think, i'm no rnn expert.
Off topic but does anyone have any advice for super long sequences? What is the maximum you think I could unroll? Are there other techniques I could use besides long unroll and refeeding the outputs? Can LSTM even learn 1000+ step dependencies?