Running language model example on multiple GPUs?

Hi all,
First of all thanks for this great library and state of the art examples provided.
I am trying to train a language model on multiple GPUs following your language model example(https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html). This is because (i) Dataset is huge (ii) Vocabulary is large.
I tried to change with tf.device("/cpu:0"): in the code to something like:
for d in ['/gpu:0', '/gpu:1']:
    with tf.device(d):
        embedding = tf.get_variable("embedding", [vocab_size, size])
        inputs = tf.nn.embedding_lookup(embedding, self._input_data) 
But got an error from MatMul. It happened even when I set it to one GPU. To resolve the problem with one GPU I tried to use:
def device_for_node(n):
  if n.type == "MatMul":
      return "/gpu:1"
  else:
      return "/cpu:0"
<some code>
with tf.Graph().as_default(), tf.Session() as session:
    with session.graph.device( device_for_node ):
which worked but speed dropped significantly, e.g., from 32,000 wps to 1200 wps which apparently means that it is not a good idea probably because TF optimizes both CPU and GPU usage.
I still can not figure out how to use multiple GPUs without losing the speed for this example. Anyone has had the same problem?
Thanks!
Hamid