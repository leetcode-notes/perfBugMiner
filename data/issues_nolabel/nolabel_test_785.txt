OOM error when using dropout

I thought I might have been going crazy so I only got around to posting this just now. I've got a GPU that can handle a batch size of 160 on my network, but I run it at 128. Thats just to show that I've got about 20% space left on my GPU ram when the network starts training. If I am using dropout, after about 100,000 iterations i get an OOM error.
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 128 } dim { size: 40 } dim { size: 40 } dim { size: 64 }
         [[Node: range_1/_1993 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_25333_range_1", tensor_type=DT_INT32, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]

Is it possible that there could be a memory leak somewhere?