Division by num_steps when calculating language model perplexity

Hi all,
I think I found something on the language modeling example that might be a bug:
As pointed in the tutorial, the perplexity can obtained from the averaged cross entropy. The code show here and here the division of the costs by num_iters.
Nevertheless, the seq2seq.sequence_loss_by_example is averaging losses across timesteps (here). As no value is passed to the parameter average_across_timesteps I'm assuming it is using the default value True.
After that, the model is further averaging the losses by the batch size here.
If I'm not wrong (and please correct me if I am), there is no need to divide the costs by num_iters as we already averaged over timesteps and the batch. There is no reference in the tutorial regarding why one should do that. If it is something that we should do it would be nice to include an instruction in the tutorial or a comment in the code. In addition, the Seq2seq tutorial is also showing the perplexity calculation and does not have any further division by the number of steps representing the length of the sequence, but by the number of steps between two verbosity points.
Also, imo, if the intention is averaging, the step parameter here  is the one that should be added to iters.