[BUG] seq2seq attention_wrapper use previous alignment?

When we use attention model, it's recommended to use previous alignment (attention weight).
But I saw the code in the attention_wrapper.py, it ignore the previous alignment.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399

@ebrevdo