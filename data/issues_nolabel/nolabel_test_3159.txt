How to remember the unit position of Dropout

I am using dropout in my neural network:
W   = tf.get_variable("W", [hidden_unit, 50]) 

def RNN_L1(x,  initial_state,  real_length):
                x = tf.transpose(x, [1, 0, 2]) 
                x = tf.reshape(x, [-1, word_dim]) 

                lstm_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)

                x = tf.split(0, sequence_length, x)
                outputs, _ = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)

                return outputs



outputs_L1  =  RNN_L1(x_vec,      self.initial_stateL1,     self.real_length)
outputs_L1  =  tf.pack(outputs_L1)


tensor_shape = outputs_L1.get_shape()

for step_index in range(tensor_shape[0]):
                output_relu= tf.matmul(outputs_L1[step_index,  :,  :], W) + B
                output_relu= tf.nn.relu(output_relu)
                output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)

outputs_L1 is the output of LSTM which has 3D-Tensor[sentence length, batchsize, hidden unit]. So, you can see the "for loop" code, I use "for loop" to do dropout for each outputs_L1[step_index,  :,  :]. But in this way, each outputs_L1[step_index,  :,  :] has it's own position of dropout.
For example:
outputs_L1[1,  :,  :] and outputs_L1[2,  :,  :] have different position of dropout at
output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob).
What I want is that they have same dropout position in "for loop". Can you help me?