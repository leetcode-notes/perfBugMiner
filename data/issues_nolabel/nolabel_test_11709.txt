Most of embedding_lookup/embedding_lookup_sparse computations are on CPU,  Most of the GPU time is transferring data.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Centos 7
TenorFlow installed from (source or binary):
source
TensorFlow version (use command below):
r.12
Python version:
2.7
Bazel version (if compiling from source):
0.5.2
CUDA/cuDNN version:
8.0
GPU model and memory:
P40

Describe the problem
I have done some profiling about embedding_lookup_sparse, i have use tf.device('\gpu:0') to specify where the ops run. But the results show:

Most of the operations are performed on the CPU.
GPU was transferring data at most of time.

After I looked into the codes, there are some Ops without GPU version, including dynamic_partition,  segment_sum, strided_slice. So I'm a little confused.
Is this intended behavior? Or are these Ops not implemented yet? Thanks for the explanations.
Here is some profiling result:
tfprof
I tensorflow/core/common_runtime/simple_placer.cc:841] hidden/embed/embedding_lookup_sparse/embedding_lookup/DynamicPartition:(DynamicPartition)/job:localhost/replica:0/task:0/cpu:0
hidden/embed/embedding_lookup_sparse: (SegmentSum): /job:localhost/replica:0/task:0/cpu:0
hidden/embed/embedding_lookup_sparse/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0

nvprof
==4762== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 44.07%  130.029s    240276  541.17us     768ns  8.1151ms  [CUDA memcpy DtoH]
 25.20%  74.3504s    480619  154.70us     992ns  8.9016ms  [CUDA memcpy HtoD]
  9.84%  29.0339s    330000  87.981us  61.922us  610.68us  void tensorflow::GatherOpKernel<float, int>(float const *, int const *, tensorflow::GatherOpKernel<float, int>*, __int64, __int64, __int64)