How to restore a distributed model and continue to train it with more workers?

Environment info
Running distributed tensorflow on kubernetes.
Installed version of CUDA and cuDNN:  No
Steps to reproduce

Train a model with 3 workers
restore the model with 4 workers

Logs or other output that would be helpful
tensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [4] rhs shape= [3]
     [[Node: save/Assign_103 = Assign[T=DT_INT64, _class=["loc:@local_steps"], use_locking=true, validate_shape=true, _device="/job:ps/replica:0/task:0/cpu:0"](local_steps, save/restore_slice_103)]]
     [[Node: save/restore_all/NoOp_S6 = _Recv[client_terminated=false, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:ps/replica:0/task:0/cpu:0", send_device_incarnation=6655275555388235088, tensor_name="edge_9844_save/restore_all/NoOp", tensor_type=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"]()]]

It seems that we need to have the same number of workers to restore a model. Is it possible to increase the number of workers without retraining the model from beginning?