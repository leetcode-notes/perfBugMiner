No gradient for log_softmax

It appears that the log_softmax doesn't have a gradient:
>>> import tensorflow as tf
>>> var = tf.Variable([[1.0, 2.0]])
>>> softmax = tf.nn.softmax(var)
>>> log_softmax = tf.nn.log_softmax(var)
>>> entropy = tf.reduce_sum(softmax * log_softmax)
>>> trainer = tf.train.GradientDescentOptimizer(0.1).minimize(-entropy)
LookupError: gradient registry has no entry for: LogSoftmax