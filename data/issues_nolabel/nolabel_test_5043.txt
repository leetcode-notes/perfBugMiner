Sudden OOM error with Tensorflow embedding_attention_seq2seq after successful runs

I've already found plenty of issues on the particular topic of memory issues with TensorFlow here, here, here, here and here and we are still awaiting fixes for the same.
I have implemented the RNN embedding Encoder Attention Decoder model as described by this tutorial using a custom language pair.
I successfully implemented it and the training started after I determined the maximum parameter values of:
Size of NN: 340
Batch Size: 32
Total Vocabulary:
English: 50000 words
Tamil: 40000 words
and trained on it quite a few times.
Preparing WMT data in /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/
Creating 3 layers of 325 units.
Reading model parameters from /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/checkpoints/translate.ckpt-9500
Reading development and training data (limit: 0).
  reading data line 100000
   Word2Vec English Encoding Successful!
   Word2Vec Tamil Encoding Successful!
global step 10000 learning rate 0.5000 step-time 1.56 perplexity 33.47
  eval: bucket 0 perplexity 10.47
  eval: bucket 1 perplexity 14.57
  eval: bucket 2 perplexity 20.74
  eval: bucket 3 perplexity 35.82
  eval: bucket 4 perplexity 42.77

.......................................
.......................................
.......................................

global step 109000 learning rate 0.4522 step-time 1.04 perplexity 3.79
  eval: bucket 0 perplexity 20.10
  eval: bucket 1 perplexity 18.52
  eval: bucket 2 perplexity 17.32
  eval: bucket 3 perplexity 28.79
  eval: bucket 4 perplexity 18.48
  eval: bucket 5 perplexity 17.07
  eval: bucket 6 perplexity 15.35
  eval: bucket 7 perplexity 29.22
  eval: bucket 8 perplexity 26.05
  eval: bucket 9 perplexity 28.58


I'm using Ubuntu 16.04 LTS with a 2GB Nvidia 650M GeForce card with:
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2015 NVIDIA Corporation
Built on Tue_Aug_11_14:27:32_CDT_2015
Cuda compilation tools, release 7.5, V7.5.17

root@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls
libcudnn.so  libcudnn.so.4  libcudnn.so.4.0.7  libcudnn_static.a

root@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# pip list | grep tensorflow
tensorflow (0.8.0)


root@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls -l /usr/lib/x86_64-linux-gnu/libcud*
-rw-r--r-- 1 root root   322936 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 -> libcudart.so.7.5.18
-rw-r--r-- 1 root root   383336 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart_static.a
lrwxrwxrwx 1 root root       12 Apr 14  2016 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1
lrwxrwxrwx 1 root root       17 Aug 14 16:25 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.367.35
-rw-r--r-- 1 root root 16881416 Mar 23  2016 /usr/lib/x86_64-linux-gnu/libcuda.so.361.42
-rwxr-xr-x 1 root root  8121032 Aug 14 14:14 /usr/lib/x86_64-linux-gnu/libcuda.so.367.35
lrwxrwxrwx 1 root root       13 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so -> libcudnn.so.4
lrwxrwxrwx 1 root root       17 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -> libcudnn.so.4.0.7
-rwxr-xr-x 1 root root 61453024 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7
lrwxrwxrwx 1 root root       17 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.3
-rwxr-xr-x 1 root root 60696704 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 62025862 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn_static.a


Since then, I required the cuDNN package for other purposes and installed cuDNN 5 first. found out that Tensorflow 0.8 works with cudnn 4 only and reinstalled the version 4 and the model testing worked correctly. But when I tried to run the model after the cudnn install:

................................
................................
...............................
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1640704 totalling 1.56MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1660928 totalling 1.58MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1676544 totalling 1.60MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 235 Chunks of size 1849600 totalling 414.52MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 1893120 totalling 3.61MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 1936640 totalling 5.54MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2023680 totalling 1.93MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2041088 totalling 1.95MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2067200 totalling 1.97MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2151936 totalling 2.05MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2154240 totalling 2.05MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2174208 totalling 2.07MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2328320 totalling 2.22MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2543872 totalling 2.43MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2590208 totalling 2.47MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2610176 totalling 2.49MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 2774272 totalling 7.94MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2861312 totalling 2.73MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2974976 totalling 2.84MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2994176 totalling 2.86MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3042816 totalling 2.90MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3445760 totalling 3.29MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 120 Chunks of size 3699200 totalling 423.34MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 3939072 totalling 7.51MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4348416 totalling 4.15MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4418816 totalling 4.21MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4980736 totalling 4.75MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5232128 totalling 4.99MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5960960 totalling 5.68MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54400000 totalling 51.88MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54545920 totalling 52.02MiB
I tensorflow/core/common_runtime/bfc_allocator.cc:692] Sum Total of in-use chunks: 1.40GiB
I tensorflow/core/common_runtime/bfc_allocator.cc:694] Stats: 
Limit:                  1534525440
InUse:                  1506072064
MaxInUse:               1506336256
NumAllocs:                  286635
MaxAllocSize:             92850432

W tensorflow/core/common_runtime/bfc_allocator.cc:270] ****************************************************************************************************
W tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 903.1KiB.  See logs for memory state.
W tensorflow/core/framework/op_kernel.cc:900] Resource exhausted: OOM when allocating tensor with shape[680,340]
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=9028 evicted_count=9000 eviction_rate=0.996899 and unsatisfied allocation rate=0
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 350, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 347, in main
    train()
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 186, in train
    target_weights, bucket_id, False)
  File "src/seq2seq_model.py", line 205, in step
    outputs = session.run(output_feed, input_feed)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 340, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 564, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 637, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 659, in _do_call
    e.code)
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[680,680]
     [[Node: gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device="/job:localhost/replica:0/task:0/gpu:0"](model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/concat, gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/add_grad/Reshape)]]
     [[Node: clip_by_global_norm_9/clip_by_global_norm_9/_2/_7992 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_624162_clip_by_global_norm_9/clip_by_global_norm_9/_2", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
Caused by op u'gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1', defined at:
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 350, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 347, in main
    train()
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 122, in train
    model = create_model(sess, False)
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 98, in create_model
    forward_only=forward_only)
  File "src/seq2seq_model.py", line 142, in __init__
    gradients = tf.gradients(self.losses[b], params)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py", line 481, in gradients
    in_grads = _AsList(grad_fn(op, *out_grads))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py", line 511, in _MatMulGrad
    math_ops.matmul(op.inputs[0], grad, transpose_a=True))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 1036, in matmul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 911, in _mat_mul
    transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py", line 655, in apply_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2154, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1154, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul', defined at:
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
[elided 5 identical lines from previous traceback]
  File "/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py", line 98, in create_model
    forward_only=forward_only)
  File "src/seq2seq_model.py", line 133, in __init__
    softmax_loss_function=softmax_loss_function)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py", line 961, in model_with_buckets
    decoder_inputs[:bucket[1]])
  File "src/seq2seq_model.py", line 132, in <lambda>
    lambda x, y: seq2seq_f(x, y, False),
  File "src/seq2seq_model.py", line 96, in seq2seq_f
    feed_previous=do_decode)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py", line 718, in embedding_attention_seq2seq
    initial_state_attention=initial_state_attention)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py", line 643, in embedding_attention_decoder
    initial_state_attention=initial_state_attention)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py", line 556, in attention_decoder
    cell_output, state = cell(x, state)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py", line 659, in __call__
    cur_inp, new_state = cell(cur_inp, cur_state)

hans@hans-Lenovo-IdeaPad-Y500:~/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec$ 


The pool allocator for my GPU is only going up to 300 from 100 when before, it went up to somewhere north of 6500. I don't know if this happened because of cudnn but this started right after I installed it. Please help.