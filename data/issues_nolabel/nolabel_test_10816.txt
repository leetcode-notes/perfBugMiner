Training LSTM RNN  model after restoration starting again with high loss

I have an LSTM based RNN language model where in after the initial training for 1000 iterations, the model is saved as follows
saver = tf.train.Saver()
saver.save(sess,'rnn_model.ckpt',global_step=1000)
I've restored the model by
        saver = tf.train.import_meta_graph("rnn_model.ckpt-1000.meta")
        saver.restore(self.sess, tf.train.latest_checkpoint('./'))
        graph = tf.get_default_graph()
        # restore the operations and placeholder as required from the graph
        # perform retraining

so far everything looks good, the model is restored, but however, when I plot the loss summary while training with the same input data as previous training, the loss again starts with high value as if it is training from start again freshly. I've tried to search all means to find the relevant forums but could not find a proper solution. please advise on the corrective steps
(related links
#6683
fchollet/keras#4875
https://stackoverflow.com/questions/41328769/tensorflow-loss-resets-after-successfully-restored-checkpoint
)