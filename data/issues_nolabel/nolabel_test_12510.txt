Support layer wise batch normalization parameter in tensorflow/contrib estimators

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):   yes
**OS Platform and Distribution:    Linux Ubuntu 14.04)
TensorFlow installed from (source or binary): installed from source
TensorFlow version (use command below):   1.2.0
Python version: Python 2.7.6
Bazel version (if compiling from source): 0.5.2
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Describe the problem
Tensorflow's current DNN classifier, regressor do not provide support to plugin in  layer-wise normalization function. This issue is fired to provide support to add a layer-wise norm func parameter in their constructors.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Following is a snippet of my proposed update in DNNClassifier. In case this change passes your review, I will submit the full code change including updates to all other related estimators:
diff
--git a/tensorflow/contrib/learn/python/learn/estimators/dnn.py b/tensorflow/contrib/learn/python/learn/estimators/dnn.py
index cb15ef2..a3d9c01 100644
--- a/tensorflow/contrib/learn/python/learn/estimators/dnn.py
+++ b/tensorflow/contrib/learn/python/learn/estimators/dnn.py
@@ -127,6 +127,10 @@ def _dnn_model_fn(features, labels, mode, params, config=None):
params.get("input_layer_min_slice_size") or 64 << 20)
num_ps_replicas = config.num_ps_replicas if config else 0
embedding_lr_multipliers = params.get("embedding_lr_multipliers", {})


layer_norm_func = params.get("layer_norm_func")


layer_norm_params = params.get("layer_norm_params", {})



layer_norm_params["mode"] = mode
features = _get_feature_dict(features)
parent_scope = "dnn"
@@ -168,6 +172,8 @@ def _dnn_model_fn(features, labels, mode, params, config=None):
net,
num_hidden_units,
activation_fn=activation_fn,


      normalizer_fn=layer_norm_func,



      normalizer_params=layer_norm_params,
       variables_collections=[parent_scope],
       scope=hidden_layer_scope)
   if dropout is not None and mode == model_fn.ModeKeys.TRAIN:



@@ -297,6 +303,8 @@ class DNNClassifier(estimator.Estimator):
weight_column_name=None,
optimizer=None,
activation_fn=nn.relu,


         layer_norm_func=None,



         layer_norm_params=None,
          dropout=None,
          gradient_clip_norm=None,
          enable_centered_bias=False,



@@ -372,6 +380,8 @@ class DNNClassifier(estimator.Estimator):
"optimizer": optimizer,
"activation_fn": activation_fn,
"dropout": dropout,


      "layer_norm_func": layer_norm_func,



      "layer_norm_params": layer_norm_params,
       "gradient_clip_norm": gradient_clip_norm,
       "embedding_lr_multipliers": embedding_lr_multipliers,
       "input_layer_min_slice_size": input_layer_min_slice_size,