Equivalent of caffe iter_size in TF

Can we get the equivalent of caffe's iter_size parameter in TF? This accumulates gradient calcs over several GPU cycles before doing the weight update. It effectively allows a larger batch size. TensorFlow doesn't natively have this but some ppl seem to have implemented sth like it themselves, e.g
https://stackoverflow.com/questions/42156957/how-to-update-model-parameters-with-accumulated-gradients
I think it'd be a useful parameter to have as part of official TF ... or is there some easy way to implement this functionality already?
Shaun