Allow variable_overwrites on scope level

This is a request for allowing to pass a dict of variable_overwrites to variable scopes which to be returned when tf.get_variable is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:
    import numpy as np
    with tf.variable_scope("one"):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope("two"):
            x1 = tf.get_variable("x", initializer=np.random.randn(5, 5).astype("float32"))
            c1 = tf.sqrt(tf.abs(x1 + a))

    variables_overwrites = {x1._shared_name: c1}
    with tf.variable_scope("one", reuse=tf.AUTO_REUSE, variables_overwrites=variables_overwrites):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope("two"):
           // x2 here is in fact the value of c1
            x2 = tf.get_variable("x", initializer=np.random.randn(5, 5).astype("float32"))
            c2 = tf.sqrt(tf.abs(x2 + a))

This is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly.