Implemented selu activation #10612

This pull request implements the SELU activation function. The activation function works properly when training a simple MLP for digit recognition. But, the following test fails: //tensorflow/python/kernel_tests:relu_op_test. This happens due to large error when checking the gradient of gradient. The log is show below:
======================================================================
FAIL: testGradGradFloat32 (__main__.SeluTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/lakshayg/.cache/bazel/_bazel_lakshayg/51c9ff9b2a8e10dfb691d982f11ff475/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/relu_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/relu_op_test.py", line 375, in testGradGradFloat32
    self.assertLess(err, 1e-4)
AssertionError: 0.080660700798034668 not less than 0.0001

======================================================================
FAIL: testGradGradFloat64 (__main__.SeluTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/lakshayg/.cache/bazel/_bazel_lakshayg/51c9ff9b2a8e10dfb691d982f11ff475/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/relu_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/relu_op_test.py", line 393, in testGradGradFloat64
    self.assertLess(err, 1e-6)
AssertionError: 0.080654564805808127 not less than 1e-06

----------------------------------------------------------------------
Ran 30 tests in 0.377s

FAILED (failures=2)
elu (float32) gradient of gradient err =  1.90436840057e-05
elu (float64) gradient of gradient err =  1.50806227728e-07
elu (float32) gradient err =  1.90436840057e-05
elu (float64) gradient err =  1.50806227728e-07
relu6 (float32) gradient err =  0.0
relu6 (float64) gradient err =  0.0
relu (float32) gradient of gradient err =  0.0
relu (float64) gradient of gradient err =  0.0
relu (float32) gradient err =  1.29342079163e-05
relu (float64) gradient err =  8.881784197e-16
selu (float32) gradient of gradient err =  0.080660700798
selu (float64) gradient of gradient err =  0.0806545648058
selu (float32) gradient err =  4.69088554382e-05
selu (float64) gradient err =  2.65132328758e-07

Any help in resolving this issue is appreciated.