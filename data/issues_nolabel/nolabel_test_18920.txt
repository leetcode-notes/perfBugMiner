Feature request: Gradient for number of loop iterations for automatic differentiation of physics simulations

System information
Not relevant.
Have I written custom code yes
OS Platform and Distribution different linux systems
TensorFlow installed from source and pip (on different systems)
TensorFlow version different versions, all >= 1.5.0
Bazel version different versions on different systems
CUDA/cuDNN version different versions, all >= 8
GPU model and memory GTX 1080, GTX 1060, Tesla P40 (potentially many others on our cluster, but not tested yet)
Exact command to reproduce run bug_test.py
Describe the problem
Motivation
(you might want to skip this)
Tensorflow is a powerful framework that can potentially be used for much more than static computations like those needed to train neural networks. We (a couple of physicists) are trying to use Tensorflow to get a gradient on some physical parameters to fit those parameters to measured data by propagating the gradient through the entire physics simulation to replicate the data. This is extremely relevant for a lot of people and the idea is not new.  Instead of performing extremely time consuming (in terms of computational and human work time) grid searches on high dimensional parameter spaces we would like to be able to directly perform gradient descent on such spaces in an automated fashion by using Tensorflows automatic differentiation. You can find our work in progress repository here.
The Problem
Like explicitly stated here on page 15 "This means that we assume that pred is not trainable", the number of loop iterations is just a constant while backpropagating the gradient. This leads to some extremely unexpected behavior like described in this issue (at least unexpected for someone who is a Tensorflow newbie like I am). For our plans to work it is mandatory for the gradient to include information on how the number of loop iterations would have changed, if the trainable variables changed.
Example
(You might want to skip this)
In our project we have a trainable variable, which describes the mean distance between scattering events of photons. Tensorflow is able to propagate the gradient through the simulation (while loop), but the gradient only "thinks" that the photons will reach further points when the scattering length is longer, but "is not aware" of the fact that there will be less iterations.
Request
I do not know if this is even possible with how Tensorflow works, but we would love to see a way to make this work. Tensorflow would be a really great tool for us to use. So is this something you might work on in the future?
Also: does anyone maybe have any idea of how to solve or get around this right now with the current implementation? We have put quite some thought into this ourselves already, but until now we have not found a working solution.
Source code
For a work in progress example of how this would be useful you can take a look at the master and experimental branches of our repo.
For a stripped down minimal one dimensional example of the simulation you can take a look at minimal_1d.py, this in its current state however actually converges. For some more information on the unexpected behavior take a look at my issue where I reference commits, which demonstrate what's happening.
A minimal example, which does not include any physics and purely demonstrates the unexpected behavior is given here.
All of the code includes detailed comments.
If you are interested in the background and general idea you can take a look at my slides, even though I am not sure how much of a help those will be for someone from outside. Slides 5 to 8 might be helpful for everyone though. Especially slide 5 which shows how we fit the data using simulations. For the general concept testing right now the data is also given by the same simulation with "true" parameters.
We really hope to get some form of feedback here and I am sorry if this is a trivial or stupid request in the eyes of someone more experienced with Tensorflow.