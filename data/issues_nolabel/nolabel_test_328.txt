initializing seq2seq embedding with pretrained w2v

Hi, is there a straightforward way to initialize seq2seq embedding with pretrained w2v?
the embedding variable is in the embedding_tied_seq2seq (the model I am using). is there anyway to access this variable from translate.py and call .assign() to assign it after random initialisation with all other variables?? is there a global variable name I could call for this embedding?