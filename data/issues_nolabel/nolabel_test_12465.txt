Tensorflow Debugger eats disk space with RNNs

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.3
Python version:  3.5

Describe the problem
I have an RNN that I'm trying to debug using LocalCLIDebugWrapperSession which runs on very long sequences (4800 steps). Even for a basic LSTM cell that I apply via tf.nn.dynamic_rnn, tfdbg uses a lot of disk space (>5 GB sometimes). When I accidentally collected gradient infromation via tf.contrib.layers.optimize_loss(..., summaries=["gradients", ...]), this ballooned even more to >70 GB (which promptly crashed my laptop).
From inspecting the tfdbg dump in /tmp/, it looks like this is because tfdbgdumps out information for each time step. Especially withtf.contrib.layers.optimize_loss` capturing gradients, this means that there are hundreds (thousands?) of small files being created on each time step.
In some sense, this is expected behavior (each time step represents a group of TF operations), but using 70 GB seems like a pretty sharp-edged API that's easy to mis-use. I'm not sure what to really do here -- maybe there's a way to somehow compress these files or to combine all these small files across time steps into one large file? It could also just be a documentation problem.