tf.igamma (lower regularized incomplete Gamma function) returns the incorrect derivative

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.11.6
TensorFlow installed from (source or binary): binary (pypi)
TensorFlow version (use command below): v1.6.0-0-gd2e24b6039
Python version: 3.6
Bazel version: using a precompiled version, not sure
CUDA/cuDNN version: not using a GPU
GPU model and memory: not using a GPU
Exact command to reproduce: python igamma_test.py

Describe the problem
tf.igamma, which is the lower regularized incomplete Gamma function, returns an incorrect derivative with respect to a
This is probably very low down on the list of things to fix, but I wanted to highlight it since I spent something like 5 hours trying to understand why my model wasn't converging. I was fitting a Gamma distribution, and deep into the code it turns out that tf.igamma doesn't return the right derivative.
I suspect the derivative wrt a isn't supported, but I would have much rather seen an exception being thrown.
My workaround ended up being not fitting a with gradient descent, but instead just perturbing it by epsilon (luckily I only had one single value that I tried to fit)
Filing this issue mostly in the hope that anyone in the future doesn't waste the same amount of time that I spent.
Source code / logs
a = tf.placeholder(dtype=tf.float32, shape=[None])
x = tf.placeholder(dtype=tf.float32, shape=[None])
y = tf.igamma(a, x)
y_grad_a = tf.gradients(y, a)  # returns None, should return a tensor
y_grad_x = tf.gradients(y, x)  # returns a tensor