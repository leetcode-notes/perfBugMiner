Incorrect second derivative for softmax cross entropy

System information

I've written custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tensorflow-gpu==1.2.1
python version: 2.7.12
CUDA/cuDNN version: CUDA version: 8.0 and CUDNN version is: 8.0
GPU model and memory: GeForce GTX 750 2GB

Incorrect second derivative for softmax cross entropy
Source code / logs
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np

tfe.enable_eager_execution()

logits = [0.5, 0.5]
y = [1, 0]

def loss_function(x):
    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)
    return loss2

grad_loss = tfe.gradients_function(loss_function)
print grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]

gradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0])
print gradgrad_loss(logits)[0] # this should be [-0.25,  0.25], but it prints [0. 0.]