Documentation of raw_rnn is wrong

In the official documentation of tf.nn.raw_rnn we have emit structure as the third output of loop_fn when the loop_fn is run for the first time
later on the emit_structure is used to copy tf.zeros_like(emit_structure) to the minibatch entries that are finished by emit = tf.where(finished, tf.zeros_like(emit_structure), emit)
my lack of understanding is: emit structure is None so tf.where(finished, tf.zeros_like(emit_structure), emit) is going to throw a ValueError as tf.zeros_like(None) does so. Can somebody please fill in what i am missing here?
basically even the example implementation of dynamic_rnn using raw_rnn is simple wrong
to quote exact parts:
in dynamic_rnn implementation shown
def loop_fn(time, cell_output, cell_state, loop_state):
emit_output = cell_output  # == None for time == 0
and in raw_rnn we have:
time = tf.constant(0, dtype=tf.int32)
(finished, next_input, initial_state, emit_structure, loop_state) = loop_fn(
time=time, cell_output=None, cell_state=None, loop_state=None)
.
.
.
Emit zeros and copy forward state for minibatch entries that are finished.
state = tf.where(finished, state, next_state)
emit = tf.where(finished, tf.zeros_like(emit_structure), emit)
This will fail at the first time step itself. But i have gone through code, this is not how it is implemented. In case of None for emit_structure  cell_output is used.
raw_rnn is really powerful and once i have understood it there is no going back to dynamic_rnn or any other rnn unfolding mechanism. But the documentation is making is really tough to understand this amazing api. An improvement is in order
Have I written custom code N/A
OS Platform and Distribution windows 10
TensorFlow installed from https://www.tensorflow.org/install/install_windows
TensorFlow version 1.7
Bazel version NA
CUDA/cuDNN version NA
GPU model and memory NA
Exact command to reproduce tf.zeros_like(None)