Saver (sharded) ignores number of checkpoints to keep

tf.train.Saver() has a default limit of 5 most recent checkpoints to keep (max_to_keep=5). When I use a Saver with sharded=True option, only one machine performs this check and keeps ALL the checkpoints without removing any of them.
Environment info
Operating System: Ubuntu 14.04 LTS
Built from source; commit hash: aa2cacd
Steps to reproduce

Init a sharded saver and pass it to the Supervisor for distributed training and auto-model saving after every save_model_secs.

saver = tf.train.Saver(sharded=True)
sv = tf.train.Supervisor(is_chief=is_chief,
                                 logdir=FLAGS.train_dir,
                                 init_op=init_op,
                                 summary_op=None,
                                 global_step=global_step,
                                 saver=saver,
                                 save_model_secs=FLAGS.save_interval_secs)


Start two workers and one ps on machine 0. Start two more workers and one ps on machine 1.
The saver keeps the 5 most recent sharded checkpoints on machine 0 (where the chief worker also runs). But machine 1 keeps all sharded checkpoints from beginning.