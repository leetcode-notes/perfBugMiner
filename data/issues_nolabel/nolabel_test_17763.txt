TensorFlowInferenceInterface: OutOfMemoryError on Android SDK 16

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android SDK 16 (4.1.2)
TensorFlow installed from (source or binary): compile 'org.tensorflow:tensorflow-android:+
TensorFlow version (use command below): latest
Python version: NA
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce: NA

Describe the problem
I found a strange bug using the TensorFlowInferenceInterface on Android SDK 16. When closing the interface once (using inferenceInterface.close()), the next start (using new TensorFlowInferenceInterface()) will cause an OutOfMemoryError, even though there is enough free RAM available. In contrast, there isn't any problem when keeping the interface alive to reuse it. The latter implies wasting resources though. The only way to prevent the OOM seems to be the manual kill of the surrounding process (android.os.Process.killProcess(android.os.Process.myPid());).
Just to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.
I found this behavior using an old Samsung Galaxy S2. Couldn't reproduce it on newer Android versions. As SDK 16 is already quite old, I don't know whether anybody is willing to dig deeper into this bug. However, it costs me a lot of time to isolate the problem and I hope the workaround (using a separate process and killing it manually after closing the interface) may at least save somebody else some time.