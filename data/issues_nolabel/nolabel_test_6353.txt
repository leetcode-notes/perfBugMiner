online add worker to  Running a distributed training session

When you use distributed tensorflow for training tasks, if you start your quest, it has been running for a long time. At this point you find that the number of workers to start too little, so can not be a good convergence.
At this point, you should increase the worker online to the distributed training tasks.
example:
On ps0.example.com:
$ python trainer.py 
     --ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 
     --worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 
     --job_name = ps --task_index = 0
On ps1.example.com:
$ python trainer.py 
     --ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 
     --worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 
     --job_name = ps --task_index = 1
On worker0.example.com:
$ python trainer.py 
     --ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 
     --worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 
     --job_name = worker --task_index = 0
On worker1.example.com:
$ python trainer.py 
     --ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 
     --worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 
     --job_name = worker --task_index = 1
When you start, then, you need to increase worker2.example.com:2222, should do so:
Write a python script:,note must call all ps servers
import tensorflow as tf
def main (_):
    with tf.Session ('grpc: //ps0.example.com:2222') as sess:
        sess.add_onlineworker (2, 'worker2.example.com: 2222')
    with tf.Session ('grpc: //ps1.example.com:2222') as sess:
        sess.add_onlineworker (2, 'worker2.example.com: 2222')
if name == "main":
    tf.app.run ()
Then  run  this  python script:,
On worker2.example.com:
$ python trainer.py 
     --ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 
    --worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222, 
     --worker2.example.com:2222 --job_name = worker --task_index = 2
#Then you can see worrker has started training
#If ps  nodes in the cluster restart, remember to start with the worker2.
For example ps0 restart, you must start:
On ps0.example.com:
$ python trainer.py 
     --ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 
    --worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222, 
     --worker2.example.com:2222 --job_name = ps --task_index = 0
if  ps1 restart, you must start:
On ps1.example.com:
$ python trainer.py 
     --ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 
    --worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222, 
     --worker2.example.com:2222 --job_name = ps --task_index = 1
Can continue to train.