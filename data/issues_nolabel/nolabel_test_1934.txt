Is there a way improve memory strategy with in-place & broadcasting & bsxfun?

Suppose I have 4GB gpu memory, see the following code.
A = tf.Variable(tf.zeros([5000, 5000, 25]))  # 5000*5000*25*4byte=2.5GB
B = tf.Variable(tf.zeros([5000, 25])  # 5000*25*4byte=500KB
sub_op = A.assign(A - B)
So, if you run sub_op, apparently, it will lead to OOM error, since A-B will need 5GB peak memory because of the broadcasting repetition.
Question 1: Is there a way to use functions like bsxfun in matlab which will not extend two object into the same big size and then calculate? i.e.
sub_op = A.assign(tf.bsxfun(@minus, A, B))
Question 2: So, there's another problem about in-place assignment. Since sub_op related to in-place assignment, if the tensorflow haven't implemented it reasonable, it will lead to OOM error two, since there's no 5GB gpu memory to handle two 2.5GB objects.
Any ideas?