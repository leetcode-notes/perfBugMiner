tf.einsum doesn't perform common subgraph elimination

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.4
TensorFlow installed from (source or binary): Binary (pip)
TensorFlow version (use command below): 'unknown' 1.6.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

Describe the problem
When I inspect the computation graph in tensorboard, there are two separate calls to einsum and no subgraphs (ie calls to matmul) are shared. I would have expected the "jb,abcd" tensors to be contracted once, then contracted with (ia + ai) afterwards.
Source code / logs
tf_eri = tf.constant(np.random.random([10,10,10,10])
tf_ao = tf.constant(np.random.random([10,10])

tf_foo = tf.einsum("ia,jb,abcd", tf_ao, tf_ao, tf_eri)
tf_bar = tf.einsum("ai,jb,abcd", tf_ao, tf_ao, tf_eri)
tf_res = tf_foo + tf_bar