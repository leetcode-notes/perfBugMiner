Support for half-floats (float16/fp16)

This is a tracking bug for adding support for the half type (aka float16, or fp16) in TensorFlow. Half computation is supported by GPUs only, although newer Intel CPUs (Haswell and newer) have support for converting back and forth between fp16 and fp32 in hardware (F16C). CUDA has some support for half since 7.5, although it's a bit cumbersome (it's not a first-class type, but relies on macros containing asm statements; effectively intrinsics).
fp16 is interesting for two primary reasons: It would allow us to fit twice as large models in available GPU RAM, and it reduces memory bandwidth use, a precious resource on the GPU. The next generation of NVIDIA GPUs (Pascal) will also be able to do computation directly on two half-floats (in a SIMD-like structure) as fast as on a single float, although that would be somewhat more intrusive in code.
It is not 100% clear exactly how much of TensorFlow we need fp16 support for; interested parties are asked to comment. CuBLAS and CuDNN already has some support for half in their latest versions (so it would be natural to provide those interfaces), and Eigen also has beginning support.