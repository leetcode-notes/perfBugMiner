Change Optimizer while training, after a certain number of steps?

I am working on machine translation, using seq2seq model in Tensorflow. I am aware that once the graph has been established, it cannot be modified during training.
What if I want to change the optimizer from SGD to Adam after certain global steps?
How should the code be?
Thanks a lot.