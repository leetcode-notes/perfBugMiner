Keras TimeDistributed wrapper around GlobalMaxPooling2D error with TPU

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.7.0-rc1
Python version: 2.7.13
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: Google Cloud TPU v2-8 running TF 1.7
Exact command to reproduce: See below

Describe the problem
Using a Keras TimeDistributed wrapper to wrap a Keras GlobalMaxPooling2D layer, and processing on a Google Cloud TPU results in a ValueError. The layer behaves as expected if the TPUEstimator is configured to use CPU. Error raised by the TPU failure case:
ValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').
Source code / logs
Test code minimal example keras_td_test.py:
""" Test for Keras model TimeDistributed wrapper on TPU.
    Based on https://github.com/tensorflow/tpu/blob/master/models/official/resnet/
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from absl import flags
import absl.logging as _logging  # pylint: disable=unused-import
import tensorflow as tf
import numpy as np

from tensorflow.contrib.tpu.python.tpu import tpu_config
from tensorflow.contrib.tpu.python.tpu import tpu_estimator
from tensorflow.contrib.tpu.python.tpu import tpu_optimizer

# Define flags for the system
FLAGS = flags.FLAGS
flags.DEFINE_bool(
    'use_tpu', True,
    help=('Use TPU to execute the model for training and evaluation. If'
          ' --use_tpu=false, will use whatever devices are available to'
          ' TensorFlow by default (e.g. CPU and GPU)'))
flags.DEFINE_string(
    'tpu_name', default=None,
    help='Name of the Cloud TPU for Cluster Resolvers.')
flags.DEFINE_string(
    'model_dir', default=None,
    help=('The directory where the model and training/evaluation summaries are'
          ' stored.'))

def main(unused_argv):
    # Get the TPU GRPC URL if it is needed
    if FLAGS.use_tpu:
        tpu_cluster_resolver = (
            tf.contrib.cluster_resolver.TPUClusterResolver(
                FLAGS.tpu_name))
        tpu_grpc_url = tpu_cluster_resolver.get_master()
    else:
        tpu_grpc_url = None

    # Set the configuration for the TPU
    config = tpu_config.RunConfig(
        master=tpu_grpc_url,
        model_dir=FLAGS.model_dir)

    # Create the TPUEstimator
    test_estimator = tpu_estimator.TPUEstimator(
            use_tpu=FLAGS.use_tpu,
            model_fn=test_model_fn,
            config=config,
            train_batch_size=1024)

    # Train the estimator for 10 steps
    test_estimator.train(input_fn, max_steps=10)

def input_fn(params):
    # Generate a random dataset of the correct shape
    data = np.random.rand(1024, 10, 10, 10).astype(np.float32)
    label = np.random.rand(1024,10).astype(np.float32)

    # Repeat and batch
    rand_dataset = tf.data.Dataset.from_tensor_slices((data, label)).repeat()
    rand_dataset = rand_dataset.apply(tf.contrib.data.batch_and_drop_remainder(params['batch_size']))

    # Make input_fn for the TPUEstimator train step
    rand_dataset_fn = rand_dataset.make_one_shot_iterator().get_next()
    return rand_dataset_fn

def test_model_fn(features, labels, mode, params):
    # Dense layer to give the system something to train
    dense_out = tf.keras.layers.Dense(10)(features)

    ##########################################
    # The Keras wrapper that causes an error #
    ##########################################
    predictions = tf.keras.layers.TimeDistributed(
        tf.keras.layers.GlobalMaxPooling2D()
    )(dense_out)

    # Create ops for the TPUEstimatorSpec
    loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)
    optimizer = tf.train.GradientDescentOptimizer(0.001)
    if FLAGS.use_tpu:
        optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)
    global_step = tf.train.get_global_step()
    train_op = optimizer.minimize(loss, global_step)

    # Return the TPUEstimatorSpec
    return tpu_estimator.TPUEstimatorSpec(
        mode=mode,
        loss=loss,
        train_op=train_op)

if __name__ == '__main__':
    # Do the thing
    print('main wrapper')
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run()
Error on TPU, and sanitized log file:
$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --tpu_name=tpu-name
WARNING: Logging before flag parsing goes to stderr.
W0329 19:53:32.386956 139824142264064 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
main wrapper
W0329 19:53:32.582829 139824142264064 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/__init__.py", line 41, in autodetect
    from . import file_cache
  File "/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/file_cache.py", line 41, in <module>
    'file_cache is unavailable when using oauth2client >= 4.0.0')
ImportError: file_cache is unavailable when using oauth2client >= 4.0.0
2018-03-29 19:53:32.619736: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-03-29 19:53:32.622444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job local -> {0 -> localhost:44849}
2018-03-29 19:53:32.623808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:44849
W0329 19:53:32.704463 139824142264064 tf_logging.py:126] Estimator's model_fn (<function test_model_fn at 0x7f2b1fc4d7d0>) includes params argument, but params are not passed to Estimator.
I0329 19:53:32.705055 139824142264064 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2b1fc4bc90>, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': u'grpc://10.240.1.2:8470', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': u'grpc://10.240.1.2:8470', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}
I0329 19:53:32.772562 139824142264064 tf_logging.py:116] Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.
2018-03-29 19:53:32.773276: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:351] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
I0329 19:53:32.776926 139824142264064 tf_logging.py:116] Found TPU system:
I0329 19:53:32.777101 139824142264064 tf_logging.py:116] *** Num TPU Cores: 8
I0329 19:53:32.777353 139824142264064 tf_logging.py:116] *** Num TPU Workers: 1
I0329 19:53:32.777436 139824142264064 tf_logging.py:116] *** Num TPU Cores Per Worker: 8
I0329 19:53:32.777509 139824142264064 tf_logging.py:116] *** Available Devices: [_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184)]
I0329 19:53:32.789855 139824142264064 tf_logging.py:116] Calling model_fn.
Traceback (most recent call last):
  File "keras_td_test.py", line 101, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 126, in run
    _sys.exit(main(argv))
  File "keras_td_test.py", line 57, in main
    test_estimator.train(input_fn, max_steps=10)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 824, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 805, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 1827, in _model_fn
    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 2016, in _train_on_tpu_system
    device_assignment=ctx.device_assignment)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py", line 491, in shard
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py", line 323, in replicate
    outputs = computation(*computation_inputs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 2009, in multi_tpu_train_steps_on_single_shard
    name=b'loop')
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py", line 207, in repeat
    cond, body_wrapper, inputs=inputs, infeed_queue=infeed_queue, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py", line 169, in while_loop
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py", line 120, in body_wrapper
    outputs = body(*(inputs + dequeue_ops))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py", line 203, in body_wrapper
    return [i + 1] + _convert_to_list(body(*args))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 1076, in train_step
    self._call_model_fn(features, labels))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 1230, in _call_model_fn
    estimator_spec = self._model_fn(features=features, **kwargs)
  File "keras_td_test.py", line 89, in test_model_fn
    train_op = optimizer.minimize(loss, global_step)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 399, in minimize
    grad_loss=grad_loss)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py", line 85, in compute_gradients
    return self._opt.compute_gradients(loss, var_list=var_list, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 492, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py", line 488, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py", line 625, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py", line 379, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py", line 625, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py", line 131, in _TensorArrayWriteGrad
    grad = g.read(index)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py", line 861, in read
    return self._implementation.read(index, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py", line 260, in read
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 6419, in tensor_array_read_v3
    dtype=dtype, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3290, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1696, in __init__
    self._control_flow_post_processing()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1707, in _control_flow_post_processing
    self._control_flow_context.AddOp(self)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2429, in AddOp
    self._AddOpInternal(op)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2450, in _AddOpInternal
    real_x = self.AddValue(x)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2382, in AddValue
    real_val = grad_ctxt.grad_state.GetRealValue(val)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 1152, in GetRealValue
    history_value = cur_grad_state.AddForwardAccumulator(cur_value)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 1017, in AddForwardAccumulator
    value, self.forward_context)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 752, in GetMaxSizeFromNestedMaximumIterations
    "the tf.while_loop call ('%s')." % (value_name, while_ctxt.name))
ValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').

Working as expected on CPU:
$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --use_tpu=False
WARNING: Logging before flag parsing goes to stderr.
W0329 19:52:51.631102 140253725816576 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
main wrapper
W0329 19:52:51.829307 140253725816576 tf_logging.py:126] Estimator's model_fn (<function test_model_fn at 0x7f8f24f2c7d0>) includes params argument, but params are not passed to Estimator.
I0329 19:52:51.829844 140253725816576 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f24f2a550>, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}
I0329 19:52:51.967509 140253725816576 tf_logging.py:116] Calling model_fn.
I0329 19:52:51.967808 140253725816576 tf_logging.py:116] Running train on CPU
I0329 19:52:52.141927 140253725816576 tf_logging.py:116] Done calling model_fn.
I0329 19:52:52.143158 140253725816576 tf_logging.py:116] Create CheckpointSaverHook.
I0329 19:52:53.315506 140253725816576 tf_logging.py:116] Graph was finalized.
2018-03-29 19:52:53.315908: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
I0329 19:52:53.452600 140253725816576 tf_logging.py:116] Running local_init_op.
I0329 19:52:53.456309 140253725816576 tf_logging.py:116] Done running local_init_op.
I0329 19:52:55.594362 140253725816576 tf_logging.py:116] Saving checkpoints for 1 into gs://my_bucket/keras_td_test/model.ckpt.
I0329 19:52:58.770798 140253725816576 tf_logging.py:116] loss = 0.62052673, step = 0
I0329 19:52:59.185467 140253725816576 tf_logging.py:116] Saving checkpoints for 10 into gs://my_bucket/keras_td_test/model.ckpt.
I0329 19:53:03.196044 140253725816576 tf_logging.py:116] Loss for final step: 0.5509924.