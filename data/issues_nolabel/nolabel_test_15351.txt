questions about shared variables between CPU and GPU

Dear developers:
I looked at cifar10_multi_gpu_train.py, the idea about sharing model params among CPU and GPUs is inspiring. However, I have a few questions that I want to understand well before I can apply to my own problem.
As far as I can tell, the model params are stored in CPU by looking into the tower_loss() function since cifar10.py explicitly pinned down all variables at "/cpu:0". Then function train() wraps up tower_loss() with gpu device like this:
for i in xrange(FLAGS.num_gpus):
with tf.device('/gpu:%d' % i):
loss = tower_loss(scope)
tf.get_variable_scope().reuse_variables()
Using this way, I bet model params are stored in CPU and there is no extra copy anywhere because it is set to just reuse the same variables in the scope, while GPU stored gradient operations written in tower_loss(). In the way, I believe the model params have to transfer from CPU to GPU whenever GPU calls for these params to operate upon. It would be inefficient if doing multiple transfer to GPU I believe. I notice "identity" operation in the end of tower_loss(). Is "tf.identity(total_loss)" doing the trick so CPU transfers model params to the GPU only once, then GPU just holds the local copy from then on?