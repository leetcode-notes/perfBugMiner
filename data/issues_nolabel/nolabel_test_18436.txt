Add GPU support for float16 batched matmul

Uses cublasGemmBatchedEx introduced in CUDA 9.1.
Includes support for Tensor Op math.
Falls back to a loop over non-batched gemm calls on older CUDA
versions or GPU architectures.

Note that //tensorflow/python/kernel_tests:batch_matmul_op_test previously passed only because it does not specify force_gpu=True and falls back to the CPU.
Notifying @tfboyd