TF Lite C++ standalone Interpreter

Feature request
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.4
Python version: 2.7.12
Bazel version (if compiling from source): 0.7.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: None
GPU model and memory: None
Exact command to reproduce: None

I deploy TensorFlow models to environments with very limited memory, and was very excited to see how tiny the TF Lite kernel is (several hundred kilobytes). I know the project is young, but I was wondering if there are any plans to support a standalone C++ build for the Lite architecture, for deployment to (Linux) environments that do not have a TensorFlow runtime.
For my purposes, this package should support the following:

Loading a model configuration
Loading input tensors
Invoking the model with input tensors
Storing the result in output tensors
Some serialization method for the input- and output tensors

I would like to know:

Is this a possible feature for TF Lite?
What would the scope be of implementing this?
Are there any plans to do so?