failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Linux 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux


TensorFlow version (use command below):

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
v1.0.0-65-g4763edf-dirty 1.0.1



CUDA/cuDNN version:  8.0


GPU model and memory:


name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.2785
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 145.50MiB

Describe the problem
If I change the order of device usage, it would report error
Source code / logs

If I use GPU first then CPU, it would be fine

with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
with tf.device('/cpu:0'):
    e = tf.constant([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], shape=[2, 9],dtype=tf.float32, name='e')
    f = tf.matmul(c,e)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess.run(f))


But if I use CPU first then GPU, it return error

with tf.device('/cpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
with tf.device('/gpu:0'):
    e = tf.constant([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], shape=[2, 9],dtype=tf.float32, name='e')
    f = tf.matmul(c,e)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess.run(f))


the error dump below

    print(sess.run(f))
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(2, 2), b.shape=(2, 9), m=2, n=9, k=2
         [[Node: MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/gpu:0"](MatMul/_1, e)]]

Caused by op 'MatMul_1', defined at:
  File "m1_n0teb00k/tensorflow_palyground.py", line 13, in <module>
    f = tf.matmul(c,e)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py", line 1765, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py", line 1454, in _mat_mul
    transpose_b=transpose_b, name=name)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py", line 763, in apply_op
    op_def=op_def)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 1226, in __init__
    self._traceback = _extract_stack()

InternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(2, 2), b.shape=(2, 9), m=2, n=9, k=2
         [[Node: MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/gpu:0"](MatMul/_1, e)]]