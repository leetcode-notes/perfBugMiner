how to call AttentionWrapper?

I am trying to write a simple seq2seq model with attention. But  it gets the following error:
attn_cell = tf.contrib.seq2seq.AttentionWrapper(
AttributeError: 'module' object has no attribute 'AttentionWrapper'

How should I call AttentionWrapper?
Here is my code:
    T=1000
    N=100
    input = tf.placeholder(tf.float32, shape=(N, T, 512), name="input_matrix")
    seq_lengths = tf.placeholder(tf.int32, shape=(N), name="input_lengths")

    cell= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])
    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell,input, parallel_iterations=32, swap_memory=True, dtype=tf.float32)

    # Attention Mechanisms. Bahdanau is additive style attention
    attn_mech = tf.contrib.seq2seq.BahdanauAttention(
        num_units = 100, # depth of query mechanism
        memory = encoder_outputs, # hidden states to attend (output of RNN)
        #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories
        normalize=False, # normalize energy term
        name='BahdanauAttention')

    cell_out= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])
        # Attention Wrapper: adds the attention mechanism to the cell

    # Attention Wrapper: adds the attention mechanism to the cell
    attn_cell = tf.contrib.seq2seq.AttentionWrapper(
        cell = cell,# Instance of RNNCell
        attention_mechanism = attn_mech, # Instance of AttentionMechanism
        attention_size = 100, # Int, depth of attention (output) tensor
        attention_history=False, # whether to store history in final output
        name="attention_wrapper")
 
    # TrainingHelper does no sampling, only uses inputs
    helper = tf.contrib.seq2seq.TrainingHelper(
        inputs = x, # decoder inputs
        sequence_length = seq_len_dec, # decoder input length
        name = "decoder_training_helper")
 
    # Decoder setup
    decoder = tf.contrib.seq2seq.BasicDecoder(
              cell = attn_cell,
              helper = helper, # A Helper instance
              initial_state = encoder_final_state, # initial state of decoder
              output_layer = None) # instance of tf.layers.Layer, like Dense
 
    # Perform dynamic decoding with decoder object
    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)