1% GPU usage and slow training times after unknown DSO update?

Tensorflow with CUDA was working fine. Cross-entropy loss was going to zero (CIFAR-10 and a very simple CNN), but training error stayed around the same as random chance. I mention this in case it may be related to my issue.
In between runs, not 5 minutes after it was just working, I mysteriously got this error message about my video driver:
kernel version 361.42.0 does not match DSO version 367.57.0
I never updated anything, so that's strange. I update my video driver using apt-get and the Ubuntu ppa repo, then restart. I didn't check what version the video driver was before the update but I assume it was 361.42?
The error message is gone, but training is now an order of magnitude slower with ~1% GPU usage and ~5% CPU usage. nvidia-smi indicates 90% memory usage, which means the model is in memory, but what before took ~0.4 seconds per batch is now taking 4+ seconds per batch.
I tried re-installing CUDA and cuDNN from official sources but no change.
CUDA bandwidth test and deviceQuery results normal.
Ubuntu 16.04
Tensorflow 0.10.0 (built from source w/ CC 3.5 and 5.0)
nvidia Quadro K2200
CUDA 8.0
cuDNN 5.1.5
Python 2.7