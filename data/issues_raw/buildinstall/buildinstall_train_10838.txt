Compiling TensorFlow gives 3k lines of warnings!

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.2 (4.8 kernel)
TensorFlow installed from (source or binary): Compiling TF from Source
TensorFlow version (use command below): 1.2.0
Bazel version (if compiling from source): 0.5.1
CUDA/cuDNN version: CUDA 8.0, CuDNN 5.1
GPU model and memory: 1050Ti 4GB (Notebook version) (Intel i5-7300hq CPU)
Exact command to reproduce: Compiling from source following documentation.

Describe the problem
I compiled tensorflow from source. The process finished successfully and the binary managed to install and run successfully. What seems strange is that during the process I got ~3k lines of warnings. I am linking to them at the end of the issue. I am wondering if that's expected behavior or indication of a (small or not_so_small?) problem.
One thing that may affect this is bazel installation. I followed Bazel Installation Instructions and used the recommended apt method. This led to me running into this issue. Installing openjdk-8-jdk on top of the ibm-java80-jdk as suggested in a comment solves the problem (although I am not sure how much technical debt this solution caries, which may have manifested in some of the warnings produced during compilation).
Source code / logs
Configuration Script options:
$ ./configure
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3
Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]
/usr/local/lib/python3.5/dist-packages
Do you wish to build TensorFlow with MKL support? [y/N] y
MKL support will be enabled for TensorFlow
Do you wish to download MKL LIB from the web? [Y/n] y
Please specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? [Y/n] y
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N] n
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Do you want to use clang as CUDA compiler? [y/N] n
nvcc will be used as CUDA compiler
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: "3.5,5.2"]: 6.1
........
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Configuration finished

Console output of compilation command
Because the output is too big to be placed within the issue I 've put it in it's own repository.