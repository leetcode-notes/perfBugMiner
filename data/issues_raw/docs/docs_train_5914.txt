No documentation for sending input parameters of request to TensorFlow Serving

I am running the sample iris program in TensorFlow Serving. Since it is a TF.Learn model, I am exporting the model using the following classifier.export(export_dir=model_dir,signature_fn=my_classification_signature_fn)
and the signature_fn is defined as shown below:
def my_classification_signature_fn(examples, unused_features, predictions):
  """Creates classification signature from given examples and predictions.
  Args:
    examples: `Tensor`.
    unused_features: `dict` of `Tensor`s.
    predictions: `Tensor` or dict of tensors that contains the classes tensor
      as in {'classes': `Tensor`}.
  Returns:
    Tuple of default classification signature and empty named signatures.
  Raises:
    ValueError: If examples is `None`.
  """
  if examples is None:
    raise ValueError('examples cannot be None when using this signature fn.')

  if isinstance(predictions, dict):
    default_signature = exporter.classification_signature(
        examples, classes_tensor=predictions['classes'])

  else:

    default_signature = exporter.classification_signature(
        examples, classes_tensor=predictions)
  named_graph_signatures={
        'inputs': exporter.generic_signature({'x_values': examples}),
        'outputs': exporter.generic_signature({'preds': predictions})}    
  return default_signature, named_graph_signatures

The model gets successfully exported using the following piece of code.
I have created a client which makes real-time predictions using TensorFlow Serving.
The following is the code for the client:
flags.DEFINE_string("model_dir", "/tmp/iris_model_dir", "Base directory for output models.")
tf.app.flags.DEFINE_integer('concurrency', 1,
                            'maximum number of concurrent inference requests')
tf.app.flags.DEFINE_string('server', '', 'PredictionService host:port')

#connection
host, port = FLAGS.server.split(':')
channel = implementations.insecure_channel(host, int(port))
stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)


# Classify two new flower samples.
new_samples = np.array([5.8, 3.1, 5.0, 1.7], dtype=float)

request = predict_pb2.PredictRequest()
request.model_spec.name = 'iris'

request.inputs["x_values"].CopyFrom(
        tf.contrib.util.make_tensor_proto(new_samples))

result = stub.Predict(request, 10.0)  # 10 secs timeout

However, on making the predictions, the following error is displayed:
grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details="Output 0 of type double does not match declared output type string for node _recv_input_example_tensor_0 = _Recv[client_terminated=true, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=2016246895612781641, tensor_name="input_example_tensor:0", tensor_type=DT_STRING, _device="/job:localhost/replica:0/task:0/cpu:0"]()")
Here is the entire stack trace.

The iris model is defined in the following manner:
# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                            hidden_units=[10, 20, 10],
                                            n_classes=3, model_dir=model_dir)

# Fit model.
classifier.fit(x=training_set.data, 
               y=training_set.target, 
               steps=2000)

Can someone provide some documentation for creating the client program which sends the request to TensorFlow Serving.