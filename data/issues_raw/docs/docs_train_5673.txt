Possible typo in docstring of embedding_rnn_seq2seq and embedding_attention_seq2seq?

So in the docstring of embedding_rnn_seq2seq:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296
It says:
  This model first embeds encoder_inputs by a newly created embedding (of shape
  [num_encoder_symbols x input_size]). Then it runs an RNN to encode

Should that instead read:
  [embedding_size x input_size]). Then it runs an RNN to encode

Since if you are creating an embedding it is to do dimensionality reduction (and so you usually want your embedding size to be smaller than the number of encoder symbols). Also in the code:
    # Encoder.
    encoder_cell = rnn_cell.EmbeddingWrapper(
        cell, embedding_classes=num_encoder_symbols,
        embedding_size=embedding_size)
    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)

It looks like the encoder inputs are mapped to vectors of size embedding_size.
Looking forward to hearing the community thoughts (this is also the case in the docstring of embedding_attention_seq2seq).