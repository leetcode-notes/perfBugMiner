Tutorial request for hybrid model (word+character)

The implementation done in the paper:
http://aclweb.org/anthology/P/P16/P16-1100.pdf
is a hybrid seq2seq model with advancements where the encoder is fed with inputs based on following two cases:
1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary
2.Output of another LSTM network - when the word is out of vocabulary and a separate character based LSTM is used to generate an embedding on the fly
Consider the following example sentence:
"The brown fox jumped over the lazy dog"
Assume these are the words present in the vocabulary: The, brown, jumped, over, dog - These words are fed to the seq2seq encoder as such
out of vocabulary(OOV) words are: fox, lazy - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words
These both word level and character level encoder needs to be trained end to end simultaneously.
Since the implementation is a bit different from the normal seq2seq can a tutorial or example of such case be added to the examples section?