Multi gpu cifa10 example, putting ops outside of towerloss to cpu actually hurt performance

From cifa 10 example tutorial  in train.py  "with tf.Graph().as_default(), tf.device('/cpu:0'):" putting all the ops but in tower loss to cpu.
as in the tutorial cifa 10 tutorial
"This setup requires that all GPUs share the model parameters. A well-known fact is that transferring data to and from GPUs is quite slow. For this reason, we decide to store and update all model parameters on the CPU (see green box). A fresh set of model parameters is transferred to the GPU when a new batch of data is processed by all GPUs."
However I find in my rnn application(tower loss 4GPU), without setting  tf.device('/cpu:0'): actually is faster.
Setting   tf.device('/cpu:0'):
about 44s per 100 step, take 9min43s to finish the first 1000 steps
2016-10-10 17:37:53 epoch:0.1664 train_step:100 duration:0.474 elapsed:56.103 train_avg_metrics:['loss:0.496']  ['loss:0.486']
2016-10-10 17:38:38 epoch:0.3328 train_step:200 duration:0.520 elapsed:45.064 train_avg_metrics:['loss:0.464']  ['loss:0.448']
2016-10-10 17:39:25 epoch:0.4992 train_step:300 duration:0.408 elapsed:46.612 train_avg_metrics:['loss:0.383']  ['loss:0.238']
2016-10-10 17:40:10 epoch:0.6656 train_step:400 duration:0.471 elapsed:44.819 train_avg_metrics:['loss:0.308']  ['loss:0.317']
2016-10-10 17:40:54 epoch:0.8319 train_step:500 duration:0.431 elapsed:44.716 train_avg_metrics:['loss:0.269']  ['loss:0.245']
2016-10-10 17:41:39 epoch:0.9983 train_step:600 duration:0.418 elapsed:44.256 train_avg_metrics:['loss:0.242']  ['loss:0.234']
2016-10-10 17:42:35 epoch:1.1647 train_step:700 duration:0.424 elapsed:56.733 train_avg_metrics:['loss:0.214']  ['loss:0.168']
2016-10-10 17:43:19 epoch:1.3311 train_step:800 duration:0.402 elapsed:43.769 train_avg_metrics:['loss:0.209']  ['loss:0.213']
2016-10-10 17:44:01 epoch:1.4975 train_step:900 duration:0.475 elapsed:41.747 train_avg_metrics:['loss:0.201']  ['loss:0.193']
2016-10-10 17:44:43 epoch:1.6639 train_step:1000 duration:0.423 elapsed:42.351 train_avg_metrics:['loss:0.190']  ['loss:0.238']
2016-10-10 17:44:43 0:08:14 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.298']
Not setting tf.device('/cpu:0'):
about 35s per 100 step, take 6min48s to finsish the first 1000 steps
2016-10-11 04:24:43 epoch:0.1664 train_step:100 duration:0.371 elapsed:47.976 train_avg_metrics:['loss:0.496']  ['loss:0.485']
2016-10-11 04:25:19 epoch:0.3328 train_step:200 duration:0.385 elapsed:36.630 train_avg_metrics:['loss:0.470']  ['loss:0.408']
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1680058 get requests, put_count=1680141 evicted_count=3000 eviction_rate=0.00178556 and unsatisfied allocation rate=0.00195172
2016-10-11 04:25:56 epoch:0.4992 train_step:300 duration:0.319 elapsed:37.185 train_avg_metrics:['loss:0.386']  ['loss:0.336']
2016-10-11 04:26:32 epoch:0.6656 train_step:400 duration:0.411 elapsed:35.670 train_avg_metrics:['loss:0.320']  ['loss:0.303']
2016-10-11 04:27:07 epoch:0.8319 train_step:500 duration:0.360 elapsed:35.204 train_avg_metrics:['loss:0.268']  ['loss:0.272']
2016-10-11 04:27:43 epoch:0.9983 train_step:600 duration:0.333 elapsed:35.526 train_avg_metrics:['loss:0.248']  ['loss:0.167']
2016-10-11 04:28:31 epoch:1.1647 train_step:700 duration:0.378 elapsed:48.393 train_avg_metrics:['loss:0.218']  ['loss:0.170']
2016-10-11 04:29:06 epoch:1.3311 train_step:800 duration:0.334 elapsed:34.472 train_avg_metrics:['loss:0.208']  ['loss:0.264']
2016-10-11 04:29:38 epoch:1.4975 train_step:900 duration:0.322 elapsed:32.613 train_avg_metrics:['loss:0.199']  ['loss:0.198']
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 7824854 get requests, put_count=7824942 evicted_count=13000 eviction_rate=0.00166135 and unsatisfied allocation rate=0.00169639
2016-10-11 04:30:11 epoch:1.6639 train_step:1000 duration:0.320 elapsed:32.889 train_avg_metrics:['loss:0.199']  ['loss:0.219']
2016-10-11 04:30:11 0:06:48 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.301']