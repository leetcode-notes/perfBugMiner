Mention that GPU reductions are nondeterministic in docs

The problem
I am trying out the MNIST for experts tutorial and I have inconsistent results on the GPU.
What do I mean by inconsistent?
With the exactly same network parameters (and randomness removed: read below in the post) every time I run the complete train-then-test process the accuracy is slightly different.
What have I done to visualize this problem?
For each iteration, I have calculated the differences between the variables (weights, biases) from two independent but identical runs and computed the L1 norm of those differences - 

plot of L1 norm for the first 1000 iterations in steps of 20.

In a consistent world, these differences should be always zero!
How did I remove randomness in the code?

Removed dropout entirely
added a graph level seed (tf.set_random_seed(1234)). With this the variable initialization is deterministic and also any other randomization in the code.
The MNIST for experts tutorial uses this script to download/load the MNIST data. I have added numpy.random.seed(3) in DataSet.__init__(self, images, labels, fake_data=False, one_hot=False, dtype=dtypes.float32) in this script to remove randomness during the shuffling process (line 154 in DataSet.next_batch(self, batch_size, fake_data=False))
config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1) which goes into the creation of session as sess = tf.Session(config=config)

What system am I using?

tensorflow 0.8 gpu version (installed via pip)
OpenSUSE LEAP 42.1 (x86_64)
Cuda Toolkit 7.5
CuDNN 4.0
Tesla K20c card with Nvidia driver 352.79