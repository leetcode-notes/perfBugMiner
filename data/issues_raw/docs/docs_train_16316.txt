Lack of clarity in tf.while_loop documentation

I believe that the documentation for tf.while_loop is lacking usage clarity, and actually provides contradictory statements.
Specifically, it seems that many people are using the tf.while_loop as a "for loop" (see stackoverflow). However, the tf.while_loop docs state:

For correct programs, while_loop should return the same result for any parallel_iterations > 0.

A loop counter inside of the "while loop" body, seems to violate this constraint despite the fact that this is given as an example usage in the docs:

python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])

So it seems that there are two bad outcomes here:


If this is indeed the canonical way of creating a "for loop", then the example explicitly creates a dependency between iterations, meaning that the "while loop" iterations cannot be run in parallel.


The example is incorrect?


It seems like the while_loop docs should have an example which better illustrates how to use it as a "for loop", if such usage is indeed intended, or a warning on the implications of the provided example.