About the implementation of attention seq2seq decoder function

I am very pleased to hear the news that the attention decoder function is added recently to the master branch.
However, the documentation of the function does not match with the actual implementation now.

Clarification on 'luong' option
Inspecting the code briefly, I guess the paper that the 'luong' option referencing is the following paper:
http://www.aclweb.org/anthology/D15-1166.


When we do not use the 'input feeding' approach, then the attention decoder is unneeded, since attention vectors depend only on the current hidden states. Thus, we can use the simple decoder and compute attention vectors afterwards.
When we use 'input feeding' approach, a computed attention vector should be fed to the next time-step's input. This is done by the added attention decoder with 'luong' option.
I think the above information should be stated to avoid confusion.


A possible bug on 'bahdanau' option
When we use 'bahdanau' option, _init_attention function just creates a zero vector as an attention vector. However, according to the referencing paper (https://arxiv.org/pdf/1409.0473.pdf) an attention vector should be taken into consideration (i.e. non-zero) when computing the first hidden state. Thus, I think _init_attention should output an attention vector if attention option is designated as 'bahdanau'.

Please tell me if I understood wrong. Thanks!