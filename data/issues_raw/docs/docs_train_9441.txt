Non deterministic behaviour of tf.train.batch in case the number of threads is higher than 1.

This is related to the StackOverflow question: http://stackoverflow.com/questions/43612366/tf-train-batch-output-is-not-deterministic/43613376#43613376
The thread owner creates a batch with the following code:
BatchedInputs = tf.train.batch(
  Inputs,
  batch_size=64,
  num_threads=8,
  capacity=500 + 3 * 64)

And he noticed that created batches are not in every run the same. They are quite similar, but sometimes the inputs are mixed or some are missing.
According to the answer on StackOverflow reducing the number of pre-fetch threads to 1 is solving this issue.
Since this could be an issue for test-set evaluation (where everyone would expect the exact same outcome for every run), I wonder if this is the intended behaviour?
It should at least be added to the documentation for tf.train.batch that the generated batches can be non-deterministic.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
TensorFlow installed from (source or binary):
Binary (pip3)
TensorFlow version (use command below):
1.1.0
Bazel version (if compiling from source):
non
CUDA/cuDNN version:
CUDA 8.0, cuDNN 5.1
GPU model and memory:
GTX680
Exact command to reproduce:
See above.

Describe the problem
see above
Source code / logs