tf.losses.softmax_cross_entropy is deviously inconsistent

There are at least three variants of softmax_cross_entropy in TensorFlow:

tf.nn.softmax_cross_entropy_with_logits

Accepts logits as the first argument, and labels as the second argument.

tf.contrib.losses.softmax_cross_entropy

Accepts logits as the first argument, and onehot_labels as the second argument. So far so good. Except this is deprecated, and displays the recommendation Use tf.losses.softmax_cross_entropy instead.

tf.losses.softmax_cross_entropy

Decides to switch things around for fun and have onehot_labels as the first argument and  logits as the second. Since onehot_labels and logits are identically shaped tensors, the call succeeds without any complaints (until something else fails as a consequence, like the gradient).
This inconsistency seems a bit error prone. If nothing else, perhaps the deprecation warning for  tf.contrib.losses.softmax_cross_entropy should include a heads up.