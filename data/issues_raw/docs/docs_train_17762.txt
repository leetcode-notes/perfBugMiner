UnimplementedError: Broadcast between <Tensor> and <Tensor> is not supported yet.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 Pro 64-bit (10.0, Build 16299)
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
b'unknown' 1.5.0
Python version:
3.6.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Describe the problem
I get 'UnimplementedError (see above for traceback): Broadcast between  and  is not supported yet' when trying to broadcast between different kinds of tensors.
What I take from this is that the broadcasting system in tensorflow has limits on how many dimensions that can be broadcasted. After having tried multiple tensor permutations, I have further concluded that  the constraints on broadcasting for tensors is a bit unintuitive. For example:
This works:
[1,2,1,1,2,2,2,2]*
[2,2,2,2,1,2,2,2]
This doesn't work:
[1,2,1,1,2,2,2,2]*
[2,2,2,2,1,2,2,1]
This works:
[2,2,2,2,1]*
[1,2,1,2,2]
This doesn't work:
[2,2,2,2,1,2]*
[1,2,1,2,2,2]
What I take from this is that broadcasting works for any permutation when the tensor rank is less than 6. When it is above 6, broadcasting only works when there is a maximum of 2 regions of consecutive 1's (both tensors' 1's taken into consideration). For example:
This has two regions of consecutive 1's:
[1,2,1,1,2,2,2,2]*
[2,2,2,2,1,2,2,2]
This has three regions of consecutive 1's:
[1,2,1,1,2,2,2,2]*
[2,2,2,2,2,1,2,2]
Although, this rule seems to be wrong when calculating the product below. This doesn't work:
[1,2,1,1,2,1,2,2]*
[2,2,2,2,1,2,2,2]
Maybe there is also a constraint on each individual tensor that it cannot contain more than two regions of consecutive 1's. An example of this:
This works:
[1,2,1,1,2,2,2,2]*
[2,1,2,2,1,2,2,2]
Nevertheless, I hope that I have demonstrated that the underlying constraints for broadcasting are confusing and unintuitive, so could you please show me the current constraints on broadcasting or confirm that above constraints are the only ones?
Configurations aside, is it possible to fix the broadcasting system such that it can support any broadcasting configuration? If not, is there at least a way to increase the number of regions of consecutive 1's in the constraints for broadcastability between two tensors? I.e. is there a way to increase the maximum of 2 regions of consecutive 1's to a maximum of e.g. 4 regions?
I understand that a workaround is to tile the tensors, but I assume that this increases memory usage (by quite a lot when the tensors' ranks are high); something that is very limited in my current program.
Source code / logs
An example of case 1 and 2 above:
import tensorflow as tf
t1 = tf.random_normal([1,2,1,1,2,2,2,2])
t2 = tf.random_normal([2,2,2,2,1,2,2,2])
t3 = d*e

t4 = tf.random_normal([1,2,1,1,2,2,2,2])
t5 = tf.random_normal([2,2,2,2,1,2,2,1])
t6 = d*e

sess = tf.InteractiveSession()
sess.run(t3)
sess.run(t6)