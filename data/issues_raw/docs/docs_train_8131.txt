Better document when to use tf.sparse_tensor_dense_matmul vs embedding lookup

tf.sparse_tensor_dense_matmul has a regular (non-sparse) tensor gradient (source):
  # gradient w.r.t. dense
  b_grad = sparse_ops.sparse_tensor_dense_matmul(sp_t, grad,
                                                 adjoint_a=not adj_a)
where sparse_ops.sparse_tensor_dense_matmul returns a regular tensor.
The documentation does not note this and it may be confusing for people coming from e.g. NLP where you frequently work with dense-sparse multiplications with sparse gradients.
In many cases where you might naively think you want to use tf.sparse_tensor_dense_matmul you actually want to use tf.nn.embedding_lookup_sparse, even if your task has nothing to do with embeddings.  It would be helpful if there were a cross-reference in the documentation to guide users in the right direction.