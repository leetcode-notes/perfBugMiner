GPU memory increases in multiples of batch_size 64 with allow_growth=True

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): ('v1.4.0-14-gb5df90f', '1.4.1')
Python version: 2.7.12
Bazel version (if compiling from source): 0.7.0
GCC/Compiler version (if compiling from source): 5.4.0-6
CUDA/cuDNN version: V9.0.176
GPU model and memory: V100 AWS P3 8X large instance
** Exact command to reproduce**: Inception V3 training as mentioned in models/research/slim. python train_image_classifier.py

Describe the problem
GPU memory is increasing in multiples of batch_size of 64. Using 8.9 GB of GPU memory for batch_size=16 or batch_size=32 or batch_size=64. And using 15.6 GB of GPU memory for batch_size=96 and batch_size=128. I'd like to use batch_size=96 so allocator won't throw warnings during global_step as it will have 2-3 GB unused.
Is this a feature? Can we change its functioning?