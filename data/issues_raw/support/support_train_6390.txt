Extract attention matrix during decoding

It seems like it's not possible to access the attention weights during decoding since the attention() method in seq2seq_model isn't called during the forward step. Otherwise, is there another feature to allow for this, or was this intended?