Tf.gradients returning all zeroes when called on result of a tf.gradient call

I have the following code snippet:
interpolates = alpha*real_data + ((1-alpha)*fake_data)
disc_interpolates = Discriminator(interpolates)
gradients = tf.gradients(disc_interpolates, [interpolates])[0]
second_grad = tf.gradients(gradients[0], [interpolates])[0]
Where Discriminator corresponds to a neural network. The first call to tf.gradients is working correctly and I get back non-zero slope values in the gradients variable. However whenever I try to find the second derivative by applying tf.gradients to the gradients variable, my result is always a vector of zeroes.
Is this expected behavior or should I be able to find the second derivatives of my neural net?