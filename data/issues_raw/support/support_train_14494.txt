Difference between distribute and local machine version when training by tensorflow

I use a common DNN network with less than 4 layers for a regression task. When I test my codes in local single machine with CPU, everything is fine. But when I train it on distribute system, which uses 30 machines' CPU and in the asynchronous parameters update way, all the layers' output values, including the last prediction values, come into Nan. The learning rate in local and distribute training are both 0.01. I changed it to 0.0001 in distribute way, it seems fine that the outputs are in normal range. I am confused about why learning rate cause this phenomenon. Any friend can try to explain it ? Thanks.
--
The label is a float, ranging from 0 to 1. Most of them falls into 0.1 to 0.3