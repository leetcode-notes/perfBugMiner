A suggestion for controlled dependency in batch normalization layer

In documents of tf.contrib.layers.batch_norm, it is suggested to bind update op of moving_mean and moving_variance with loss function, as follows:
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) 
if update_ops: 
    updates = tf.group(*update_ops)
    total_loss = control_flow_ops.with_dependencies([updates], total_loss)

Is it better to bind update_op with train_op? Say, maybe one want to evaluate loss but do not update parameters?