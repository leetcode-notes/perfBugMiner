Recurrent Spatial Transformer Network

i'm trying to replicate Recurrent Spatial Transformer Network implemented here (https://github.com/skaae/recurrent-spatial-transformer-code) , however the loss didn't decrease at all .
the configuration of the network is as follow:
1 -  relu activations .
2 - xavier weight initialization for weights , zero initialization for biases .
3 - cost function is softmax_cross_entropy_with_logits .
4 - optimizer is RMSProp (i tried 1e-6 ;1e-10 espilon) .
5 - gradient clipping by value .
so what should i try next ?