Distributed Tensorflow: data feeding in the beginning

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.1.31
Python version: 2.7
Bazel version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce:

Describe the problem
I am using distributed tensorflow. I have a huge parameter matrix partitioned across multiple parameter servers. In the beginning of training, only some workers will have the following exception. If the failed workers are restarted (possibly need multiple restarts), then the workers can go through and continue the training.
I have checked the hdfs paths and files. They all look good. It seems the problem is that the failed workers cannot get the parameters from some ps (in the last line, see embedding_lookup/DynamicPartition_S751)
Source code / logs
Exception thrown:
Caused by op u'ReaderReadV2', defined at:
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py", line 174, in _run_module_as_main
"main", fname, loader, pkg_name)
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py", line 72, in _run_code
exec code in run_globals
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py", line 180, in 
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py", line 157, in manager
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py", line 61, in worker
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py", line 174, in main
process()
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py", line 169, in process
serializer.dunits processed is zero
ump_stream(func(split_index, iterator), outfile)
File "/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py", line 2408, in pipeline_func
File "/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py", line 2408, in pipeline_func
File "/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py", line 2408, in pipeline_func
File "/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py", line 345, in func
File "/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py", line 793, in func
File "/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/tfspark.zip/tensorflowonspark/TFSparkNode.py", line 240, in mapfn
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyfiles/my_distributed_learning.py", line 105, in map_fun
((x, y), units_completed, records_prodcued, sg_key) = read_csv_examples(skipgrams, None, batch_size, num_epochs, task_index, num_workers)
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyfiles/my_distributed_learning.py", line 56, in read_csv_examples
sg_key, sg_csv = sg_reader.read(sg_queue)
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py", line 193, in read
return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py", line 422, in _reader_read_v2
queue_handle=queue_handle, name=name)
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 767, in apply_op
op_def=op_def)
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2359, in create_op
original_op=self._default_original_op, op_def=op_def)
File "/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1242, in init
self._traceback = _extract_stack()
OutOfRangeError (see above for traceback): FIFOQueue '_1_sg_queue' is closed and has insufficient elements (requested 1, current size 0)
[[Node: ReaderReadV2 = ReaderReadV2[_device="/job:worker/replica:0/task:1/cpu:0"](sg_reader, sg_queue)]]
[[Node: embedding_lookup/DynamicPartition_S751 = _Recv client_terminated=false, recv_device="/job:ps/replica:0/task:6/cpu:0", send_device="/job:worker/replica:0/task:1/cpu:0", send_device_incarnation=6148508285306453080, tensor_name="edge_726_embedding_lookup/DynamicPartition", tensor_type=DT_INT32, _device="/job:ps/replica:0/task:6/cpu:0"]]