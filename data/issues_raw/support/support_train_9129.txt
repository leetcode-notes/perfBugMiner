tensorflow1.1 rnn lstm:ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights....

Environment info
Operating System: Ubuntu 14.04.5 LTS
Installed version of CUDA and cuDNN:
No CUDA, I use CPU-only.
Pip version: pip 1.5.4
Python version: 2.7.6
Operating System: Ubuntu 14.04.5 LTS
Tensorflow version: tensorflow-1.1.0rc0-cp27-none-linux_x86_64 , CPU-only
Description:
I was testing the tutorial example of LSTM .
my main function  train_rnn_classify.py:
import tensorflow as tf
import numpy as np
import os
import time
import datetime
from rnn_model import RNN_Model
import data_helper


flags =tf.app.flags
FLAGS = flags.FLAGS


flags.DEFINE_integer('batch_size',64,'the batch_size of the training procedure')
flags.DEFINE_float('lr',0.1,'the learning rate')
flags.DEFINE_float('lr_decay',0.6,'the learning rate decay')
flags.DEFINE_integer('vocabulary_size',20000,'vocabulary_size')
flags.DEFINE_integer('emdedding_dim',128,'embedding dim')
flags.DEFINE_integer('hidden_neural_size',128,'LSTM hidden neural size')
flags.DEFINE_integer('hidden_layer_num',1,'LSTM hidden layer num')
flags.DEFINE_string('dataset_path','/home/hadoop/lstm/subj0.pkl','dataset path')
flags.DEFINE_integer('max_len',40,'max_len of training sentence')
flags.DEFINE_integer('valid_num',100,'epoch num of validation')
flags.DEFINE_integer('checkpoint_num',1000,'epoch num of checkpoint')
flags.DEFINE_float('init_scale',0.1,'init scale')
flags.DEFINE_integer('class_num',2,'class num')
flags.DEFINE_float('keep_prob',0.5,'dropout rate')
flags.DEFINE_integer('num_epoch',60,'num epoch')
flags.DEFINE_integer('max_decay_epoch',30,'num epoch')
flags.DEFINE_integer('max_grad_norm',5,'max_grad_norm')
flags.DEFINE_string('out_dir',os.path.abspath(os.path.join(os.path.curdir,"runs")),'output directory')
flags.DEFINE_integer('check_point_every',10,'checkpoint every num epoch ')

class Config(object):

    hidden_neural_size=FLAGS.hidden_neural_size
    vocabulary_size=FLAGS.vocabulary_size
    embed_dim=FLAGS.emdedding_dim
    hidden_layer_num=FLAGS.hidden_layer_num
    class_num=FLAGS.class_num
    keep_prob=FLAGS.keep_prob
    lr = FLAGS.lr
    lr_decay = FLAGS.lr_decay
    batch_size=FLAGS.batch_size
    num_step = FLAGS.max_len
    max_grad_norm=FLAGS.max_grad_norm
    num_epoch = FLAGS.num_epoch
    max_decay_epoch = FLAGS.max_decay_epoch
    valid_num=FLAGS.valid_num
    out_dir=FLAGS.out_dir
    checkpoint_every = FLAGS.check_point_every


def evaluate(model,session,data,global_steps=None,summary_writer=None):


    correct_num=0
    total_num=len(data[0])
    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):

         fetches = model.correct_num
         feed_dict={}
         feed_dict[model.input_data]=x
         feed_dict[model.target]=y
         feed_dict[model.mask_x]=mask_x
         model.assign_new_batch_size(session,len(x))
         state = session.run(model._initial_state)
         for i , (c,h) in enumerate(model._initial_state):
            feed_dict[c]=state[i].c
            feed_dict[h]=state[i].h
         count=session.run(fetches,feed_dict)
         correct_num+=count

    accuracy=float(correct_num)/total_num
    dev_summary = tf.scalar_summary('dev_accuracy',accuracy)
    dev_summary = session.run(dev_summary)
    if summary_writer:
        summary_writer.add_summary(dev_summary,global_steps)
        summary_writer.flush()
    return accuracy

def run_epoch(model,session,data,global_steps,valid_model,valid_data,train_summary_writer,valid_summary_writer=None):
    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):

        feed_dict={}
        feed_dict[model.input_data]=x
        feed_dict[model.target]=y
        feed_dict[model.mask_x]=mask_x
        model.assign_new_batch_size(session,len(x))
        fetches = [model.cost,model.accuracy,model.train_op,model.summary]
        state = session.run(model._initial_state)
        for i , (c,h) in enumerate(model._initial_state):
            feed_dict[c]=state[i].c
            feed_dict[h]=state[i].h
        cost,accuracy,_,summary = session.run(fetches,feed_dict)
        train_summary_writer.add_summary(summary,global_steps)
        train_summary_writer.flush()
        valid_accuracy=evaluate(valid_model,session,valid_data,global_steps,valid_summary_writer)
        if(global_steps%100==0):
            print("the %i step, train cost is: %f and the train accuracy is %f and the valid accuracy is %f"%(global_steps,cost,accuracy,valid_accuracy))
        global_steps+=1

    return global_steps





def train_step():

    print("loading the dataset...")
    config = Config()
    eval_config=Config()
    eval_config.keep_prob=1.0

    train_data,valid_data,test_data=data_helper.load_data(FLAGS.max_len,batch_size=config.batch_size)

    print("begin training")

    # gpu_config=tf.ConfigProto()
    # gpu_config.gpu_options.allow_growth=True
    with tf.Graph().as_default(), tf.Session() as session:
        initializer = tf.random_uniform_initializer(-1*FLAGS.init_scale,1*FLAGS.init_scale)
        with tf.variable_scope("model",reuse=None,initializer=initializer):
            model = RNN_Model(config=config,is_training=True)

        with tf.variable_scope("model",reuse=True,initializer=initializer):
            valid_model = RNN_Model(config=eval_config,is_training=False)
            test_model = RNN_Model(config=eval_config,is_training=False)

        #add summary
        # train_summary_op = tf.merge_summary([model.loss_summary,model.accuracy])
        train_summary_dir = os.path.join(config.out_dir,"summaries","train")
        train_summary_writer =  tf.train.SummaryWriter(train_summary_dir,session.graph)

        # dev_summary_op = tf.merge_summary([valid_model.loss_summary,valid_model.accuracy])
        dev_summary_dir = os.path.join(eval_config.out_dir,"summaries","dev")
        dev_summary_writer =  tf.train.SummaryWriter(dev_summary_dir,session.graph)

        #add checkpoint
        checkpoint_dir = os.path.abspath(os.path.join(config.out_dir, "checkpoints"))
        checkpoint_prefix = os.path.join(checkpoint_dir, "model")
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)
        saver = tf.train.Saver(tf.all_variables())


        tf.initialize_all_variables().run()
        global_steps=1
        begin_time=int(time.time())

        for i in range(config.num_epoch):
            print("the %d epoch training..."%(i+1))
            lr_decay = config.lr_decay ** max(i-config.max_decay_epoch,0.0)
            model.assign_new_lr(session,config.lr*lr_decay)
            global_steps=run_epoch(model,session,train_data,global_steps,valid_model,valid_data,train_summary_writer,dev_summary_writer)

            if i% config.checkpoint_every==0:
                path = saver.save(session,checkpoint_prefix,global_steps)
                print("Saved model chechpoint to{}\n".format(path))

        print("the train is finished")
        end_time=int(time.time())
        print("training takes %d seconds already\n"%(end_time-begin_time))
        test_accuracy=evaluate(test_model,session,test_data)
        print("the test data accuracy is %f"%test_accuracy)
        print("program end!")



def main(_):
    train_step()


if __name__ == "__main__":
    tf.app.run()

model code rnn_model.py   :
`
import tensorflow as tf

import numpy as np



class RNN_Model(object):







    def __init__(self,config,is_training=True):



        self.keep_prob=config.keep_prob

        self.batch_size=tf.Variable(0,dtype=tf.int32,trainable=False)



        num_step=config.num_step

        self.input_data=tf.placeholder(tf.int32,[None,num_step])

        self.target = tf.placeholder(tf.int64,[None])

        self.mask_x = tf.placeholder(tf.float32,[num_step,None])



        class_num=config.class_num

        hidden_neural_size=config.hidden_neural_size

        vocabulary_size=config.vocabulary_size

        embed_dim=config.embed_dim

        hidden_layer_num=config.hidden_layer_num

        self.new_batch_size = tf.placeholder(tf.int32,shape=[],name="new_batch_size")

        self._batch_size_update = tf.assign(self.batch_size,self.new_batch_size)



        #build LSTM network



        lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_neural_size,forget_bias=0.0,state_is_tuple=True)

        if self.keep_prob<1:

            lstm_cell =  tf.contrib.rnn.DropoutWrapper(

                lstm_cell,output_keep_prob=self.keep_prob

            )



        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell]*hidden_layer_num,state_is_tuple=True)



        self._initial_state = cell.zero_state(self.batch_size,dtype=tf.float32)



        #embedding layer

        with tf.device("/cpu:0"),tf.name_scope("embedding_layer"):

            embedding = tf.get_variable("embedding",[vocabulary_size,embed_dim],dtype=tf.float32)

            inputs=tf.nn.embedding_lookup(embedding,self.input_data)



        if self.keep_prob<1:

            inputs = tf.nn.dropout(inputs,self.keep_prob)



        out_put=[]

        state=self._initial_state

        with tf.variable_scope("LSTM_layer"):

            for time_step in range(num_step):

                if time_step>0: tf.get_variable_scope().reuse_variables()

                (cell_output,state)=cell(inputs[:,time_step,:],state)

                out_put.append(cell_output)



        out_put=out_put*self.mask_x[:,:,None]



        with tf.name_scope("mean_pooling_layer"):



            out_put=tf.reduce_sum(out_put,0)/(tf.reduce_sum(self.mask_x,0)[:,None])



        with tf.name_scope("Softmax_layer_and_output"):

            softmax_w = tf.get_variable("softmax_w",[hidden_neural_size,class_num],dtype=tf.float32)

            softmax_b = tf.get_variable("softmax_b",[class_num],dtype=tf.float32)

            self.logits = tf.matmul(out_put,softmax_w)+softmax_b



        with tf.name_scope("loss"):

            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target,logits=self.logits+1e-10,)

            self.cost = tf.reduce_mean(self.loss)



        with tf.name_scope("accuracy"):

            self.prediction = tf.argmax(self.logits,1)

            correct_prediction = tf.equal(self.prediction,self.target)

            self.correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))

            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name="accuracy")



        #add summary

        loss_summary = tf.contrib.deprecated.scalar_summary("loss",self.cost)

        #add summary

        accuracy_summary=tf.contrib.deprecated.scalar_summary("accuracy_summary",self.accuracy)



        if not is_training:

            return



        self.globle_step = tf.Variable(0,name="globle_step",trainable=False)

        self.lr = tf.Variable(0.0,trainable=False)



        tvars = tf.trainable_variables()

        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),

                                      config.max_grad_norm)





        # Keep track of gradient values and sparsity (optional)

        grad_summaries = []

        for g, v in zip(grads, tvars):

            if g is not None:

                grad_hist_summary = tf.summary.histogram("{}/grad/hist".format(v.name), g)

                sparsity_summary = tf.contrib.deprecated.scalar_summary("{}/grad/sparsity".format(v.name), tf.nn.zero_fraction(g))

                grad_summaries.append(grad_hist_summary)

                grad_summaries.append(sparsity_summary)

        self.grad_summaries_merged = tf.summary.merge(grad_summaries)



        self.summary =tf.summary.merge([loss_summary,accuracy_summary,self.grad_summaries_merged])







        optimizer = tf.train.GradientDescentOptimizer(self.lr)

        optimizer.apply_gradients(zip(grads, tvars))

        self.train_op=optimizer.apply_gradients(zip(grads, tvars))



        self.new_lr = tf.placeholder(tf.float32,shape=[],name="new_learning_rate")

        self._lr_update = tf.assign(self.lr,self.new_lr)



    def assign_new_lr(self,session,lr_value):

        session.run(self._lr_update,feed_dict={self.new_lr:lr_value})

    def assign_new_batch_size(self,session,batch_size_value):

        session.run(self._batch_size_update,feed_dict={self.new_batch_size:batch_size_value})`

data handle code  data_helper.py:

import numpy as np

import cPickle as pkl



#file path

dataset_path='/home/hadoop/lstm/subj0.pkl'



def set_dataset_path(path):

    dataset_path=path









def load_data(max_len,batch_size,n_words=20000,valid_portion=0.1,sort_by_len=True):

    f=open(dataset_path,'rb')

    print ('load data from %s',dataset_path)

    train_set = np.array(pkl.load(f))

    test_set = np.array(pkl.load(f))

    f.close()



    train_set_x,train_set_y = train_set









    #train_set length

    n_samples= len(train_set_x)

    #shuffle and generate train and valid dataset

    sidx = np.random.permutation(n_samples)

    n_train = int(np.round(n_samples * (1. - valid_portion)))

    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]

    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]

    train_set_x = [train_set_x[s] for s in sidx[:n_train]]

    train_set_y = [train_set_y[s] for s in sidx[:n_train]]





    train_set = (train_set_x, train_set_y)

    valid_set = (valid_set_x, valid_set_y)





    #remove unknow words

    def remove_unk(x):

        return [[1 if w >= n_words else w for w in sen] for sen in x]



    test_set_x, test_set_y = test_set

    valid_set_x, valid_set_y = valid_set

    train_set_x, train_set_y = train_set



    train_set_x = remove_unk(train_set_x)

    valid_set_x = remove_unk(valid_set_x)

    test_set_x = remove_unk(test_set_x)







    def len_argsort(seq):

        return sorted(range(len(seq)), key=lambda x: len(seq[x]))



    if sort_by_len:

        sorted_index = len_argsort(test_set_x)

        test_set_x = [test_set_x[i] for i in sorted_index]

        test_set_y = [test_set_y[i] for i in sorted_index]



        sorted_index = len_argsort(valid_set_x)

        valid_set_x = [valid_set_x[i] for i in sorted_index]

        valid_set_y = [valid_set_y[i] for i in sorted_index]





        sorted_index = len_argsort(train_set_x)

        train_set_x = [train_set_x[i] for i in sorted_index]

        train_set_y = [train_set_y[i] for i in sorted_index]



    train_set=(train_set_x,train_set_y)

    valid_set=(valid_set_x,valid_set_y)

    test_set=(test_set_x,test_set_y)









    new_train_set_x=np.zeros([len(train_set[0]),max_len])

    new_train_set_y=np.zeros(len(train_set[0]))



    new_valid_set_x=np.zeros([len(valid_set[0]),max_len])

    new_valid_set_y=np.zeros(len(valid_set[0]))



    new_test_set_x=np.zeros([len(test_set[0]),max_len])

    new_test_set_y=np.zeros(len(test_set[0]))



    mask_train_x=np.zeros([max_len,len(train_set[0])])

    mask_test_x=np.zeros([max_len,len(test_set[0])])

    mask_valid_x=np.zeros([max_len,len(valid_set[0])])







    def padding_and_generate_mask(x,y,new_x,new_y,new_mask_x):



        for i,(x,y) in enumerate(zip(x,y)):

            #whether to remove sentences with length larger than maxlen

            if len(x)<=max_len:

                new_x[i,0:len(x)]=x

                new_mask_x[0:len(x),i]=1

                new_y[i]=y

            else:

                new_x[i]=(x[0:max_len])

                new_mask_x[:,i]=1

                new_y[i]=y

        new_set =(new_x,new_y,new_mask_x)

        del new_x,new_y

        return new_set



    train_set=padding_and_generate_mask(train_set[0],train_set[1],new_train_set_x,new_train_set_y,mask_train_x)

    test_set=padding_and_generate_mask(test_set[0],test_set[1],new_test_set_x,new_test_set_y,mask_test_x)

    valid_set=padding_and_generate_mask(valid_set[0],valid_set[1],new_valid_set_x,new_valid_set_y,mask_valid_x)



    return train_set,valid_set,test_set





#return batch dataset

def batch_iter(data,batch_size):



    #get dataset and label

    x,y,mask_x=data

    x=np.array(x)

    y=np.array(y)

    data_size=len(x)

    num_batches_per_epoch=int((data_size-1)/batch_size)

    for batch_index in range(num_batches_per_epoch):

        start_index=batch_index*batch_size

        end_index=min((batch_index+1)*batch_size,data_size)

        return_x = x[start_index:end_index]

        return_y = y[start_index:end_index]

        return_mask_x = mask_x[:,start_index:end_index]

        # if(len(return_x)<batch_size):

        #     print(len(return_x))

        #     print return_x

        #     print return_y

        #     print return_mask_x

        #     import sys

        #     sys.exit(0)

        yield (return_x,return_y,return_mask_x)

When I open a terminal and run
python train_rnn_classify.py
then  has error:
Traceback (most recent call last):
  File "train_rnn_classify.py", line 176, in <module>
    tf.app.run()
  File "/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "train_rnn_classify.py", line 172, in main
    train_step()
  File "train_rnn_classify.py", line 128, in train_step
    valid_model = RNN_Model(config=eval_config,is_training=False)
  File "/home/hadoop/lstm/rnn_model.py", line 51, in __init__
    (cell_output,state)=cell(inputs[:,time_step,:],state)
  File "/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py", line 953, in __call__
    cur_inp, new_state = cell(cur_inp, cur_state)
  File "/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py", line 235, in __call__
    with _checked_scope(self, scope or "basic_lstm_cell", reuse=self._reuse):
  File "/home/hadoop/anaconda2/lib/python2.7/contextlib.py", line 17, in __enter__
    return self.gen.next()
  File "/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py", line 93, in _checked_scope
    "the argument reuse=True." % (scope_name, type(cell).__name__))
ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'model/LSTM_layer/multi_rnn_cell/cell_0/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.

Why can't I run this example?How to solve this  problem?
Thank you all for your kind help!!!