Unable to run inference on mobilenet

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.4.1
Python version: 3.5.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Describe the problem
I want to run mobilenet inference on one image. I have converted the JPEG image to binary. The binary file size is 602112 bytes [3 x 224 x 224 x sizeof(float32)]
I downloaded the mobilenet model from:
https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md
And tried running inference on it as follows:
init
protobuf = 'mobilenet_v1_0.25_128_frozen.pb'
img_fname = 'ILSVRC2012_val_00000001.bin'
iname = 'prefix/input:0'
oname = 'prefix/MobilenetV1/Predictions/Softmax:0'
read graph definition
gfile = tf.gfile.GFile(protobuf, "rb")
graph_def = tf.GraphDef()
graph_def.ParseFromString(gfile.read())
_ = tf.import_graph_def(graph_def, name='prefix')
load image
image_data = tf.gfile.GFile(img_fname,'rb').read()
run inference
with tf.Session() as sess:
output_tensor = sess.graph.get_tensor_by_name(oname)
output = sess.run(output_tensor, {iname:image_data})
data_fname, _ = oname.split(':0')
data_fname = data_fname.replace("/", ".") + ".bin"
output.tofile(data_fname)
print(data_fname, 'saved')
It doesn't seem to work. It's not crashing but printing a lot of numbers like:
\x86Bo\x13QB\x05\xf6\x89B\xeb\x01qB\x9bb\x80B\xe7\xa7VBA\xb3\x80B\xe3c\x85Bs\x03kB\xd7\xe5\x7fBs;{B\x8be\x8eB[+jB/\x08{B\xff6RB\x87\xe3&B\x15\x1f\xb3A\xc7\xfdQB\xf3\xe6UB\x0b\xa8\nB\xab\xc5GB\xef\x12JB'