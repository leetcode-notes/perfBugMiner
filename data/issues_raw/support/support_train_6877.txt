TF-slim: slim.evaluation.evaluation_loop results in code run that is stuck without progress.

Hi all, I am unsure why the evaluation loop would result in the code not running. My GPU memory is not consumed or whatsoever, and my CPU memory usage is at a minimum. This is what I get:
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
Starting Evaluation of Validation Dataset
predictions type: Tensor("Cast_2:0", shape=(?,), dtype=float32)
Running evaluation Loop...
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/evaluation.py:426 in evaluation_loop.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.
INFO:tensorflow:Waiting for new checkpoint at ../preprocessedData/model.ckpt-50

All I did was to run this:
print 'Starting Evaluation of Validation Dataset'
#Load the validation dataset for evaluation while training
test_dataset = get_split('validation', train_dir)
test_images, _, test_labels = load_batch(test_dataset, height = image_size, batch_size = batch_size, width = image_size, is_training = False)

with slim.arg_scope(inception_resnet_v2_arg_scope()):
	logits, _ = inception_resnet_v2(test_images, num_classes = test_dataset.num_classes, is_training = False)

predictions = tf.argmax(logits, 1)
predictions = tf.cast(predictions, tf.float32)
print 'predictions type:', predictions
# print tf.Print(predictions)

# Define the metrics:
names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
	'streaming_auc': slim.metrics.streaming_auc(predictions, test_labels),
    # 'eval/Accuracy': slim.metrics.streaming_accuracy(predictions, test_labels),
    # 'eval/Recall@5': slim.metrics.streaming_recall_at_k(logits, test_labels, 5),
})

print'Running evaluation Loop...'
checkpoint_path = tf.train.latest_checkpoint(log_dir)
metric_values = slim.evaluation.evaluation_loop(
    master='',
    checkpoint_dir=checkpoint_path,
    logdir=log_dir,
    eval_op=names_to_updates.values(),
    final_op=names_to_values.values())

It's been a really long time since there is no progress. Is the function working?