Training Slows Down -- But Speeds up If Entire Net is Reloaded

NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
There are a few issues but none that simulate the same problem.
Environment info
Operating System: Ubuntu 14.04, tf 0.12
Installed version of CUDA and cuDNN: 8.0
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide: tensorflow pip gpu 0.12

A link to the pip package you installed:
The output from python -c "import tensorflow; print(tensorflow.__version__)".

Problem
I am training a network, but after about 4000 backward passes, the training of the network starts to slow down. I have tried reloading the network after saving it. And again, 4000 steps later it starts to slow down. This means that the slow down is independent of what global step the training is on. The network size never changes, which is usually the problem with these slow downs. Average Step times are like this:
2.4, 2.4, 2.4, 4.6, 3.9, 12.2, 4.2, 2.4, 6.7
I am feeding the model with placeholders, so there is no FIFO Queue problems, and feeding it with text data so its pretty light. I have tested this on three separate machines and the same behavior is replicated throughout them. It is the actual tensorflow session.run command that slows it down tremendously.
When I look at the gpu usage, both gpu's are at 0 percent the entire time (even during the session.run command). They spike up only when the actual forward and backward passes occur but remain dormant the rest of the time. I'm using allocator type 2 in the tf.gradient operation. This perhaps is the most revealing part.
My htop indicates there's no swap memory problems at all.
From what I have concluded the problem must be with tensorflow. The fact that You can reload the model and its completely fine for a little bit is really weird. I've also found that running the following command does not help:
sync; echo 3 > /proc/sys/vm/drop_caches
INFO:tensorflow:global step 2001 learning rate 0.0003882 step-time 2.94 perplexity 28.97 loss 3.3662
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929
INFO:tensorflow:16:01:09 01/10/17 EST
INFO:tensorflow:global step 2251 learning rate 0.0003867 step-time 2.94 perplexity 27.17 loss 3.3022
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.920
INFO:tensorflow:16:13:25 01/10/17 EST
INFO:tensorflow:global step 2501 learning rate 0.0003853 step-time 2.94 perplexity 26.03 loss 3.2594
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929
INFO:tensorflow:16:25:48 01/10/17 EST
INFO:tensorflow:global step 2751 learning rate 0.0003838 step-time 2.97 perplexity 24.80 loss 3.2109
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.956
INFO:tensorflow:16:44:37 01/10/17 EST
INFO:tensorflow:global step 3001 learning rate 0.0003824 step-time 4.52 perplexity 23.77 loss 3.1686
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 4.500
INFO:tensorflow:16:58:05 01/10/17 EST
INFO:tensorflow:global step 3251 learning rate 0.0003809 step-time 3.23 perplexity 23.10 loss 3.1396
INFO:tensorflow:get_batch_step_time 0.016 actual_step_time 3.216
INFO:tensorflow:17:18:34 01/10/17 EST
INFO:tensorflow:global step 3501 learning rate 0.0003795 step-time 4.92 perplexity 21.82 loss 3.0828
INFO:tensorflow:get_batch_step_time 0.015 actual_step_time 4.902

Any help would be greatly appreciated!