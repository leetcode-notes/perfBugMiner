[Bug] tf.contrib.layers.layer_norm: third-party implementation does not reflect the original paper

Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:

It must be a bug or a feature request.
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.3
Python version: 2.7
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: CUDA 8.0/cuDNN 6.0
GPU model and memory: Titan X
Exact command to reproduce:

Describe the problem
In the original layer normalization paper, it's been said that beta/gamma should have the same dimension as hidden variable (see the lines below Eq (4) in Page 3). Based on current implementation, there is no such option that reflects the original paper.
Please run the following source code to verify that.
Also, see the comment #3671 .
Source code / logs
import tensorflow as tf
layer_norm = tf.contrib.layers.layer_norm
batch_size = 10
hidden_dim = 5
input = tf.zeros((batch_size, hidden_dim), dtype=tf.float32)
output = layer_norm(input)
tf.trainable_variables()