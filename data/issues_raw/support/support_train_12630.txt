It seems that tf.gfile.Copy can not support larget hdfs file

It seems that when using tf.gfile.Copy("hdfs://default/some_hdfs_file","./test"), if some_hdfs_file are large file such as 11GB in my case, it will report Exception.  Change to a 3GB file works fine.
I'm using the latest version of tensorflow.
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py", line 384, in copy
    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)
  File "/home/x/anaconda2/lib/python2.7/contextlib.py", line 24, in __exit__
    self.gen.next()
  File "/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://default/tmp/x/x/dmcluster_predict_data_v3/20170817/023645/part-00000