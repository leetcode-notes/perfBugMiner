tf.contrib.learn.LinearClassifier.evaluate reports incorrect labels/prediction_mean

Relatively new to machine learning but this appears to be a reproducible bug. Have not been able to find related issues on web searches.
OS is MacOS,  no CUDA. Installed using pip package:
Mac OS X, CPU only, Python 2.7:
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl
tensorflow version: 0.12.0-rc0
I first noted discrepancy between on my data using a tf.contrib.learn.LinearClassifier. The evaluate function returned a 'labels/actual_label_mean' that was accurate but a 'labels/prediction_mean' that was too low. I was able to reproduce similar results (i.e. 'labels/prediction_mean' too low) using the linearclassifier tutorial at:
https://www.tensorflow.org/versions/r0.12/tutorials/wide/index.html
Following the tutorial, I get the following from m.evaluate(input_fn=eval_input_fn, steps=1)
{'accuracy': 0.83582091,
'accuracy/baseline_label_mean': 0.23622628,
'accuracy/threshold_0.500000_mean': 0.83582091,
'auc': 0.88367212,
'global_step': 400,
'labels/actual_label_mean': 0.23622628,
'labels/prediction_mean': 0.23983434,
'loss': 0.35221672,
'precision/positive_threshold_0.500000_mean': 0.70658684,
'recall/positive_threshold_0.500000_mean': 0.52158087}
The  'labels/actual_label_mean' value exactly matches sum(df_test.label)/len(df_test.label)
If I actually use the predict using
pred = m.predict(input_fn = lambda:input_fn(df_test))
predictions = []
for i in range( df_test.shape[0] ):
    predictions.append(pred.next())
### I realize above code looks ugly but I can't get it to work in a less kludgy way, e.g.
### predictions = list(pred) will hang indefinitely until control-c
### Is this a separate bug?

sum(predictions) -> 2839
len(predictions) -> 16281

The ratio is 0.1743750 which is much lower than the reported value for 'labels/prediction_mean', 0.23983434.
I think that this is a serious bug as predicting/guessing the more likely label artificially inflates the accuracy. As an extreme example, predicting 'no meteor today' will give you a 99.99_% accuracy.
If this is not a bug but rather a misunderstanding of how prediction/evaluation work, pardon my error.