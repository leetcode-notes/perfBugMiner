Segmentation fault or no convergence when using XLA_CPU with distributed TensorFlow

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have modified tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py to use MonitoredTrainingSession and PS/worker hosts.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version: 1.6.0
Python version: 2.7
Bazel version (if compiling from source): 0.11.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: run with a local PS and a local worker

Describe the problem
I'm seeing segmentation fault or no convergence when using mnist_softmax_xla.py in a distributed environment (one PS and one worker, both local) and train_step placed on XLA_CPU.
with tf.device('/device:XLA_CPU:0'):
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

Is this the expected behavior? What is the recommended way of using XLA_CPU in distributed training?