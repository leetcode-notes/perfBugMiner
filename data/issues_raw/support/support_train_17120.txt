Imagenet classification with VGG16 pretrained weights (Keras interface) doesnt seem to work

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.5.0
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: Described below in detail

Describe the problem
I tried to use VGG16 image net classifier which is given through keras interface in tensorflow (tf.keras.applications.VGG16) by grabbing the graph given by Keras and then using it. But it doesn't seem to work.
I thought it might be issue with how I am using it ,but after thinking over it a lot I have concluded that this might an issue with Tensorflow. I had posed about it on SO at https://stackoverflow.com/questions/48850537/issue-with-imagenet-classification-with-vgg16-pretrained-weights
Source code / logs
import tensorflow as tf
import numpy as np
from PIL import Image
from tensorflow.python.keras._impl.keras.applications import imagenet_utils


model = tf.keras.applications.VGG16()
VGG = model.graph

VGG.get_operations()
input = VGG.get_tensor_by_name("input_1:0")
output = VGG.get_tensor_by_name("predictions/Softmax:0")
print(input)
print(output)

I = Image.open("Elephant.jpg")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array = np.expand_dims(image_array, axis=0)
image_array = image_array.astype(np.float32)
image_array = image_array/255


with tf.Session(graph=VGG) as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    pred = (sess.run(output,{input:image_array}))
    print(imagenet_utils.decode_predictions(pred))

And below is the result I get when run:
Tensor("input_1:0", shape=(?, 224, 224, 3), dtype=float32)
Tensor("predictions/Softmax:0", shape=(?, 1000), dtype=float32)
[[('n02281406', 'sulphur_butterfly', 0.0022673723), ('n01882714', 'koala', 0.0021256246), ('n04325704', 'stole', 0.0020583202), ('n01496331', 'electric_ray', 0.0020416214), ('n01797886', 'ruffed_grouse', 0.0020229272)]]
Clearly the classfication results are not the one expected.
Also If I dont run init_op for global variable initializing I get an error Attempting to use uninitialized value block1_conv1/bias