Different buckets give different outputs at first time step during decoding

Hi,
Background
I'm trying to build a chat bot and have a basic understanding of tensorflow Sequence2Sequence API  by reading from here:
https://www.tensorflow.org/tutorials/seq2seq/
I've also read related papers for Neural translation, attention mechanism during encoding/decoding etc.
RNN translate code is taken from here:
https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py
Bucketing/Attention mechanism code is picked from here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py
During my training(attention mechanism, GRU cell), I gave my buckets as [(2,2),(4,4),(6,6),(,8,8),(10,10)].
Issue
During decoding, if I force a bucket_index on the same input I'm getting totally different outputs at the first time step. For example:


Input : How're you ?
Bucket index : 1
Output : NAME UNK NAME
Input : How're you ?
Bucket index : 2
Output : Hi NAME to the


Observations


I can fix the bad outputs at each time step using beam-search. However, why I'm getting different word as output at the first time step during decoding ? Shouldn't the smaller bucket output be a subset of larger bucket output ?


I've tried searching online and everywhere it's mentioned that parameters are shared across various buckets.  Buckets are used for training efficiency and not for model tuning. I've also verified that my trainable parameters are common across all the buckets.


The biggest difference I can see between encoder inputs for various buckets is the extra padding at the very start. We are doing attention mechanism on encoded_states, and softmax weights are already learned for those encoded states during training. Therefore,  Can those softmax weights for those extra padded inputs cause sufficient difference leading to different output during first time step of decoding ?


Did any one else also encounter the above issue?  Any help for fixing the above error would be greatly appreciated.