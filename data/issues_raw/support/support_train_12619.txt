batch normalization

These days i have meet some problem about BN layers, code is here, i want run my net on mnist
dataset, it worked when i am training, how when i verify on valiation data or test date,  when i change the state 'is_training'. what is wrong when i am verifing and how can i save mean and val in training state?
``import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data",one_hot=True)
#define some weights
def weight_variable(shape):
initial = tf.truncated_normal(shape, stddev=0.01)
return tf.Variable(initial)
def bias_variable(shape):
initial = tf.constant(0.01, shape=shape)
return tf.Variable(initial)
def conv2d(input, in_features, out_features, kernel_size, with_bias=False):
W = weight_variable([ kernel_size, kernel_size, in_features, out_features ])
conv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')
if with_bias:
return conv + bias_variable([ out_features ])
return conv
def batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):
current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)
current = tf.nn.relu(current)
current = conv2d(current, in_features, out_features, kernel_size)
current = tf.nn.dropout(current, keep_prob)
return current
def block(input, layers, in_features, growth, is_training, keep_prob):
current = input
features = in_features
for idx in xrange(layers):
tmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)
current = tf.concat([current, tmp],3)
features += growth
return current, features
def avg_pool(input, s):
return tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')
#define graph
layers =  12
print 'create graph ...'
x = tf.placeholder(tf.float32, [None, 784])
y_label = tf.placeholder(tf.float32, [None, 10])
lr = tf.placeholder(tf.float32)
keep_prob = tf.placeholder(tf.float32)
is_training = tf.placeholder(tf.bool, shape=[])
current = tf.reshape(x, [ -1, 28, 28, 1 ])
current = conv2d(current, 1, 16, 3)
current, features = block(current, layers, 16, 12, is_training, keep_prob)
current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)
current = avg_pool(current, 2)  #14x14
current, features = block(current, layers, features, 12, is_training, keep_prob)
current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)
current = avg_pool(current, 2)#7x7
current, features = block(current, layers, features, 12, is_training, keep_prob)
current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)
current = tf.nn.relu(current)
current = avg_pool(current, 7)
final_dim = features
current = tf.reshape(current, [-1, final_dim])
Wfc = weight_variable([final_dim, 10])      #set classifiers
bfc = bias_variable([10])
y_predict = tf.nn.softmax(tf.matmul(current, Wfc) + bfc)
cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_predict))
l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])
weight_decay = 1e-4
#update moving_mean and moving_variance
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy + l2 * weight_decay)
correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_label, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # caculate the right numbers
def mytrain():
print 'train ...'
saver = tf.train.Saver()
with tf.Session() as sess:
sess.run(tf.global_variables_initializer())
for epoch in xrange(1, 10):    #train
if epoch < 150:
l = 0.5
elif epoch <200:
l = 0.1
else:
l = 0.01
print 'epoch: ',epoch
batch_x, batch_y = mnist.train.next_batch(500)
_,acc,loss = sess.run([train_step,accuracy,cross_entropy],
feed_dict={x: batch_x, y_label: batch_y, lr:l,is_training: True, keep_prob: 0.8})
print 'train acc : ',acc," loss: ",loss
#val
batch_x_val = mnist.validation.images
batch_y_val = mnist.validation.labels
acc,loss = sess.run([accuracy,cross_entropy],feed_dict={x: batch_x_val, y_label: batch_y_val, is_training: False, keep_prob: 0.8})
print 'val acc : ',acc,' loss: ',loss
saver.save(sess, 'temp/densenet.ckpt')
def mytest():
print 'test ...'
saver = tf.train.Saver()
with tf.Session() as sess:
saver.restore(sess, './temp/densenet.ckpt')
x_val = mnist.validation.images
y_val = mnist.validation.labels
val_results = sess.run(accuracy,
feed_dict={x: x_val, y_label: y_val, is_training: True, keep_prob: 1.})
print 'val acc: ', val_results
right = 0
for i in range(100):
x_test, y_test = mnist.validation.next_batch(500)
test_results = sess.run(accuracy,
feed_dict={x: x_test, y_label: y_test, is_training: False, keep_prob: 1.})
#right = right + test_results
print 'test:  acc: ', test_results
if name == 'main':
mytrain()
mytest()