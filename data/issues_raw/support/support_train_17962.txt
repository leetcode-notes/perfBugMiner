Stopping back-prop when using tf.cond

I am trying to stop back-prop from happening through one branch of a tf.cond statement. I am doing two forward passes through a CNN and trying to only back-prop the one with lower TCE (train_cross_entropy). The code is as follows:
def f1(): 
        with tf.control_dependencies([tf.stop_gradient(train_cross_entropy2)]):
            return train_cross_entropy1
def f2():
        with tf.control_dependencies([tf.stop_gradient(train_cross_entropy1)]):
            return train_cross_entropy2
train_cross_entropy = tf.cond(train_cross_entropy1 < train_cross_entropy2, f1, f2) 

The speed of this approach is equivalent to writing:
train_cross_entropy = tf.add(train_cross_entropy1, train_cross_entropy2)

Whereas I would hope that the speed would be more similar to
train_cross_entropy = tf.add(train_cross_entropy1, tf.stop_gradient(train_cross_entropy2))

which happens to compute gradients about twice as fast.
I opened up a stack overflow question (https://stackoverflow.com/questions/49436264/proper-use-of-tf-cond-in-cnn/49439351) but I haven't found any help and I am wondering if this is the expected behaviour?
How do I go about stopping gradients from being calculated backwards through both parts of the graph?
Thanks!