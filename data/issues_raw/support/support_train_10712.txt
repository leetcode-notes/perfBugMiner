GPU Error with tf.losses.sparse_softmax_cross_entropy

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): See below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Binary (PIP)
TensorFlow version (use command below): v1.1.0-rc0-61-g1ec6ed5 1.1.0
Bazel version (if compiling from source): n/a
CUDA/cuDNN version: 8.0/5.1.5
GPU model and memory: NVIDIA Titan X - 12GB
Exact command to reproduce:

import tensorflow as tf
import numpy as np

y = np.random.randint(0, 10, size=40, dtype=np.int32)
logits = np.float32(np.random.rand(40, 10))
weights = np.ones(40, dtype=np.float32)

y_t = tf.placeholder(tf.int32)
logits_t = tf.placeholder(tf.float32)

cost = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=logits, weights=tf.ones(40,dtype=tf.float32))
sess = tf.Session()
sess.run(cost, feed_dict={y_t: y, logits_t: logits})

Describe the problem
Am I doing something wrong or does this function not have a GPU implementation yet?  I guess I can do this on the CPU, but I thought I would check.  It does work on the CPU, I did check that.
Source code / logs
The error message is attached.
error_message.txt