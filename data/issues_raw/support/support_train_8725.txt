semantic about softmax_cross_entropy_with_logits

In https://www.tensorflow.org/get_started/mnist/beginners, it says:

we call softmax_cross_entropy_with_logits on tf.matmul(x, W) + b)

In https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits, it says

logits: Unscaled log probabilities.

Here is the question, take the following code for example:
z = tf.matmul(x, W) + b)
cost = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=the_lables)

z is not log probability, why pass it to the parameter logits?