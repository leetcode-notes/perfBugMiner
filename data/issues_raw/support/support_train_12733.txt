Incompatible Shapes during Validation for TensorFlow's seq2seq module

I'm using TensorFlow's seq2seq module. During validation, my decoder will occasionally produce output sequences with different lengths than the target sequences; this causes the following error when calculating the loss using tf.nn.sigmoid_cross_entropy_with_logits (batch major, not time major):
InvalidArgumentError (see above for traceback): Incompatible shapes: [128,22,4] vs. [128,26,4]
What is the best practice for dealing with this problem?
I checked how the NMT tutorial solved the problem. As far as I can tell, they use a TrainingHelper during validation, which forces the decoder to unroll the same number of steps as the target sequence, but this seems like cheating - they're estimating how the model will perform during inference, but the decoder is receiving additional information (the target sequence length) that it won't have at inference time. I opened an issue to clarify, but I haven't heard back.
I also posted on StackOverflow, but from earlier experience, I doubt I'll receive a response.
My problem isn't specific to platform or TensorFlow version, but here's that information regardless:
OS: macOS Sierra version 10.12.6
TensorFlow installed from source
TensorFlow version: ('v1.3.0-rc2-20-g0787eee', '1.3.0')