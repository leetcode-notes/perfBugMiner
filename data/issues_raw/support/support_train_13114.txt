Extremely slow first epoch.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.2 LTS
TensorFlow installed from (source or binary): PIP, tensorflow-gpu==1.2.1
TensorFlow version (use command below): 1.2.1
Python version: Python 3.6.2
CUDA/cuDNN version: CUDA Version 8.0.61 / cuDNN Version 5.1.10
GPU model and memory: GTX 1080ti x 2, 11171MiB each.

Describe the problem
When training model, "only First epoch after executing training script is extremely slow (More specifically, It takes 12 hours while second and third epoch after executed takes 2.5 hours with my environment and dataset)"
I uploaded this issue because similar issue in stackoverflow is not handled long time.
(https://stackoverflow.com/questions/44966831/tensorflow-first-epoch-is-extremely-slow-maybe-related-to-pool-allocator)
In more detail,
my code uses model with  1d convolution layers (tf.nn.conv1d - tf.bias_add - tf.nn.relu) as model.
tf.ctc_loss as loss function.
Data is served with tf.PaddingFIFOQueue && tf.QueueBase.dequeue_many and "each data has different size".
Here is what I tried,
First I also assumed that pool allocator makes this problem like above stackoverflow link (but now I think this issue is not from pool allocator)

Uses tcmalloc
LD_PRELOAD="/usr/lib/libtcmalloc.so" which is suggested in here
Uses BFC allocator type.


With TF_CPU_ALLOCATOR_USE_BFC=true
And I tried tf.ConfigProto().gpu_options. allocator_type = 'BFC' also.


Build tensorflow from source with modified initial pool size limit (100 to 10000), (Which is hard coded in here)

And all trying was ineffective.
Except extremely slow first epoch, everything was fine. Weights of model is trained well, save and load checkpoint and continuing training or inferencing is also fine without any warning and error.
Is it one of avoidable characteristic of Tensorflow or bugs?
Thank you for your reading.
If you need any more information, please notify me.