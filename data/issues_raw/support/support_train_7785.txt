Distributed Tensorflow fails : no device

I am following the example in https://www.tensorflow.org/versions/r0.10/how_tos/distributed/  to create a distributed tensorflow model with a parameter server and n workers. I do not have any GPU, all work is distributed on CPU
In the chief worker, I want to save my variables every some steps, but invoking the saver results in the following exception :
Cannot assign a device to node 'save_1/RestoreV2_21': 
Could not satisfy explicit device specification 
'/job:ps/task:0/device:CPU:0' because no devices matching that 
specification are registered in this process; available devices: 
/job:localhost/replica:0/task:0/cpu:0

[[Node: save_1/RestoreV2_21 = RestoreV2[dtypes=[DT_INT32],
_device="/job:ps/task:0/device:CPU:0"](save_1/Const, 
save_1/RestoreV2_21/tensor_names, save_1/RestoreV2_21/shape_and_slices)]]

I tried :
server = tf.train.Server(cluster,
                         job_name=self.calib.params['job_name'],
                         task_index=self.calib.params['task_index'],
                         config=tf.ConfigProto(allow_soft_placement=True)

I am using a supervisor as in the example in the documentation:
sv = tf.train.Supervisor(
                         is_chief=is_chief,
                       ...)

and creating my session as follows :
sess = sv.prepare_or_wait_for_session(server.target) as sess:

but I am still having the exact same error.
When I print server.target I obtain
grpc://localhost:xxxx

where xxxx is 2200 for my first worker, 2201 for my second worker and so on