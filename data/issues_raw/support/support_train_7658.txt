Optimizer var_list does not have effect on the excluded variable!

I have written piece of TensorFlow code which has two optimizers and I would like to exclude specific variables from being updated while calling "run" on any of these optimizers. As suggested by the TensorFlow documentation, I have specifically generated a list of variables to be updated for each optimizer, like below:
```

mentor_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "mentor")
train_op_mentor = mnist.training(loss_mentor, FLAGS.learning_rate, mentor_training_vars)
mentee_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "mentee")
train_op_mentee = mnist.training(loss_mentee, FLAGS.learning_rate, mentee_training_vars)
mentee_indep_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "mentee_indep")
train_op_mentee_indep = mnist.training(loss_mentee_indep, FLAGS.learning_rate, mentee_indep_training_vars)

The training functions in the mnist object is defined as:L

def training(loss, learning_rate, var_list):
  # Add a scalar summary for the snapshot loss.
  tf.summary.scalar('loss', loss)
  # Create the gradient descent optimizer with the given learning rate.
  optimizer = tf.train.GradientDescentOptimizer(learning_rate)
  # Create a variable to track the global step.
  global_step = tf.Variable(0, name='global_step', trainable=False)
  # Use the optimizer to apply the gradients that minimize the loss
  # (and also increment the global step counter) as a single training step.
  train_op = optimizer.minimize(loss, global_step=global_step, var_list=var_list)
  return train_op



As it's clear in the above code, I have three namescopes, where each has their own variables.
Now, let's say I only want to train the mentor variables. When I put a breakpoint after running session on the mentor optimizer, I can see that the mentee variables content is being changed after each run. Now I'm wondering whether I'm using this feature correctly, or there is something wrong with this API?