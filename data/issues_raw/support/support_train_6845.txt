input pipeline spends all time in QueueDequeueMany

My training process use tfrecord format for train&eval dataset .
I test the benchmark of reader , only 8000records/second.  and io speed(see from iotop command) just 400KB-500KB/s.
I'm using the cpp version of protobuf here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#4467
#4732
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
def read_and_decode(filename_queue):
     reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    return serialized_example

  serialized_example = read_and_decode(filename_queue)
  batch_serialized_example = tf.train.shuffle_batch(
      [serialized_example],
      batch_size=batch_size,
      num_threads=thread_number,
      capacity=capacity,
      min_after_dequeue=min_after_dequeue)
  features = tf.parse_example(
      batch_serialized_example,
      features={
          "label": tf.FixedLenFeature([], tf.float32),
          "ids": tf.VarLenFeature(tf.int64),
          "values": tf.VarLenFeature(tf.float32),
      })

What other attempted solutions have you tried?
I try to set num_threads in  tf.train.shuffle_batch but not working. It seems that when set to 2 threads, it work at 8000records/s, when enlarge the thread number, it get slower. (I remove all ops that cost cpus. Just read data.)
My sever are 24 core cpus.