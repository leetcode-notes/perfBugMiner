enable get the 'mathematical' gradient of output w.r.t neural network parameters

In some situation, it is necessary to get the 'mathematical' gradient of output w.r.t neural network parameters.
For example, suppose I have a neural network out = f(s), where s is a batch of input with shape[None, dim_s], while out is a scaler, f is simply a MLP. With tf.gradient(out, tf.trainable_variables()) I can get gradient of out w.r.t neural network parameters of f, which is a list of gradient. Now, I have two different batch of s: s1 and s2, then we can get two different the above gradients G1 and G2. It seems that it is impossible to compute cosine between G1 and G2 using current tensorflow? Do I need to flatten both gradients first? Do G1 and G2 are the usual gradient in math?