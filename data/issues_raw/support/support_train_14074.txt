A problem about AttentionWrapper

Hi,
It is different to use Bahdanau Attention and Luong Attention in seq2seq model. When I use Bahdanau Attention I found a problem.
The hint said that when use Bahdanau Attention, need to set the output_attention=False when create AttentionWrapper. But if I set this parameter = False, I cant get the attention information in every timestamp and can not calculate the logits from both attention and cell_output.
Is there any good way to get the attention of every timestamp?
And if simply use cell_output to get the logits is quite similar with the way which both use attention and cell_output?
Thx a lot.