Gradients be dropped in tf.train.SyncReplicaOptimizer

Problem Description
tf.train.SyncReplicaOptimizer is very helpful for backup workers, but the gradients computed by slow workers are just dropped. Below is the documentation I see

Once the gradients have been computed, push them into gradient_queue only if local_step equals global_step, otherwise the gradients are just dropped

The problem is, I want to use backup workers in online training, it means every sample can be just consumed by TensorFlow once and I don't want some samples to be just ignored, is it possible to make backup servers behave like the one in Hadoop which is guaranteed to utilize all data partitions?
Or can the workers know whether its gradient is dropped or not by the aggregator?
I think this feature will be useful.