Jupyter notebook: Kernel dies when running timeline trace [Nvidia-docker]

I am trying to use timeline to profile GPU memory usage on an EC2 instance using Tensorflow-Gpu Nvidia-Docker. When I add a few lines to my notebook which runs a convNet, it keeps restarting with a "Kernel died" message.
Some of the changes I made:
from tensorflow.python.client import timeline

run_metadata = tf.RunMetadata()

with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:
    tf.global_variables_initializer().run()
    for step in range(num_steps):
        batch_data, batch_labels = generate_batch(
          batch_size, num_skips, skip_window)
        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
        if step == 1000:
            _, l = session.run([optimizer, loss], feed_dict=feed_dict, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)
            trace = timeline.Timeline(step_stats=run_metadata.step_stats)
            with open('timeline.ctf.json', 'w') as trace_file:
                trace_file.write(trace.generate_chrome_trace_format())
        else:
            _, l = session.run([optimizer, loss], feed_dict=feed_dict)
        ...