Memory violation when adding an new op

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 14.04
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
commit 3bee923
Bazel version (if compiling from source):
0.4.5
CUDA/cuDNN version:
cuda 8.0/cudnn 5.1.5
GPU model and memory:
Tesla P40
Exact command to reproduce:

Describe the problem
I created a custom operation, it works well with bazel test  --run_under=valgrind. However, when I uses the custom operation using python api, memory violation is happend. Even though no modification of the input tensor, input tensor error comes out.
Source code / logs
namespace tensorflow {

class InGraphAutoParallelOp : public OpKernel {
 public:
  explicit InGraphAutoParallelOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* ctx) override {
    // Get graph definition
    const Tensor* meta_graph_proto_tensor;
    OP_REQUIRES_OK(ctx, ctx->input("meta_graph_def_str", &meta_graph_proto_tensor));
    MetaGraphDef meta_graph_def;
    meta_graph_def.ParseFromString(meta_graph_proto_tensor->scalar<string>()());

    // Get num replicas
    const Tensor* num_replicas_tensor;
    OP_REQUIRES_OK(ctx, ctx->input("num_replicas", &num_replicas_tensor));
    int num_replicas = num_replicas_tensor->flat<int>()(0);

    MetaGraphDef out_meta_graph_def;
    if (num_replicas == 1) {
      out_meta_graph_def = meta_graph_def;
    } else {
      rdag::grappler::AutoParallel auto_parallel(num_replicas);
      auto_parallel.BuildGraph(meta_graph_def, out_meta_graph_def.mutable_graph_def());
    }

    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(ctx,
        ctx->allocate_output(0, TensorShape({}), &output_tensor));
    CHECK(out_meta_graph_def.SerializeToString(&output_tensor->scalar<string>()()));
  }
};

REGISTER_KERNEL_BUILDER(Name("InGraphAutoParallel").Device(DEVICE_CPU), InGraphAutoParallelOp);

 

Below is the modification of auto_parallel code in TensorFlow.
#include "tensorflow_rdag/grappler/auto_parallel.h"

#include "tensorflow_rdag/grappler/grappler_item_builder.h"
#include "tensorflow_rdag/grappler/op_types.h"
#include "tensorflow_rdag/grappler/utils.h"

namespace tensorflow {
namespace rdag {
namespace grappler {

static const std::set<std::string> APPLY_GRADIENT_OPS = {"ApplyGradientDescent",
                                                     "ApplyProximalGradientDescent",
                                                     "ApplyAdadelta",
                                                     "ApplyAdagrad",
                                                     "ApplyProximalAdagrad",
                                                     "ApplyAdagradDA",
                                                     "ApplyFtrl",
                                                     "ApplyMomentum",
                                                     "ApplyAdam",
                                                     "ApplyRMSProp",
                                                     "ApplyCenteredRMSProp"};
static std::map<std::string, int> GRADIENT_POS = {{"ApplyGradientDescent", 2},
                                             {"ApplyProximalGradientDescent", 4},
                                             {"ApplyAdadelta", 6},
                                             {"ApplyAdagrad", 3},
                                             {"ApplyProximalAdagrad", 5},
                                             {"ApplyAdagradDA", 3},
                                             {"ApplyFtrl", 3},
                                             {"ApplyMomentum", 3},
                                             {"ApplyAdam", 9},
                                             {"ApplyRMSProp", 7},
                                             {"ApplyCenteredRMSProp", 8}};
const char kAutoParallelPrefix[] = "AutoParallel";

std::string AutoParallel::GetGradientNodeName(const std::string& apply_gradient_node_name) {
  auto apply_gradients_node = all_nodes_[apply_gradient_node_name];
  return apply_gradients_node->input(GRADIENT_POS[apply_gradients_node->op()]);
}

Status AutoParallel::Initialize(const GrapplerItem& item) {
  LOG(INFO) << "Number of replicas: " << num_replicas_;
  item_ = &item;
  graph_ = item.graph;
  LOG(INFO) << "Original graph size: " << item.graph.node_size();
  if (item.fetch.empty()) {
    return Status(error::INVALID_ARGUMENT, "No fetch nodes provided.");
  }

  if (item.MainVariables().empty()) {
    return Status(error::INVALID_ARGUMENT, "No variables provided.");
  }

  for (const auto& init : item.init_ops) {
    VLOG(1) << "Init node: " << init;
  }

  for (const auto& fetch : item.fetch) {
    VLOG(1) << "Fetch node: " << fetch;
  }

  for (const auto& var : item.MainVariables()) {
    VLOG(2) << "Variable: " << var->name();
  }

  for (QueueRunnerDef def : item.queue_runners) {
    queue_runners_.insert(std::make_pair(def.queue_name(), def));
  }

  for (VariableDef def : item.variables) {
    variables_.insert(std::make_pair(def.variable_name(), def));
  }

  std::vector<std::string> queue_nodes;
  for (int i = 0; i < graph_.node_size(); i++) {
    all_nodes_.insert(
        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));
    if (APPLY_GRADIENT_OPS.find(graph_.node(i).op()) !=
      APPLY_GRADIENT_OPS.end()) {
      apply_gradients_nodes_.insert(graph_.node(i).name());
      VLOG(2) << "Apply gradients node: " << graph_.node(i).name();
    }
  }

  std::set<std::string> dont_replicate_nodes;
  for (const auto& variable : item.MainVariables()) {
    dont_replicate_nodes.insert(variable->name());
    VariableDef def = variables_[variable->name()];
    dont_replicate_nodes.insert(def.initializer_name());
    dont_replicate_nodes.insert(def.snapshot_name());
  }

  std::vector<std::string> gradient_nodes;
  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {
    gradient_nodes.push_back(GetGradientNodeName(apply_gradient_node_name));
  }

  auto train_nodes = ComputeTransitiveFanin(graph_, gradient_nodes);
  LOG(INFO) << "Number of training nodes: " << train_nodes.size();

  for (const auto& fetch_node_name : item.fetch) {
    dont_replicate_nodes.insert(fetch_node_name);
  }

  std::vector<const NodeDef*> enqueue_dequeue_nodes;
  std::vector<const NodeDef*> visitied;
  for (const auto& node : train_nodes) {
    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {
      replica_nodes_.insert(node->name());
      if (IsDequeueOp(*node)) {
        enqueue_dequeue_nodes.push_back(node);
      }
    }
  }
  LOG(INFO) << "Number of replica nodes: " << replica_nodes_.size();

  std::vector<const NodeDef*> input_nodes;
  while (!enqueue_dequeue_nodes.empty()) {
    // Pop first node in eneque_dequeue_nodes.
    const NodeDef* enqueue_dequeue_node = *enqueue_dequeue_nodes.begin();
    enqueue_dequeue_nodes.erase(enqueue_dequeue_nodes.begin());
    if(std::find(visitied.begin(), visitied.end(), enqueue_dequeue_node) != visitied.end())
      continue;
    visitied.push_back(enqueue_dequeue_node);

    auto temp_input_nodes = ComputeTransitiveFanin(graph_, {enqueue_dequeue_node->name()});

    for (const NodeDef* input_node : temp_input_nodes) {
      input_nodes.push_back(input_node);

      if (IsQueueOp(*input_node)) {
        QueueRunnerDef def = queue_runners_[input_node->name()];
        for (int i = 0; i < def.enqueue_op_name_size(); i++) {
          const auto& enqueue_op = all_nodes_[def.enqueue_op_name(i)];
          input_nodes.push_back(enqueue_op);
          enqueue_dequeue_nodes.push_back(enqueue_op);
        }
        input_nodes.push_back(all_nodes_[def.close_op_name()]);
        input_nodes.push_back(all_nodes_[def.cancel_op_name()]);
      } else if (IsDequeueOp(*input_node)) {
        enqueue_dequeue_nodes.push_back(input_node);
      }
    }
  }

  LOG(INFO) << "Number of input nodes: " << input_nodes.size();

  // Replicate all input pipeline nodes
  for (const auto& input_node : input_nodes) {
    replica_nodes_.insert(input_node->name());
  }

  for (const auto& node : all_nodes_) {
    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {
      shared_nodes_.insert(node.first);
    }
  }
  LOG(INFO) << "Number of shared nodes: " << shared_nodes_.size();
  return Status::OK();
}

bool AutoParallel::NotSharedNode(const std::string& name) {
  return shared_nodes_.find(name) == shared_nodes_.end();
}

void AutoParallel::AddSharedNodes(GraphDef* graph) {
  std::string prefix = strings::StrCat(kAutoParallelPrefix, "-Replica-", 0);
  for (const auto& node : shared_nodes_) {
    auto new_node = graph->add_node();
    *new_node = *all_nodes_[node];
    for (int i = 0; i < new_node->input_size(); i++) {
      if (NotSharedNode(NodeName(new_node->input(i)))) {
        std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);
        *new_node->mutable_input(i) = new_name;
      }
    }
  }
}

void AutoParallel::AddOneReplica(GraphDef* graph, int number) {
  std::string prefix = strings::StrCat(kAutoParallelPrefix, "-Replica-", number);
  for (const auto& node : replica_nodes_) {
    auto new_node = graph->add_node();
    *new_node = *all_nodes_[node];
    assert(new_node != all_nodes_[node]);
    if (NotSharedNode(new_node->name())) {
      new_node->set_name(AddPrefixToNodeName(new_node->name(), prefix));
      if (num_replicas_ > 0) {
        // TODO : keep previous device setting except gpu device
        // new_node->set_device(std::strings::StrCat("/gpu:", number % num_replicas_));
      }
      for (int i = 0; i < new_node->input_size(); i++) {
        if (NotSharedNode(NodeName(new_node->input(i)))) {
          std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);
          *new_node->mutable_input(i) = new_name;
        }
      }
    }
  }
}

NodeDef* AutoParallel::AddNodeDivConst(GraphDef* graph) {
  NodeDef* node = graph->add_node();
  node->set_name(strings::StrCat(kAutoParallelPrefix, "-Div-Const"));
  node->set_op("Const");

  AttrValue attr_data_type;
  attr_data_type.set_type(DT_FLOAT);
  node->mutable_attr()->insert({"dtype", attr_data_type});

  AttrValue attr_tensor;
  auto tensor = attr_tensor.mutable_tensor();
  tensor->add_float_val(static_cast<float>(num_replicas_));
  tensor->set_dtype(DT_FLOAT);
  node->mutable_attr()->insert({"value", attr_tensor});
  return node;
}

NodeDef* AutoParallel::AddNodeDiv(GraphDef* graph, const std::string& name, const std::string& input_a,
                                  const std::string& input_b) {
  NodeDef* node = graph->add_node();
  node->set_name(strings::StrCat(kAutoParallelPrefix, "-Div-", name));
  node->set_op("RealDiv");
  node->add_input(input_a);
  node->add_input(input_b);
  AttrValue attr_type;
  attr_type.set_type(DT_FLOAT);
  node->mutable_attr()->insert({"T", attr_type});
  return node;
}

void AutoParallel::AddApplyGradientDescent(GraphDef* graph) {
  std::map<std::string, NodeDef*> apply_gradients_nodes;
  for (int i = 0; i < graph->node_size(); i++) {
    if (APPLY_GRADIENT_OPS.find(graph->node(i).op()) !=
        APPLY_GRADIENT_OPS.end()) {
        apply_gradients_nodes.insert(
          std::make_pair(graph->node(i).name(), graph->mutable_node(i)));
    }
  }

  auto div_const_node = AddNodeDivConst(graph);

  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {
    auto apply_gradients_node = apply_gradients_nodes[apply_gradient_node_name];

    // Add all gradients
    NodeDef* add_node = graph->add_node();
    add_node->set_name(strings::StrCat(kAutoParallelPrefix, "-Add-", apply_gradient_node_name));
    add_node->set_op("AddN");
    const auto& input_name = GetGradientNodeName(apply_gradient_node_name);
    for (int i = 0; i < num_replicas_; i++) {
      add_node->add_input(strings::StrCat(kAutoParallelPrefix, "-Replica-", i, "/", input_name));
      LOG(INFO) << "Gradient Node Name: " << strings::StrCat(kAutoParallelPrefix, "-Replica-", i, "/", input_name);
    }
    AttrValue attr_type;
    attr_type.set_type(DT_FLOAT);
    add_node->mutable_attr()->insert({"T", attr_type});
    AttrValue attr_type2;
    attr_type2.set_type(DT_INT32);
    attr_type2.set_i(num_replicas_);
    add_node->mutable_attr()->insert({"N", attr_type2});

    // Divide by number of GPUs
    auto div_node = AddNodeDiv(
        graph,
        apply_gradient_node_name,
        add_node->name(),
        div_const_node->name());

    LOG(INFO) << "Change gradient node as : " << div_node->name();
    *apply_gradients_node->mutable_input(GRADIENT_POS[apply_gradients_node->op()]) =
        div_node->name();
  }
}

void AutoParallel::BuildGraph(const MetaGraphDef& meta_graph, GraphDef* graph) {
  ItemConfig cfg;
  std::unique_ptr<GrapplerItem> grappler_item = GrapplerItemFromMetaGraphDef(
      "graph_to_optimize", meta_graph, cfg);
  Initialize(*grappler_item);

  //GraphDef* out_graph_def = out_graph.mutable_graph_def();
  AddSharedNodes(graph);
  for (int i = 0; i < num_replicas_; i++) {
    AddOneReplica(graph, i);
  }
  AddApplyGradientDescent(graph);
  //
  *(graph->mutable_library()) = meta_graph.graph_def().library();
  // LOG(INFO) << "Parallelized graph size: " << out_graph_def->node_size();

  // SetMetaGraphForQueueRunners(out_graph, replicated_queues);
  // SetMetaGraphForVariables(out_graph, variables);
  //return graph;
}

}  // end namespace grappler
}  // end namespace rdag
}  // end namespace tensorflow