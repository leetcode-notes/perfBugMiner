tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)

I try to run the Sequence-to-Sequence Models with tensorflow, but when I run the training set, I have the problem like this

Traceback (most recent call last):
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1022, in _do_call
return fn(*args)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1004, in _run_fn
status, run_metadata)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/contextlib.py", line 89, in exit
next(self.gen)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 469, in raise_exception_on_not_ok_status
pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)
[[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=["loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding"], validate_indices=true, _device="/job:localhost/replica:0/task:0/cpu:0"](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File "translate.py", line 322, in 
tf.app.run()
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 44, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File "translate.py", line 319, in main
train()
File "translate.py", line 210, in train
target_weights, bucket_id, False)
File "/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py", line 251, in step
outputs = session.run(output_feed, input_feed)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 767, in run
run_metadata_ptr)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 965, in _run
feed_dict_string, options, run_metadata)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1015, in _do_run
target_list, options, run_metadata)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1035, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)
[[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=["loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding"], validate_indices=true, _device="/job:localhost/replica:0/task:0/cpu:0"](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]
Caused by op 'model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15', defined at:
File "translate.py", line 322, in 
tf.app.run()
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 44, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File "translate.py", line 319, in main
train()
File "translate.py", line 178, in train
model = create_model(sess, False)
File "translate.py", line 136, in create_model
dtype=dtype)
File "/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py", line 179, in init
softmax_loss_function=softmax_loss_function)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py", line 1180, in model_with_buckets
decoder_inputs[:bucket[1]])
File "/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py", line 178, in 
lambda x, y: seq2seq_f(x, y, False),
File "/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py", line 142, in seq2seq_f
dtype=dtype)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py", line 876, in embedding_attention_seq2seq
initial_state_attention=initial_state_attention)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py", line 772, in embedding_attention_decoder
embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py", line 772, in 
embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py", line 111, in embedding_lookup
validate_indices=validate_indices)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1359, in gather
validate_indices=validate_indices, name=name)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 763, in apply_op
op_def=op_def)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 2395, in create_op
original_op=self._default_original_op, op_def=op_def)
File "/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1264, in init
self._traceback = _extract_stack()
InvalidArgumentError (see above for traceback): indices[49] = 60000 is not in [0, 60000)
[[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=["loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding"], validate_indices=true, _device="/job:localhost/replica:0/task:0/cpu:0"](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]

It seems the problem happens randomly when running training set. It means when I have this problem,I can continue and this may not happen in next steps.
I checked this problem on previous issues, but I am sure that I have set the parameter of vocabulary size
Here are my input commands:

python translate.py --data_dir /private/tmp/ch-en  --train_dir /private/tmp/ch-en/train_result --size=256 --num_layers=2 --steps_per_checkpoint=200 --from_vocab_size=60000 --to_vocab_size=60000

So how to solve this problem?