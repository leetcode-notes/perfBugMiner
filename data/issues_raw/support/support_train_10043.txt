Dynamic_attention_wrapper using rnn output or state to caculate next attention?

In tensorflow 1.1,  https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py#L535, dynamic_attention_wrapper use RNN output to calculate next attention
but in previous seq2seq api, https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L692, attention_decoder use RNN state to calculate next attention.
In paper, https://arxiv.org/pdf/1409.0473.pdf. page 3, under equation 6, it use RNN state. I hope some could tell me whether it will affect model in practice.
There is also a stackoverflow post without satisfied answer http://stackoverflow.com/questions/43248613/attention-decoder-implementation-in-tensorflow-r1-0