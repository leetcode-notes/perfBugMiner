Inception retraining / transfer learning can only utilize one CPU core and one GPU

The retraining code is example code, but it does seem like there are many people experimenting with retraining and improving performance could make the example code much more useful as a launchpad for others if it can scale across GPUs and multiple CPU cores.
At the moment, running
bazel-bin/tensorflow/examples/image_retraining/retrain --num_gpus=2 --image_dir /images
will only result in a single GPU performing computations in nvidia-smi, even though both GPUs are running python in nvidia-smi.  GPU #2 will see zero memory/power utilization during the computation beyond idle levels.  GPU #1 will utilize all memory TensorFlow and an additional 30 watts or so.
The other related issue is that, during training, a single CPU core will be maxed out at 100% running python; given the capabilities of the card, I'm guessing removing this bottleneck would increase retraining speed by perhaps 200-400%.  nmon shows that disk IO is not a bottleneck.
Thanks for your time and for making TensorFlow available to everyone!
Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: 8.0 and 5 with two GTX 1080 cards
retrain.py: train_batch_size is 20000 and learning_rate is 0.5

The commit hash (git rev-parse HEAD): r0.9.0
The output of bazel version: Build label: 0.2.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue May 17 14:21:13 2016 (1463494873)
Build timestamp: 1463494873
Build timestamp as int: 1463494873