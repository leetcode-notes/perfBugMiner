No gradient defined for operation EluGrad

tf.nn.elu does not support second derivatives currently.
This would be awesome, as in variational autoencoder models these activation functions improve performance by a few nats in the objective compared to tanh, sigmoid (which do support second derivatives).
In the meantime I'm using elu = lambda x: tf.select(x < 0., tf.exp(x) - 1., x) which seems to work:
In [29]: elu = lambda x: tf.select(x < 0., tf.exp(x) - 1., x)

In [30]: sess.run(tf.gradients(elu(z), z), {z: -1.})
Out[30]: [0.36787945]

In [31]: sess.run(tf.gradients(elu(z), z), {z: 1.})
Out[31]: [1.0]