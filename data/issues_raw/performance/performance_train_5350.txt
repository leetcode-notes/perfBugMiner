Sampled and regular softmax should use the same weight matrix shape

As it is now, the softmax samplers sampled_softmax_loss and nce_loss require a |C|x|H| weight matrix, while the logits from softmax come from a |H|x|C|-sized one. This discrepancy is problematic on two levels:

it makes for an inconsistent API;
it results in a performance hit, because if one uses a sampled method for training and softmax for testing (as is usual when the number of classes is huge), one has to call tf.transpose somewhere, which does not work well with sparse input (see #4138).

In my case, I can choose between 21150 / 18300 or 22350 / 11900 train / test wps, depending on whether the tf.transpose happens on the sampled softmax loss during training or on regular softmax during testing. In either case, the performance is suboptimal.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
As mentioned above, #4138. The fix there, however, is in the client code, not the API in question.
Environment info
Operating System: Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u3 (2016-01-17) x86_64 GNU/Linux
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44; I don't have cuDNN at the moment
If installed from binary pip package, provide:

A link to the pip package you installed: the official GPU install for Python 3.4
The output from python -c "import tensorflow; print(tensorflow.__version__)": 0.11.0rc1

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I cannot do that right now, but one can experiment with e.g. the PTB example.
What other attempted solutions have you tried?
N/A