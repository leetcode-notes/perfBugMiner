tf.layers.batch_normalization does not support fused

tf.layers.batch_normalization does not accept a fused argument, and appears to always use tf.nn.batch_normalization instead of tf.nn.fused_batch_norm. As such, it does not appear to be possible to follow performance best-practices when using tf.layers.batch_normalization.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Searched issues for "fused" and "batch_normalization"
Environment info
Operating System: gcr.io/tensorflow/tensorflow:1.0.0-devel-gpu
What other attempted solutions have you tried?
Currently shimming tf.contrib.layers.batch_norm