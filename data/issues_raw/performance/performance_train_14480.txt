Dataset API batching is slow for numpy arrays

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
1.4rc1
Python version:
3.5.2
Bazel version (if compiling from source):
Not sure, think 0.7?
GCC/Compiler version (if compiling from source):
5.4
CUDA/cuDNN version:
8/6
GPU model and memory:
gtx980
Exact command to reproduce:
see attached

Describe the problem
The tf.data.Dataset.batch() functions seems to be slow when concatenating numpy arrays into batches.
The attached code basically pushes dummy data ([0, 1, 2, ... , NUM_ITEMS] resembling the output of range()) through tensorflow by different approaches.
There are basically two ways to go about this:

Use the tf.data.Dataset.range()
Use custom generators and tf.data.Dataset.from_generator()

Also the data can be batched to increase throughput as it is usually done in deep learning. This can be done either in the generator itself or via tf.data.Dataset.batch(). The latter appears to be ~30x slower for numpy arrays. Somehow this is not the case for 'native' tensorflow generation.
I attached a benchmark to reproduce this. Please note that the behavior for larger amounts of data e.g. images is similar, resulting in only tens of images per second instead of hundreds.
Comments to the source code:
gen() is a python generator resembling range().
gen_batch() is also a python generator, but yields batches of 100 numbers instead of single numbers.
These two generators are benchmarked first, yielding millions of elements per second. They should not be a bottleneck.
Then we define a few tf.data.Dataset:
ds_range_single is similar to gen(), using tf.data.Dataset.range()
ds_range_batch is similar to gen_batch(), using tf.data.Dataset.range().batch()
ds_npy_single uses gen() with tf.data.Dataset.from_generator()
ds_npy_batch uses gen() with tf.data.Dataset.from_generator().batch()
ds_npy_single_batch uses uses gen_batch() and will perform much better than ds_npy_batch. In fact it will be close to the 'native' ds_range_batch.
Source code / logs
from time import time
import numpy as np
import tensorflow as tf

NUM_ITEMS = 100000
BATCH_SIZE = 100


def gen():
    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64):
        yield x


def gen_batch():
    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64).reshape(NUM_ITEMS//BATCH_SIZE, BATCH_SIZE):
        yield x

start = time()
for _ in gen():
    pass
py_single_generator_time = time() - start

start = time()
for _ in gen_batch():
    pass
py_batch_generator_time = time() - start

ds_range_single = tf.data.Dataset.range(NUM_ITEMS)\
    .make_one_shot_iterator().get_next()

ds_range_batch = tf.data.Dataset.range(NUM_ITEMS)\
    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()

ds_npy_single = tf.data.Dataset.from_generator(
    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\
    .make_one_shot_iterator().get_next()

ds_npy_batch = tf.data.Dataset.from_generator(
    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\
    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()

ds_npy_single_batch = tf.data.Dataset.from_generator(
    gen_batch, output_types=tf.int64, output_shapes=tf.TensorShape([BATCH_SIZE]))\
    .make_one_shot_iterator().get_next()

with tf.Session() as sess:
    start = time()
    for _ in range(NUM_ITEMS):
        sess.run(ds_npy_single)
    npy_single_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS // BATCH_SIZE):
        sess.run(ds_npy_batch)
    npy_batch_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS // BATCH_SIZE):
        sess.run(ds_npy_single_batch)
    npy_single_batch_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS):
        sess.run(ds_range_single)
    range_single_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS // BATCH_SIZE):
        sess.run(ds_range_batch)
    range_batch_time = time() - start


print('Python single generator examples/s :', NUM_ITEMS/py_single_generator_time)
print('Python batch generator examples/s :', NUM_ITEMS/py_batch_generator_time)
print('tf npy single examples/s :', NUM_ITEMS/npy_single_time)
print('tf npy batch examples/s :', NUM_ITEMS/npy_batch_time)
print('tf npy single batch examples/s', NUM_ITEMS/npy_single_batch_time)
print('tf range single examples/s :', NUM_ITEMS/range_single_time)
print('tf range batch examples/s :', NUM_ITEMS/range_batch_time)
Prints the following on my machine:
Python single generator examples/s : 3779128.899140432
Python batch generator examples/s : 137113566.52500817
tf npy single examples/s : 2623.528591111384
tf npy batch examples/s : 10184.987886595052
tf npy single batch examples/s 308841.62594729604
tf range single examples/s : 4673.512357184766
tf range batch examples/s : 352137.31184393575