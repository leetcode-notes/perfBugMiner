consuming Dataset becomes slower and slower, if make_one_shot_iterator each epoch

Problem
I run make_one_shot_iterator() each epoch because  I want re-shuffle the dataset each epoch. I Know that the dataset.shuffle().repeat().batch() pipeline can do almost the same thing, but when data_num can not be divided exactly by batch_size, the pipeline merges two epochs at their boundary to construct a complete batch,  which I HATE.
So, I choose to run dataset.shuffle().batch() and make_one_shot_iterator() before each epoch. But I find that the speed of consuming dataset becomes slower and slower, significantly. I tried different settings to find out that it is make_one_shot_iterator() which makes consuming slow.
By the way, I build tensorflow 1.4 from source on OS X with support of GPU, some hacky workaround.
Code
num_data = 1000
num_epoch = 50
batch_size = 32
dataset = tf.data.Dataset.range(num_data)

mode=3 
# model = 1,2,or 3
# 1: re-shuffle, re-batch and re-make-iterator each epoch
# 2: re-batch and re-make-iterator
# 3: only re-make-iterator

with tf.Session() as sess:
    for epoch in xrange(num_epoch):
        t1 = time.time()
        if mode==1: 
            _dataset = dataset.shuffle(num_data).batch(batch_size)
            iterator = _dataset.make_one_shot_iterator()
        elif mode==2:
            _dataset = dataset.batch(batch_size)
            iterator = _dataset.make_one_shot_iterator()
        elif mode==3: 
            iterator = dataset.make_one_shot_iterator()
        t2 = time.time()
        for i in xrange(num_data/batch_size):
            a = sess.run(iterator.get_next())
        t3 =time.time()
        print 'epoch %d make_iterator_time %.4f comsuming_time %.4f'%(epoch,t2-t1,t3-t2)

and the outputs:
epoch 0 make_iterator_time 0.0181 comsuming_time 0.1366
epoch 1 make_iterator_time 0.0036 comsuming_time 0.1444
epoch 2 make_iterator_time 0.0040 comsuming_time 0.1559
epoch 3 make_iterator_time 0.0036 comsuming_time 0.1695
epoch 4 make_iterator_time 0.0036 comsuming_time 0.1899
epoch 5 make_iterator_time 0.0036 comsuming_time 0.1955
epoch 6 make_iterator_time 0.0036 comsuming_time 0.2082
epoch 7 make_iterator_time 0.0037 comsuming_time 0.2191
epoch 8 make_iterator_time 0.0036 comsuming_time 0.2334
epoch 9 make_iterator_time 0.0040 comsuming_time 0.2461
epoch 10 make_iterator_time 0.0036 comsuming_time 0.2621
epoch 11 make_iterator_time 0.0036 comsuming_time 0.2720
epoch 12 make_iterator_time 0.0036 comsuming_time 0.2886
epoch 13 make_iterator_time 0.0036 comsuming_time 0.3006
epoch 14 make_iterator_time 0.0037 comsuming_time 0.3134
epoch 15 make_iterator_time 0.0039 comsuming_time 0.3260
epoch 16 make_iterator_time 0.0445 comsuming_time 0.3438
epoch 17 make_iterator_time 0.0037 comsuming_time 0.3576
epoch 18 make_iterator_time 0.0037 comsuming_time 0.3678
epoch 19 make_iterator_time 0.0040 comsuming_time 0.3827
epoch 20 make_iterator_time 0.0037 comsuming_time 0.3937
epoch 21 make_iterator_time 0.0038 comsuming_time 0.4172
epoch 22 make_iterator_time 0.0036 comsuming_time 0.4222
...

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X 10.12.5
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.4
Python version:  2.7
Bazel version (if compiling from source): 0.7.0
GCC/Compiler version (if compiling from source): clang-802.0.42
CUDA/cuDNN version: 9.0/7
GPU model and memory: GTX1080 8G
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"