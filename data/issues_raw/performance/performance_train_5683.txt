Synchronous distributed training is too slow

While trying to train Imagenet using Inceptionv3, I noticed the training was proceeding too slow. I then tested with Alexnet and Resnet and I saw the same slow training speed.
Here is the images/sec I'm able to achieve on various models:
Inceptionv3:
# GPU, Images/sec
1,        17.525
2,        34.568
4,        62.580
8,        94.832
16,       99.072
32,       140.704
64,       183.488
128,      322.816

Alexnet:
1,        92.464
2,        189.736
4,        344.336
8,        532.40
16,       710.224
32,       878.944
64,       848.128
128,      874.624

Resnet 152:
1,        10.567
2,        20.224
4,        32.332
8,        37.952
16,       42.416
32,       66.016
64,       95.168
128,      152.192


Is it possible there is some bug causing this slow performance?
I'm using EC2 P2 instances (p2.16xlarge). Each instance has 16 GPUs. Each GPU is a Tesla K80. I'm running one worker per GPU and one PS per host. Here is more info on P2 instances: https://aws.amazon.com/ec2/instance-types/p2/
I use randomly generated synthetic data for images and label. I've written some scripts to reproduce this issue in case it helps: https://github.com/indhub/tfperftest/tree/master/perftest

Inception model is based on https://github.com/tensorflow/models/tree/master/inception. I've done some changes to use randomly generated synthetic data for images and labels. Modified code here: https://github.com/indhub/tfperftest/tree/master/inception
Resnet model is based on https://github.com/tensorflow/models/tree/master/resnet. I've done some changes to build Resnet 152. Modified code here: https://github.com/indhub/tfperftest/tree/master/resnet
Alexnet model is based on https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py. I've added the fully connected layers at the end. Modified code here: https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py

What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow
Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
ubuntu@ip-172-31-52-161:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root 415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root 775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a


If installed from binary pip package, provide:

A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".
ubuntu@ip-172-31-52-161:~$ python -c "import tensorflow; print(tensorflow.version)"

I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0rc2


If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Just running https://github.com/tensorflow/models/tree/master/inception reproduces the problem.