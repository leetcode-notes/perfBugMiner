Optimized compiled Tensorflow uses almost 2x more memory than non-optimized binary

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): Both
TensorFlow version (use command below): ('v1.0.0-65-g4763edf-dirty', '1.0.1') (compiled at the same git commit of the binary version)
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: N/A (CPU only)
GPU model and memory: N/A
Exact command to reproduce:

import pdb
import numpy as np
import tensorflow as tf

def get_layer(input_size, output_size, name):
    # W_val is loaded from a file using numpy.load
    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)
    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),
                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),
                        dtype=tf.float32)
    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),
                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),
                        dtype=tf.float32)
    return W, b

sessions = []
for i in range(3):
    g = tf.Graph()
    with g.as_default():
        W1, b1 = get_layer(158238, 900, '1')
        W2, b2 = get_layer(900, 1000, '2')
        W3, b3 = get_layer(1000, 1, '3')

        init = tf.global_variables_initializer()
    session = tf.Session(graph=g)
    session.run(init)
    print 'Loaded {}'.format(i)
    sessions.append(session)

pdb.set_trace()
Describe the problem
After running the code snippet under the non-optimized binary Tensorflow installation, the used memory is ~6GB. However, when the same snippet is run with Tensorflow compiled with -c opt --copt=-march=native directives, the memory usage is ~11GB (1.8x larger).
Note that there's no memory-usage difference when I use tf.truncated_normal(stddev=0.1...) instead of tf.constant_initializer with a numpy array.
Not sure if this is a bug, or a side-effect of the optimized version or if there's something I can do to optimize memory usage?
Source code / logs
I can attach chrome traces if necessary.