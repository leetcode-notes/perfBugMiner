Feature: evaluating multiple datasets in tf.estimator.Estimator.evaluate without reloading checkpoint

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora release 25 (Twenty Five)
TensorFlow installed from (source or binary): binary for CPU
TensorFlow version (use command below): v1.6.0-0-gd2e24b6039
Python version: 3.5.5
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

Describe the problem
We have several datasets on which we want to track performance during training, corresponding to multiple data splits or different types of data that are processed by the model. To do this we run Estimator.evaluate for each dataset after each epoch of training.
The problem with this is that Estimator.evaluate reconstructs the graph and loads the variables from checkpoint each time that it is called. In our case, reconstructing the large graph takes longer than doing the evaluation on the relatively small datasets. I'd propose a feature to allow evaluating on multiple datasets without reloading the graph.
Ideas:

Add an Estimator.evaluate_multiple method that takes a mapping from name to input_fn.
Add functionality to explicitly start an evaluation session for an estimator and allow passing this as input to Estimator.evaluate

Source code / logs
model = Estimator(my_model_fn)
for epoch in range(n_epochs):
  estimator.train(train_input_fn, steps=n_steps_per_epoch)
  for name, eval_input_fn in eval_datasets:
    # eval_input_fn create one shot iterators
    estimator.evaluate(eval_input_fn, name=name)