Distributed training hangs

As mentioned in #4651, during training with syncreplicasoptimizer, tensorflow must run into hanging(after thousands steps). This does not happen with simple network setup.
Both workers and pses are still "alive", no errors/exceptions are reported. The saver thread keeps working.
Environment info
the cluster is set on 2 servers, one ps and one worker on each. And each worker works on 4 gpu cards.
Operating System:
Centos (customized)
Installed version of CUDA and cuDNN:
CUDA 7.5
cnDNN 5.1
installed from source,
build on branch r0.11, and r0.10
trace log after hanging
ps0_pstack.txt
ps0_strace.txt
ps1_pstack.txt
ps1_strace.txt
worker0_pstack.txt
worker0_strace.txt
worker1_pstack.txt
worker1_strace.txt