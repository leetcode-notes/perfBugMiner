docs: batch normalization usage in slim

How to use batch normalization in the testing phase?
I tried to use batch normalization to train a model like this:
bn = lambda x: slim.batch_norm(x, is_training=is_training)
conv = slim.conv2d(images, 64, [3, 3], 1, normalizer_fn=bn, padding='SAME', scope='conv')

But when I finished training and restored my model from checkpoint files, the model's performance on the testing set was poor, just like random guessing.
If these parameters are not dumped as model variables, is it possible to make an example to illustrate how to use batch normalization in slim, esp. for inference?