Running Bidirectional RNN cells on parallel

is there any method to run Bidirectional RNNs on parallel as they should (they have no dependency on each other)
I noticed that stacking 2 LSTM layer has the exact same performance as bidirection LSTM (BLSTM)
even 4 stacked LSTM has same performance as 2 parallel BLSTMs (which simply means  that TF runtime doesn't parallelize BLSTMs at all, I also notice the same behavior on both the CPU and the GPU)
Note: I am using small cells that are trivial to fit simultaneously
Thanks,