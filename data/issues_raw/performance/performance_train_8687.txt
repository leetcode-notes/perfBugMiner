[HDFS]OutOfRangeError cause java runtime error

Environment info
TensorFlow: v1.0.0 and v1.0.1
JDK: 1.8.0_111
Hadoop: 2.6.0
GCC: 4.8.2
OutOfRangeError
I think the tf.errors.OutOfRangeError is a bug, at least, if you want to read data(tfrecords, csv, etc) from hdfs with readers and queues.
How to reproduce
I followed the guide of reading data.
I used the recommended mnist example here.
Firstly, convert MNIST dataset by this script: convert_to_records.py
Secondly, put the tfrecords of MNIST dataset into your hdfs. For example, assume the url is hdfs://host:port/tfrecords/mnist-data/.
Then, train the MNIST network by this script: fully_connected_reader.py. And feed the train_dir argument with hdfs://host:port/tfrecords/mnist-data/.
Finally, you can get the train log and error log as below:
Train log
Step 0: loss = 2.29 (1.795 sec)
Step 100: loss = 1.99 (0.045 sec)
Step 200: loss = 1.62 (0.045 sec)
Step 300: loss = 1.39 (0.043 sec)
Step 400: loss = 1.01 (0.048 sec)
Step 500: loss = 0.77 (0.043 sec)
Step 600: loss = 0.65 (0.045 sec)
Step 700: loss = 0.60 (0.043 sec)
Step 800: loss = 0.65 (0.043 sec)
Step 900: loss = 0.46 (0.043 sec)
Error log
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f16a2a39660, pid=43645, tid=0x00007f16a2eff740
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpython2.7.so.1.0+0x134660]  visit_decref+0x0
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/mind/projects/tf_hdfs/hs_err_pid43645.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
Aborted (core dumped)
Ask for a better way
I know it's hard to avoid the OutOfRangeError because the multithreads of readers. But can we have a better way to read? Especially when you want to read data from hdfs, it's inconvenient and difficult to catch the exception.