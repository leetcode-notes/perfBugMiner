Eigen implemented CPU op is 10 times slower than OpenMP

I implemented a phase CPU operator consisting of four loop levels. I couldn't find any Eigen tensor docs at that stage so I use OpenMP to trivially parallelise the outer loops. I recently found the Eigen tensor documentation so I thought I'd take advantage of it and get all the multithreading/AVX/SSE goodies for free!
Unfortunately the Eigen version is about 10 times slower!
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Environment info
Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root 558720 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root 415432 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root 775162 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a

$ ls -l /usr/local/cudnn-5.1-cuda-8.0/lib64/lib*
lrwxrwxrwx 1 root root       13 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn_static.a

If installed from binary pip package, provide:

A link to the pip package you installed: python 2.7 linux GPU nightly
The output from python -c "import tensorflow; print(tensorflow.__version__)".

$ python -c "import tensorflow; print(tensorflow.__version__)"
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so locally
0.11.0rc1

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Source code.
Makefile. In terms of optimisations I'm using -O2 and -fopenmp
Test Case and timing code.

OpenMP

Timings

Tensorflow custom GPU time 0.342187
Tensorflow expression GPU time 0.270267
Tensorflow CPU time 0.417076
Numpy CPU time 2.542890


Code

// Compute the complex phase
#pragma omp parallel for
for(int src=0; src<nsrc; ++src)
{
    FT l = lm(src,0);
    FT m = lm(src,1);
    FT n = std::sqrt(1.0 - l*l - m*m) - 1.0;

    for(int time=0; time<ntime; ++time)
    {
        for(int antenna=0; antenna<na; ++antenna)
        {
            FT u = uvw(time,antenna,0);
            FT v = uvw(time,antenna,1);
            FT w = uvw(time,antenna,2);

            FT real_phase_base = minus_two_pi_over_c*(l*u + m*v + n*w);

            for(int chan=0; chan<nchan; ++chan)
            {
                // Our real phase input to the exponential function is purely imaginary so we can
                // can elide a call to std::exp<complex<FT>> and just compute the cos and sin
                FT real_phase = real_phase_base*frequency(chan);
                complex_phase(src,time,antenna,chan) = { std::cos(real_phase), std::sin(real_phase) };
            }
        }
    }
}
Eigen

Timings

Tensorflow custom GPU time 0.344653
Tensorflow expression GPU time 0.275525
Tensorflow CPU time 9.616667
Numpy CPU time 2.505482


Code

// Doing it this way might give us SIMD's and threading automatically...
const CPUDevice & device = context->eigen_device<CPUDevice>();

// Shapes for reshaping and broadcasting
Eigen::DSizes<int, 4>   lm_shape(nsrc, 1,     1,  1    );
Eigen::DSizes<int, 4>  uvw_shape(1,    ntime, na, 1    );
Eigen::DSizes<int, 4> freq_shape(1,    1,     1,  nchan);

auto l = lm.slice(
        Eigen::DSizes<int, 2>(0,    0),
        Eigen::DSizes<int, 2>(nsrc, 1))
    .reshape(lm_shape);
auto m = lm.slice(
        Eigen::DSizes<int, 2>(0,    1),
        Eigen::DSizes<int, 2>(nsrc, 1))
    .reshape(lm_shape);

auto u = uvw.slice(
        Eigen::DSizes<int, 3>(0,     0,  0),
        Eigen::DSizes<int, 3>(ntime, na, 1))
    .reshape(uvw_shape);

auto v = uvw.slice(
        Eigen::DSizes<int, 3>(0,     0,  1),
        Eigen::DSizes<int, 3>(ntime, na, 1))
    .reshape(uvw_shape);

auto w = uvw.slice(
        Eigen::DSizes<int, 3>(0,     0,  2),
        Eigen::DSizes<int, 3>(ntime, na, 1))
    .reshape(uvw_shape);

// Compute n
auto n = (l.constant(1.0) - l*l - m*m).sqrt() - l.constant(1.0);

// Compute the real phase
auto real_phase = (
    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +
    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +
    n.broadcast(uvw_shape)*w.broadcast(lm_shape))
        .broadcast(freq_shape);

// Reshape and broadcast frequency to match real_phase
auto f = frequency.reshape(freq_shape).broadcast(
    Eigen::DSizes<int, 4>(nsrc, ntime, na, 1));

// Calculate the phase
auto phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);
auto sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op<FT>());
auto cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op<FT>());

// Now evaluate the complex phase on the device
// by combining the cosine and sine of the phase
// to form a complex number
complex_phase.device(device) = cosp.binaryExpr(
    sinp, make_complex_functor<FT>());
What other attempted solutions have you tried?
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).