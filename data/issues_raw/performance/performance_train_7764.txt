native Cuda kernel for tf.dynamic_stitch op

Correspond to issue #7251
Here is the comparison between
my implementation:

and current implementation:

generated by Nvidia visual profiler with this code. Since the kernel uses CudaDeviceArrayStruct, which depends on code from //tensorflow/core:lib, I can't generate custom_op library and use the custom_op for performance comparison. I ended up manually switch the tensorflow directory python uses between runs
Currently, I still have two problems:

For some reason, tensorflow won't use the GPU kernel automatically, I had to force it with tf.device. What's even worse, tensorflow seems to refuse loading the GPU kernel in test_session() even if I tried to force it with tf.device, so this code is currently not tested with dynamic_stitch_test.py.
2. As @yaroslavvb suggested, Cuda kernels need to be real-valued, so I ended up calling TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_DYNAMIC_STITCH_GPU); to register the kernel. However, I feel like this will break some people's current code if their input data is from the unsupported data type. fixed in the latest commit.

Finally, this implementation is really under-optimized for cases where the size of each input data slice is small, because the indices are still processed by the CPU (plus CUDA reads data in warps of 32 threads). However, I still don't think it will be worse than processing everything on CPU then copy them back to GPU though.