Memory Leak from Deep Learning Training Step? (Finalized Graph)

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, although my code is somewhat based on the MNIST deep learning tutorial.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 14.04 VERSION="14.04.5 LTS, Trusty Tahr"
TensorFlow installed from (source or binary):
binary (I think, not 100% sure since it's been a few months since install. How can I check?)
TensorFlow version (use command below):
('v0.11.0-2614-g14aeb08-dirty', '0.12.0-rc0')
Bazel version (if compiling from source):
CUDA/cuDNN version:
CUDA Version 8.0.44
GPU model and memory:
GeForce GTX 780M 4GB
Exact command to reproduce:
self.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})

Describe the problem
I apologize if this is not an actual bug; I am relatively new to TensorFlow. I have posted on StackOverflow here and the only response I have gotten suggests filing a bug report here.
If I comment the sess.run line above, and only that line, out (but still do all my pre-processing and validation/testing and such for a few thousand training batches), the memory leak does not happen.
The leak is on the order of a few GB per hour (I am running Ubuntu, and have 16GB RAM + 16GB swap; the system becomes very laggy and unresponsive after 1-3 hours of running, when about 1/3-1/2 the RAM is used, which is a bit weird to me since I still have lots of RAM and the CPU is mostly free when this happens...)
Here is some of the initializer code (only run once, at the beginning) if it is relevant:

    with tf.name_scope('after_final_layer') as scope:
        self.layer1 = weights["wc1"]
        self.y_conv = network(self.x, weights, biases, self.keepratio)['out']
        variable_summaries(self.y_conv)
        # Note: Don't add a softmax reducer in the network if you are going to use this
        # cross-entropy function
        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.y_conv, self.y_true, name = "softmax/cross_ent"), name = "reduce_mean")
        self.train_step = tf.train.AdamOptimizer(learning_rate, name = "Adam_Optimizer").minimize(self.cross_entropy)

        self.prediction = tf.argmax(self.y_conv, 1)
        self.correct_prediction = tf.equal(self.prediction, tf.argmax(self.y_true, 1))

        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))

        if tensorboard:
            # Merge all the summaries and write them out to the directory below
            self.merged = tf.summary.merge_all()
            self.my_writer = tf.summary.FileWriter('/home/james/PycharmProjects/AI_Final/my_tensorboard', graph=self.sess.graph)

        # self.sess.run(tf.initialize_all_variables()) #old outdated way to do below
        tf.global_variables_initializer().run(session=self.sess)
        self.sess.graph.finalize() #make sure nothing new is added to graph


Notice that I have finalized the graph, so nothing new should be added to it.
Am I doing something wrong/is this expected behavior, or is this a real bug?
I have attached the source code as well (two .py files in directory).  Note: I am happy put in the work to reduce the source to a minimal recreation of the bug, but first I'd like verification that 1) this would be helpful (i.e. that the above info is not enough) and that 2) this is probably a bug, and not just an obvious beginner mistake on my part.
Thank you in advance.
source.zip