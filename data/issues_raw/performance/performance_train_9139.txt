Incorrect results when graph is split across several GPUs.

Background info:

Custom code
Tensorflow r1.0 installed from binaries on Windows
CUDA 8.0, cuDNN 5.1.5
2 GPUs: GTX Titan X and Titan X Pascal

Problem:
I have a model that is small enough to be trained on a single GPU with 12GB memory. Training works fine and converges.
However, when I evaluate the model with a validation set that is too large to fit on one of my GPUs, TensorFlow seems to use both of my GPUs (one GTX Titan X and one Titan X Pascal). When this happens, a large fraction of the results returned by TensorFlow are incorrect. The returned values are not completely missing, i.e. not all zero or something like that, but so inaccurate that the validation performance is terrible.
More specifically, my model consists of a shared initial stage, followed by a list of ~50 sub-networks that all receive input from the shared stage, but are otherwise independent. Data is split using tf.dynamic_partition(). From the results that I get, it appears that TensorFlow moves some of the 50 sub-networks to the second GPU if the memory on the first one isn't sufficient. The moved sub-models then return incorrect results (the others are unchanged).
If I instead force evaluation to be performed on the CPU, all results are as expected. All I need to do is add with tf.device('/cpu:0') to the very top of my script. The results also look good if I reduce the size of the validation set so that it fits onto one GPU.
I am sorry for not providing a working example. I will try to create one, but it might take a while since, by nature of the problem, it needs to be a fairly large/complex model.