poor performance when XLA works with dynamic control_flow ops

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.2.1
Python version:  2.7.14
Bazel version (if compiling from source): 0.4.5
GCC/Compiler version (if compiling from source): 4.8.5
CUDA/cuDNN version: 8.0/5.1.10
GPU model and memory: M40
Exact command to reproduce:

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the problem
When XLA works with:
1, dynamic_rnn
2, static_rnn with "dynamic calculation" enabled, specificaly, when the seq_len is assigned.
In these cases, the performance of XLA is poor, even result in negative performance optimization.
From the time line it seems that the switch/merge ops are breaking the XLA fused instructions into pieces. But i still don't understand why it leads to negative optimization.
Pls let me know if this is a known issue, are there any special reasons for XLA not to support control flow ops? or if there's anything i can do to fix it. Thanks.
Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.