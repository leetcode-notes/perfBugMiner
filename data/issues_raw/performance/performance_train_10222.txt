tf.nn.moments with tf.concat numerical ambiguity

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code, see below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTE
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
Bazel version (if compiling from source): n/a
CUDA/cuDNN version: CUDA: 8.0 / cuDNN: 5.1
GPU model and memory: GTX960m 4GB and GTX1080 8GB
Exact command to reproduce: run the code below

Describe the problem
tf.nn.moments GPU version produces different mean and variance values for numerically same input tensors. The only difference between the inputs is that they are the outputs of one and two tf.concat operations (see the code below). CPU version works well.
The bad output is:
input diff:0.0
mean diff:2.98023223877e-08
var diff:7.45058059692e-09

The correct output should be:
input diff:0.0
mean diff:0.0
var diff:0.0

Source code / logs
import numpy as np
import tensorflow as tf

with tf.device("/gpu:0"):
    input = tf.placeholder(shape=[16, 4, 4, 8], dtype=tf.float32)
    input1 = tf.concat([input], axis=0)
    input2 = tf.concat([tf.concat([input], axis=0)], axis=0)

    mean1, var1 = tf.nn.moments(input1, axes=[0,1,2])
    mean2, var2 = tf.nn.moments(input2, axes=[0,1,2])

    input_diff_max = tf.reduce_max(tf.abs(input1 - input2))
    mean_diff_max = tf.reduce_max(tf.abs(mean1 - mean2))
    var_diff_max = tf.reduce_max(tf.abs(var1 - var2))

    with tf.Session() as sess:
        i_v, m_v, v_v = sess.run([input_diff_max, mean_diff_max, var_diff_max], feed_dict={input: np.random.rand(16, 4, 4, 8)})
        print("input diff:{}".format(i_v))
        print("mean diff:{}".format(m_v))
        print("var diff:{}".format(v_v))