Dataset.list_files is impractical for large number of files

I would like to use Dataset.list_files on a large dataset. The dataset is similar to ImageNet in that it's a big list of images nested in subdirectories.
I originally guessed that under the hood the api would use queues to start training early, but that isn't the case - the implementation walks the entire directory tree and loads the filenames into memory before the next operation starts.
This means you can wait tens of minutes before training starts.
Two things would help here:

Use queues under the hood so downstream ops can start immediately
Allow specifying a limit, eg. Dataset.list_files("**/*.jpg", limit=1000)

The latter is mostly useful for quicker iteration. The former needs more work due to the different backends for GetMatchingPaths.
Is this something being worked on already?
I am using os.scandir() as a workaround but the Dataset.list_files() is a more natural api for this task and should be faster due to not needing to pass the file names via feed_dict.