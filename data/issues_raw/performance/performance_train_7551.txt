Addition is much slower on non-last axis (non-fused batch norm with NCHW)

I noticed this from observing my models training many times slower when using non-fused batch norm and the NCHW data format. When looking at the timeline, it's dominated by addition (and multiplication) operations.
I can mostly work around this by using the fused batch norm, but DenseNet models in principle (and in practice when using NHWC here) benefit from splitting up the learned beta/gamma and the normalization steps from batch normalization, and using a straightforward implementation (included below) makes my model run significantly slower when using NCHW.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I did a quick search on the issue tracker and SO, and couldn't find anything similar reported.
Environment info
gcr.io/tensorflow/tensorflow:1.0.0-devel-gpu on AWS p2.xlarge
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Compare:
%%time

with tf.Graph().as_default(), tf.Session() as sess:
    zeros = tf.zeros((64, 32, 32, 256))
    beta = tf.get_variable(
        'beta',
        (256,),
        initializer=tf.ones_initializer(),
        trainable=True,
    )

    loss = tf.reduce_mean((zeros + beta) ** 2)
    optimizer = tf.train.MomentumOptimizer(0.1, 0.9)
    train_op = optimizer.minimize(loss)

    sess.run(tf.global_variables_initializer())

    for i in range(500):
        sess.run(train_op)
CPU times: user 7.8 s, sys: 868 ms, total: 8.67 s
Wall time: 9.69 s

v.
%%time

with tf.Graph().as_default(), tf.Session() as sess:
    zeros = tf.zeros((64, 256, 32, 32))
    beta = tf.get_variable(
        'beta',
        (256,),
        initializer=tf.ones_initializer(),
        trainable=True,
    )
    beta = tf.reshape(beta, (256, 1, 1))

    loss = tf.reduce_mean((zeros + beta) ** 2)
    optimizer = tf.train.MomentumOptimizer(0.1, 0.9)
    train_op = optimizer.minimize(loss)

    sess.run(tf.global_variables_initializer())

    for i in range(500):
        sess.run(train_op)
CPU times: user 14.6 s, sys: 2.81 s, total: 17.4 s
Wall time: 18.9 s

This is the scale/bias transform that triggers the problem for me in practice:
def affine_transformation(
    inputs,
    axis=-1,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
):
    inputs_shape = inputs.get_shape()
    params_dim = inputs_shape[axis]

    beta = tf.get_variable(
        'beta',
        (params_dim,),
        initializer=beta_initializer,
        regularizer=beta_regularizer,
        trainable=True,
    )
    gamma = tf.get_variable(
        'gamma',
        (params_dim,),
        initializer=gamma_initializer,
        regularizer=gamma_regularizer,
        trainable=True,
    )

    if axis != -1:
        params_shape = [1] * len(inputs_shape)
        params_shape[axis] = params_dim.value

        beta = tf.reshape(beta, params_shape)
        gamma = tf.reshape(gamma, params_shape)

    return gamma * inputs + beta