Scale-out performance limitation for distributed session

Scale-out performance for scatter-gather dataflow pipelines is limited.
I use Tensorflow to build custom pipelines (i.e. I write my own OpKernels) of "embarrassingly parallel" problems (no coordination between "local" pipelines required). Typically these involve the local pipeline being replicated across all the machines in my cluster, and having a source and sink queue to feed input and receive output, respectively.
The issue: the performance degrades linearly with the number of nodes in the cluster, compared to a perfect scale-out performance.
I am not sure if this is a fundamental limitation of the distributed session, or if there is a way within Tensorflow to build such graphs better (with less coordination needed between independent local pipelines).
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Not much guidance. When speaking with other folks at conferences, it seems common to sidestep the distributed session in Tensorflow for custom, non-TF solutions (e.g. MPI). I personally use ZeroMQ to facilitate these sort of scatter-gather patterns.
Environment info

Operating System: Linux (Ubuntu 16.04, 4.4.0-22)
installed from pip following the default download and setup instructions
output of python -c "import tensorflow; print(tensorflow.__version__)" -> 0.12.1

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

here is a gist with vanilla tensorflow, with the versions I specified above. My local pipelines that I stamp out across all nodes in my cluster has custom ops, which I replicated in standard Tensorflow with a while loop to imitate the delay of my custom pipeline. I'm not sure if there is a better way to simulate the delay.
here is the data I collected on my cluster of 10 machines (only up to 9 to keep the source/sink queue machine unloaded). Note that the workload scales up linearly with only changes with the number of nodes; if the scale-out was perfect, the time should be the same regardless of the number of nodes.

What other attempted solutions have you tried?

I get around this by limitation by using ZeroMQ as a higher-performance substitute for the source/sink queues (the equivalent of "source_queue" and "sink_queue" in the example script)