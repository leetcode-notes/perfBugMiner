Poor performance with multi-GPU

I tried to train the example inception-v3 network on multiple GPUs. The following commands are used:
/* use 4 GPUs */
bazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=64 --max_steps=2000 --train_dir=... --data_dir=...
/*use 2 GPUs */
export CUDA_VISIBLE_DEVICES=0,1
bazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --max_steps=2000 --train_dir=... --data_dir=...
/*use 1 GPU */
export CUDA_VISIBLE_DEVICES=0
bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=16 --max_steps=2000 --train_dir=... --data_dir=...
The performance results are as follows:
GPUs   Training time(s)       samples/sec
1           657                          52.7
2           844                          97.3
4           1104                        150
The training time was got from the log file by subtracting the first time from the last time and therefore excluded the start-up time. For example
2016-10-03 15:28:23.239148: step 0, loss = 13.08 (4.6 examples/sec; 13.916 sec/batch)
....
2016-10-03 15:46:47.830959: step 1990, loss = 12.21 (174.6 examples/sec; 0.367 sec/batch)
Now the part that I do not understand is that the samples/sec scales normally with increasing number of GPUs, but the training time does not reduce. Instead the training also increases with more GPUs.
Has anyone observed the similar case?
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I tried to set --input_queue_memory_factor=0 as in the post tensorflow/models#47, but it does not help.
Also the post #4272 has similar performance issue, but it was not solved.
Environment info
Operating System: Redhat 7.2
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3
-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a
If installed from source, provide

The commit hash (git rev-parse HEAD): a63b0cb
The output of bazel version:

.
Build label: 0.3.1- (@non-git)
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Sep 29 22:19:27 2016 (1475187567)
Build timestamp: 1475187567
Build timestamp as int: 1475187567
Logs or other output that would be helpful
By the way, before the actual computation, the overhead of the initialization took >7minutes, is that normal? And I also got many warnings:
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)
WARNING:tensorflow:Passing a GraphDef to the SummaryWriter is deprecated. Pass a Graph object instead, such as sess.graph.
Will all these warnings impact the performance?