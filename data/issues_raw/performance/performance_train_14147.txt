tf.cast zeroes out input tensor when GPU does not have any free memory

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from (source or binary): tf-nightly-gpu binary from 10/31/2017
TensorFlow version (use command below): ('v1.3.0-rc1-4007-gc44f67a', '1.5.0-dev20171031')
Python version: 2.7.12
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: CUDA v8, cuDNN v6
GPU model and memory: 1080 Ti (11 GB)
Exact command to reproduce:

x = tf.cast(tensor, tf.float32)
x = tf.to_float(tensor)
Above commands return a tensor with all values of tensor set to zero WHEN:
The gpu is in full use by another tensorflow process. I'm trying to cast the tensor from dtype=tf.uint16 to tf.float32, using above commands, but whenever this runs while the gpu is in full use (i.e. I get a CUDA out-of-memory error), the program completes execution normally but the casting commands set the tensor to zeros.
I upgraded from an older nightly-gpu binary to the current one, but didn't help. User should not try to use an already in-use gpu, but this error should at least be communicated to the user.
If I set the gpu to a different gpu with available memory, or if I hide all gpus to force it to use CPU, I do not observe this issue.