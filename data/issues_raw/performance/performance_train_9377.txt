grpc error in distributed tensorlfow

pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0)
# E0422 12:13:21.315971987   26528 tcp_server_posix.c:148]     check for SO_REUSEPORT: {"created":"@1492834401.315943300","description":"OS Error","errno":92,"file":"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.c","file_line":181,"os_error":"Protocol not available","syscall":"setsockopt(SO_REUSEPORT)"}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:8865}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8866}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:8865
I try distributed tensorflow and start only one server and one worker, the model does not converge as the local version.