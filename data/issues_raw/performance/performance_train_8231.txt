memory leak when training complex neural networks

When training some complex (seq-to-seq) neural networks, the memory cost of my program will keep growing, and this only happens on GPU...(On CPU, everything is OK)
I used to report this problem in this issue: #6599
After that, I solved this problem by encapsulating my encoding method as an RNN cell, so this issue was closed, though no one knows the reason...
But now, this problem happens again, because I changed the structure of my network...
I do not think this is due to the bugs in my program, because it runs very well on CPU...