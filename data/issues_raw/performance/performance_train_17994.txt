tensorflow-gpu OOM with Geforce GTX980

Hello there!
Sorry for the noob question, I'm new in the world of deep learning, especially with GPU processing.
I'm trying to run Tensorflow-GPU but probably I'm doing something wrong.
I tried many scripts with always the same result without any solution.
I think I installed Cuda and cuDNN properly, as well tensorflow.
This is my system:
Ubuntu 16.04
GPU: NVIDIA Geforce GTX 980
RAM: 16 GB
Cuda 9.0, cuDNN 7.0.
conda environment in the example reported below:
python==3.6
pathlib==1.0.1
scandir==1.6
h5py==2.7.1
Keras==2.1.2
opencv-python==3.3.0.10
tensorflow-gpu==1.5.0
scikit-image
dlib
face_recognition
tqdm
and basically this is what I get everytime I try to start some kind of training. In this example faceswap.py training with https://github.com/deepfakes/faceswap.
I can't see any change on nvidia-smi while this is going on, so I think the GPU is not actually used.
Thank you in advance for any kind of help, I'm going mad about this :(
Loading Trainer from Model_Original plugin...
Starting. Press "Enter" to stop training and save model
2018-03-26 02:29:53.146441: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-26 02:29:53.146998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-26 02:29:53.147230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.291
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 2.72GiB
2018-03-26 02:29:53.147248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2)
2018-03-26 02:29:54.731329: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.21GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-03-26 02:29:54.788791: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.20GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-03-26 02:29:54.832066: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 288.00MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-03-26 02:29:54.832100: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.29GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-03-26 02:29:54.837129: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 220.50MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-03-26 02:30:04.837415: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 3.00MiB**.  Current allocation summary follows.
2018-03-26 02:30:04.837533: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (256): 	Total Chunks: 49, Chunks in use: 48. 12.2KiB allocated for chunks. 12.0KiB in use in bin. 504B client-requested in use in bin.
2018-03-26 02:30:04.837564: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (512): 	Total Chunks: 6, Chunks in use: 6. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 3.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837593: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1024): 	Total Chunks: 13, Chunks in use: 13. 13.2KiB allocated for chunks. 13.2KiB in use in bin. 13.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837620: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2048): 	Total Chunks: 12, Chunks in use: 12. 25.5KiB allocated for chunks. 25.5KiB in use in bin. 24.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837646: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4096): 	Total Chunks: 17, Chunks in use: 17. 68.0KiB allocated for chunks. 68.0KiB in use in bin. 68.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837673: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8192): 	Total Chunks: 6, Chunks in use: 6. 48.0KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837700: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16384): 	Total Chunks: 9, Chunks in use: 8. 168.8KiB allocated for chunks. 150.0KiB in use in bin. 150.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837727: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (32768): 	Total Chunks: 6, Chunks in use: 6. 225.0KiB allocated for chunks. 225.0KiB in use in bin. 225.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837755: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (65536): 	Total Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837779: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-03-26 02:30:04.837817: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (262144): 	Total Chunks: 1, Chunks in use: 1. 256.0KiB allocated for chunks. 256.0KiB in use in bin. 256.0KiB client-requested in use in bin.
2018-03-26 02:30:04.837854: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (524288): 	Total Chunks: 1, Chunks in use: 0. 597.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-03-26 02:30:04.837892: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1048576): 	Total Chunks: 6, Chunks in use: 6. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.
2018-03-26 02:30:04.837931: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2097152): 	Total Chunks: 8, Chunks in use: 8. 23.62MiB allocated for chunks. 23.62MiB in use in bin. 22.75MiB client-requested in use in bin.
2018-03-26 02:30:04.837971: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4194304): 	Total Chunks: 13, Chunks in use: 13. 58.25MiB allocated for chunks. 58.25MiB in use in bin. 53.62MiB client-requested in use in bin.
2018-03-26 02:30:04.838012: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8388608): 	Total Chunks: 12, Chunks in use: 12. 123.00MiB allocated for chunks. 123.00MiB in use in bin. 123.00MiB client-requested in use in bin.
2018-03-26 02:30:04.838046: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16777216): 	Total Chunks: 13, Chunks in use: 13. 222.00MiB allocated for chunks. 222.00MiB in use in bin. 222.00MiB client-requested in use in bin.
2018-03-26 02:30:04.838075: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (33554432): 	Total Chunks: 11, Chunks in use: 11. 461.50MiB allocated for chunks. 461.50MiB in use in bin. 442.00MiB client-requested in use in bin.
2018-03-26 02:30:04.838100: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (67108864): 	Total Chunks: 23, Chunks in use: 23. 1.50GiB allocated for chunks. 1.50GiB in use in bin. 1.47GiB client-requested in use in bin.
2018-03-26 02:30:04.838124: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-03-26 02:30:04.838147: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-03-26 02:30:04.838172: I tensorflow/core/common_runtime/bfc_allocator.cc:644] Bin for 3.00MiB was 2.00MiB, Chunk State: 
2018-03-26 02:30:04.838203: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0000 of size 1280
2018-03-26 02:30:04.838229: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0500 of size 256
2018-03-26 02:30:04.838254: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0600 of size 256
[...]
2018-03-26 02:30:04.842321: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7949c2300 of size 67108864
2018-03-26 02:30:04.842337: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7989c2300 of size 67108864
2018-03-26 02:30:04.842353: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79c9c2300 of size 3145728
2018-03-26 02:30:04.842371: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79ccc2300 of size 79944960
2018-03-26 02:30:04.842390: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ed32200 of size 19200
2018-03-26 02:30:04.842407: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72edaed00 of size 611328
2018-03-26 02:30:04.842423: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ee45700 of size 256
2018-03-26 02:30:04.842442: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size: 
2018-03-26 02:30:04.842469: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 48 Chunks of size 256 totalling 12.0KiB
2018-03-26 02:30:04.842490: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 512 totalling 3.0KiB
2018-03-26 02:30:04.842509: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 12 Chunks of size 1024 totalling 12.0KiB
2018-03-26 02:30:04.842527: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.2KiB
2018-03-26 02:30:04.842547: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 11 Chunks of size 2048 totalling 22.0KiB
2018-03-26 02:30:04.842565: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 3584 totalling 3.5KiB
2018-03-26 02:30:04.842585: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 17 Chunks of size 4096 totalling 68.0KiB
2018-03-26 02:30:04.842604: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 8192 totalling 48.0KiB
[...]
2018-03-26 02:30:04.843025: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 7 Chunks of size 75497472 totalling 504.00MiB
2018-03-26 02:30:04.843044: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 79944960 totalling 76.24MiB
2018-03-26 02:30:04.843062: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 2.38GiB
2018-03-26 02:30:04.843086: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: 
Limit:                  2555510784
InUse:                  2554880000
MaxInUse:               2554880256
NumAllocs:                     303
MaxAllocSize:            364109056

2018-03-26 02:30:04.843151: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ****************************************************************************************************
2018-03-26 02:30:04.843199: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: **OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc**
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1350, in _do_call
    return fn(*args)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1329, in _run_fn
    status, run_metadata)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format="NHWC", dilations=[1, 1, 1, 1], padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1608_loss/mul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py", line 181, in processThread
    raise e
  File "/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py", line 161, in processThread
    trainer.train_one_step(epoch, self.show if (save_iteration or self.save_now) else None)
  File "/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Trainer.py", line 26, in train_one_step
    loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/training.py", line 1839, in train_on_batch
    outputs = self.train_function(ins)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2357, in __call__
    **self.session_kwargs)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format="NHWC", dilations=[1, 1, 1, 1], padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1608_loss/mul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'model_2/conv2d_9/convolution', defined at:
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py", line 147, in processThread
    model = PluginLoader.get_model(trainer)(get_folder(self.arguments.model_dir), self.arguments.gpus)
  File "/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/AutoEncoder.py", line 16, in __init__
    self.initModel()
  File "/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Model.py", line 22, in initModel
    self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py", line 603, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py", line 2061, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py", line 2212, in run_internal_graph
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/layers/convolutional.py", line 164, in call
    dilation_rate=self.dilation_rate)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3195, in conv2d
    data_format=tf_data_format)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 754, in convolution
    return op(input, filter)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 838, in __call__
    return self.conv_op(inp, filter)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 502, in __call__
    return self.call(inp, filter)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 190, in __call__
    name=self.name)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 639, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
[[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format="NHWC", dilations=[1, 1, 1, 1], padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

[[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1608_loss/mul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.