SparseApply* operators for the GPU

At the moment, it appears that only GradientDescentOptimizer supports running on the GPU when there is a SparseTensor update. This is particularly relevant for any RNNs which train their embeddings, including the word2vec example (yes w2v isn't an RNN ;)
So far the common workaround everyone seems to use is to force the embedding variables to be on the CPU, but there can be substantial speed improvements by allowing them to be stored on the GPU. For one, there is no need to transfer the vectors of embeddings to/from the GPU, and instead one can just transfer the embedding indexes, and then the gradients also don't need to be transferred backwards. In one test I've run where I implemented a version on the GPU, the difference in one epoch was roughly 1100s vs 300s. Basically anyone who doesn't freeze their embeddings can substantially benefit from this.
This is related to #1310 and #464. In #1310, there is a bug where variables are placed on the GPU, even though an op that appears later isn't available on the GPU. My understanding is a fix for this is under development, but it will only make sure variables are placed on devices which can perform the necessary ops. It will not actually include SparseApply's. This feature request would resolve the OP's reported bug in #1310, but not solve the variable-operator placement issue.
In #464, the issue is that no one has implemented any SparseApplyRMSProp, along with another bug that has since been resolved. The issue used to contain all optimizers except GDO, but since then SparseApply ops have been added for AdaGrad, yet they're currently only implemented for the CPU, not the GPU. This feature request is a rough superset of #464.
One thing that is presently unclear to me, is whether these really need to be implemented as C++ operators, with all the associated book keeping. Currently that is how the CPU version of the SparseApply*'s are updated (see example), except for GradientDescent. Its _sparse_apply is implemented in python and uses scatter_sub.
I already have a Python implementation of _sparse_apply for MomentumOptimizer, which uses tf.gather, tf.scatter_update, and tf.scatter_sub. It passes unit tests, and performs on both CPU and the GPU, though it does take about a 1% performance hit compared to the current CPU C++ SparseApplyMomentum. Would this be of interest, or is there a motivation behind implementing this in C++?