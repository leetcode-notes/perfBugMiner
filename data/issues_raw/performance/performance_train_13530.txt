Pandas_input_fn slow, starving CPU/GPU

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): It is a customized version of the Deep & Wide example code. Fairly close to original code.
**OS Platform and Distribution: Windows Server 2012 R2
**TensorFlow installed from: nightly build WHL through pip (this was tried after numerous other versions, including install through pip)
TensorFlow version (use command below): b'unknown' 1.4.0-dev20170926
Python version: 3.5 and 3.6
Bazel version (if compiling from source): Not compiling
CUDA/cuDNN version: CUDA 8, CUDnn 6.1
GPU model and memory: Tesla M60 GPU 8GB
Exact command to reproduce:  See attached Script.
-For the record, the server vm has 8 xeon physical cores and 240 gb ram allocated. The CPU only machine is a new skylake i7 with 32gb ram.

Describe the problem
To start, I submitted to stack overflow (https://stackoverflow.com/questions/46457476/tensorflow-pandas-input-fn-slow-starving-cpu-gpu) and have not been able to garner assistance after multiple edits to make sure it was framed correctly. I truly believe this is a bug since I am sticking so close to the example code, but if I have made a mistake I am deeply sorry to all of you.
I am working on a wide and deep model following the framework in the Tensorflow Wide and Deep tutorial (https://www.tensorflow.org/tutorials/wide_and_deep). Model works fine when built the old way (load entire dataset from pandas, convert to tensors, feed in input_fn) which is ok for running on a CPU.
However, to make it work on the GPU the dataset is too large to fit into GPU memory, so batching is necessary. I tried using the pandas_input_fn to batch data to the video card and noticed I get spikes of activity followed by long lulls while the next batch is prepared. The odd thing is, this happens even if I run it on a machine with CPU only. The lulls are almost the exact same length, so it isn't simply the video card crushing through a simple model faster than the proc can deliver it. It seems like it is always waiting to begin loading the next batch until the last one is done training.
(If this function simply cannot be used in this way, can we get an example of Deep and Wide using the dataset API? or a manual build of deep and wide using layers and queues? At the moment, the example code for the dataset api using make_one_shot_iterator for canned estimators doesn't run.)
I increased the complexity of the model to make sure it wasn't too easy to compute and still have the same issue. I have tried increasing the number of threads allocated to pandas_input_fn, I have tried increasing the queue size to far larger than seems reasonable (10x dataset size) which helps a bit, but not much. I am not sure if the slowdown is when it is queueing or de-queueing, but I have been unable to solve the issue after two weeks of troubleshooting. The data I am working with is 117 columns, 400k rows.
I have created a generic script that generates fake values to simulate the problem. However, there are far fewer fake columns than real ones, so the gap between steps is not nearly as long, but still noticeable. Code attached.
--
Source code / logs
attached
pandas_input_example.txt