GPU->CPU Memcpy failed Error or InternalError c2c fft failed Error when using FFT2D

The problem
I wrote a python script that used FFT2D from tensorflow you can find the Python script attached : TestFFT2D.py using GPU.
In this script, I first create a convolution apply to a tensor and than compute a loss between a reference input tensor and a variable one with ftt2d and ifft2d operations.
When I launch my script, I get a GPU->CPU Memcpy failed Error or an InternalError c2c fft failed Error. I don't know why the error change when I run again my script.
I test my code on two differents machine with Ubuntu 16.04 one with a GeForce GTX 680 GPU and one with a GeForce GTX 1080. In both case, I can randomly get both of the error messages.
Abandon.txt
InternalError.txt
Sometimes, my machine just crashes and I need to reboot it.
Source code / logs
You can find the terminal messages errors attached.
This is the TestFFT2D.py code : TestFFT2D.py 
`
import tensorflow as tf
import numpy as np
def get_loss(sess,net,img_ref,layer):
total_loss  = 0.

sess.run(net['input'].assign(img_ref))  
x = net[layer]
a = sess.run(net[layer])
x = tf.transpose(x, [0,3,1,2])
a = tf.transpose(a, [0,3,1,2])
_,N,_,_ = a.shape
F_x = tf.fft2d(tf.complex(x,0.))
F_x_conj = tf.conj(F_x)
F_a = tf.fft2d(tf.complex(a,0.))
F_a_conj = tf.conj(F_a)

for i in range(N):
    inter_corr_x = tf.multiply(F_x,F_x_conj)
    inter_corr_a = tf.multiply(F_a,F_a_conj)
    
    ifft2_corr_x = tf.ifft2d(inter_corr_x)
    ifft2_corr_a = tf.ifft2d(inter_corr_a)
    
    R_x = tf.real(ifft2_corr_x)
    R_a = tf.real(ifft2_corr_a)
    
    style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
    total_loss += style_loss
    
    # Shift the tensor from on 1 unit on the dimension 1
    F_x = tf.concat([tf.expand_dims(F_x[:,-1,:,:],0), F_x[:,:-1,:,:]], axis=1)
    F_a = tf.concat([tf.expand_dims(F_a[:,-1,:,:],0), F_a[:,:-1,:,:]], axis=1)
        
return(total_loss)

def main(args):
# Definition of the first operations :
height, width, numberChannels = 400,300,3
net = {}
current = tf.Variable(np.zeros((1, height, width, numberChannels), dtype=np.float32))
net['input'] = current
kernel = tf.constant(np.random.uniform(low=-1,high=1,size=(400,300,3,64)),dtype=np.float32)
conv = tf.nn.conv2d(current, kernel, strides=(1, 1, 1, 1),padding='SAME',name='conv')
bias = tf.constant(np.random.uniform(low=-1,high=1,size=(64)),dtype=np.float32)
conv_add_bias = tf.nn.bias_add(conv, bias)
net['conv1_1'] = conv_add_bias

img_ref = np.random.uniform(low=-128,high=128,size=(1, height, width, numberChannels))
init_img = np.random.uniform(low=-128,high=128,size=(1, height, width, numberChannels))

sess = tf.Session()

sess.run(net['input'].assign(img_ref))  
# Definition of the loss 
loss = get_loss(sess,net,img_ref,'conv1_1')
    
# Preparation of the assignation operation
placeholder = tf.placeholder(tf.float32, shape=init_img.shape)
assign_op = net['input'].assign(placeholder)
    
sess.run(tf.global_variables_initializer())
sess.run(assign_op, {placeholder: init_img})
print("Before loss evaluation")
loss_evaluation = sess.run(loss)
print("loss_evaluation",loss_evaluation)

return(0)

if name == 'main':
import sys
sys.exit(main(sys.argv))
`
System information

**Have I written custom code **: TestFFT2D.py
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from: Source
TensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1
CUDA/cuDNN version:
Cuda compilation tools, release 7.5, V7.5.17 and  cuDNN  5 with the GeForce GTX 680
Cuda compilation tools, release 8.0, V8.0.61and cuDNN 5 with the GeForce GTX 1080
GPU model and memory: Tested on GeForce GTX 680 with 2Go and on GeForce GTX 1080 with 12Go
** Python version**: Python 3.6.1 |Anaconda 4.4.0 (64-bit)|
Exact command to reproduce: python TestFFT2D.py : copy-paste the code above and run it with python 3