Supervisor: SummaryWriter and Saver stop after some time

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 / anaconda3 / python 3.6
TensorFlow installed from (source or binary): binary, (pip install tensorflow-gpu)
TensorFlow version (use command below): 1.2.0
Bazel version (if compiling from source): -
CUDA/cuDNN version: 5.1
GPU model and memory: 1080 / Titan X / K80
Exact command to reproduce: -

System information: tf_env.txt
Describe the problem
Hey guys!
I get the problem on 3 different machines (all on ubuntu 16.04 and the tensorflow 1.2). All experiments were executed on a single machine with a single GPU.
To initialize a session I use a tf.Supervisor and supervisor.managed_sessions() with the default
summary_writer and saver.  It works all well for up to 30mins - 1h30mis. But after that time the
summary_writer stops to write events, and the saver also stops to save the model parameters. However, the model still runs and produces valid outputs.
I also checked the python-log for tensorflow with level DEBUG. But all I got was some information logs, until it suddenly stops (see below).
Is there a way to track the SVSummaryThread / SVTimerCheckpointThread?
Thanks in advance and keep up the good work!
Cheers
Source code / logs
supervisor = tf.train.Supervisor(logdir=model_path, save_summaries_secs=60, global_step=model.global_step)
  sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True), allow_soft_placement=True)

  with supervisor.managed_session(config=sess_config) as sess, sess.as_default():
    step_time, loss = 0.0, 0.0
    epoch = 0

    dataset_size = reader.train_size
    epoch_time = time.time()
    avg_lm = 0.

    while epoch < config["max_epochs"]:
      sess.run(reader.iterator.initializer)

      while True:
        try:
          step_loss, step_lm = model.step(sess)
          avg_lm += step_lm / float(dataset_size) * float(model.batch_size)
          loss += step_loss / float(dataset_size) * float(model.batch_size)
        except tf.errors.OutOfRangeError:
          break

tensorflow python log:
017-06-21 15:57:33,386 - tensorflow - INFO - Starting queue runners.
2017-06-21 15:57:33,393 - tensorflow - INFO - global_step/sec: 0
2017-06-21 15:57:34,539 - tensorflow - INFO - Recording summary at step 0.
2017-06-21 15:58:33,385 - tensorflow - INFO - Recording summary at step 15264.
2017-06-21 15:58:33,391 - tensorflow - INFO - global_step/sec: 254.444
2017-06-21 15:59:33,384 - tensorflow - INFO - Recording summary at step 31239.
2017-06-21 15:59:33,391 - tensorflow - INFO - global_step/sec: 266.25
2017-06-21 16:00:33,385 - tensorflow - INFO - Recording summary at step 47206.
2017-06-21 16:00:33,391 - tensorflow - INFO - global_step/sec: 266.117
2017-06-21 16:01:33,385 - tensorflow - INFO - Recording summary at step 62157.
2017-06-21 16:01:33,391 - tensorflow - INFO - global_step/sec: 249.183
2017-06-21 16:02:33,384 - tensorflow - INFO - Recording summary at step 77621.
2017-06-21 16:02:33,391 - tensorflow - INFO - global_step/sec: 257.733
2017-06-21 16:03:33,383 - tensorflow - INFO - Recording summary at step 93657.
2017-06-21 16:03:33,391 - tensorflow - INFO - global_step/sec: 267.283
2017-06-21 16:04:33,391 - tensorflow - INFO - global_step/sec: 263.883
2017-06-21 16:04:33,545 - tensorflow - INFO - Recording summary at step 109493.
2017-06-21 16:05:33,391 - tensorflow - INFO - global_step/sec: 262.483
2017-06-21 16:05:33,558 - tensorflow - INFO - Recording summary at step 125242.
2017-06-21 16:06:33,383 - tensorflow - INFO - Recording summary at step 141072.
2017-06-21 16:06:33,391 - tensorflow - INFO - global_step/sec: 263.883
2017-06-21 16:07:33,384 - tensorflow - INFO - Recording summary at step 157265.
...