Optimized einsum

In Numpy, it's my understanding that the np.einsum function has been extended with the functionality of
opt_einsum,
which computes an optimal (or near-optimal) way to perform the tensor contractions.
As far as I've heard, the Tensorflow einsum is a lot more basic (but extremely convenient), and cannot be relied upon for performance. Also, even though opt_einsum can give allegedly perfect decomposition paths, these optimizations appear to not always work well in Tensorflow - which might be down to memory order, available numerical routines, views vs reshapes, etc.
As a feature request, I think it would be hugely beneficial to have a high-performing version of tf.einsum in Tensorflow, as it makes it easy to handle many complicated linear algebra tasks compactly.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.5
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A