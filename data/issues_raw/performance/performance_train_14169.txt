Tensorflow predicts odd results with insufficient GPU memory

Hi all,
I am currently having a problem that, when my code tries to initialize two or more predict instances in a GPU with insufficient memory, instead of throwing an OOM exception, the instances are initialized normally. However, when I try to predict an image with these instances, they produce weird results like [1.0, 0.0, 0.0, â€¦].
Is this a bug when two or more sessions try to race for limited amount of GPU memory?
Please see below for my system information and the exact steps to reproduce the problem. I would very much appreciate it if you could take a look.
Cheers,
Vincent
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution: CentOS Linux release 7.2.1511
TensorFlow installed from (source or binary): official binaries
TensorFlow version (use command below): 1.3.0/ 1.2.0/ 1.1.0
Python version: 2.7.5
CUDA/cuDNN version: CUDA 8.0.61, CUDNN 6.0
GPU model and memory: 1080 Ti (11172MB)
Exact steps to reproduce:


Download and uncompress the file below with two scripts;
Download the official inception_v1 imagenet pretrained model  from http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz
Try to occupy most of the memory in the GPU to run on - the exact amount of memory to occupy needs to be carefully tuned to reproduce the problem, in my case, I use pycuda to allocate 10720 out of the 11172MB of my 1080 Ti.
Run test_monitor.py in two separate command prompts, we should see weird results within a few minutes (the scripts stop when encounter such results).

reproduce_code.zip