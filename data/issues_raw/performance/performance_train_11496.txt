Turning on XLA JIT Compilation during session crashes computer?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.2.1
Python version:  2.7.12
Bazel version (if compiling from source): 0.5.2
CUDA/cuDNN version: 8.0/5.1
GPU model and memory: Titan X Pascal 12GB
Exact command to reproduce: Running any code with the following configuration passed to a supervisor managed session:

# Config to turn on JIT compilation
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1

with sv.managed_session(config=config) as sess:
...

Describe the problem
No idea why but when I tried to turn on the XLA JIT compilation according to this guide:
https://www.tensorflow.org/performance/xla/jit
My computer crashes immediately and I've no way of diagnosing the problem. Note that I installed TF from source and enabled the XLA JIT option. Does this option consume more power than normal?
Previously, I encountered a similar problem when I ran the TF-slim VGG model with a pip-installed TensorFlow-gpu r1.2, and the computer crashes simply. I could not find any issue until I ran the exact same code in another computer (a laptop with weaker power supply) with source-compiled tensorflow, which works. I then replaced the pip-installed TensorFlow with a source-compiled one, and everything works fine --- this led me to think there might be some issue with either my installation or the source code (although I can't verify).
Has anyone faced a similar problem? Also, is XLA activated by default if I configured it to be enabled during the source installation? i.e. it could be because I called the compilation twice when it was already activated by default, causing the system failure. Is there a way to verify whether XLA is activated - so far the only indication I see are the messages "XLA service 0x62bb180 executing computations on platform Host" and "XLA service 0x62a43b0 executing computations on platform CUDA". Is this sufficient, i.e. to say I do not have to manually activate XLA?