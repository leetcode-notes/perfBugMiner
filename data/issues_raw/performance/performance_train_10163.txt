Custom Poets Models Run Slow on Android

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android/Windows
TensorFlow installed from (source or binary): Binary (From android nightly
TensorFlow version (use command below): 1.2

Describe the problem
I've noticed that using a retrained inception model within the demo app, following the guidelines suggested, is awfully slow. Shouldn't the custom models generated in the style of Tensorflow for poets be pretty similar to the inception model that the demo comes with? I have noticed inference times to be around 5 times as slow on two devices. (Nexus 6P and Pixel C) compared to the original demo.
Even when the graphs are quantized I am getting no apparent performance increase (apart from model size). If anything it's actually slower.
Is this normal behaviour? I'm aware of the image size is different (224 vs 299) but is that enough to haemorrhage the performance?
Source code / logs
Avg. ms for Conv2D is 1366ms
Inference time ~1700ms (Pixel C) ~3500 (Nexus 6P)
Model building steps: Normal Model via Tensorflow for poets etc. --> strip nodes --> quantize --> replace in apk.
Same performance regardless of quantization.
@andrewharp this was what I referred to in the windows/android thread. Can move to s/o if preferred.