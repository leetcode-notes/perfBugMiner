feature request: shared memory concat with new allocation for subsequent operations

DenseNet is an effective network design that relies on applying nn layers on recursive concatenations of data along the channel axis. Unfortunately, this has the side effect of quadratic memory growth in TensorFlow as completely new blocks of memory are allocated after each concat operation, resulting in poor performance during all phases of execution.
This is a feature request for a new allocation='shared' option for operations such as tf.concat(allocation='shared') which works seamlessly with later operations that might modify the data such as BatchNorm, which might also need to share memory. This would make it is possible to utilize the Memory-Efficient Implementation of DenseNets, a paper which demonstrates that this memory utilization can be dramatically reduced through sharing of allocations. This image from the paper + pytorch implementation illustrates the shared memory approach:


Pytorch efficient DenseNet implementation
Keras DenseNet Implementation with "naive" allocations, works with TensorFlow backend.

This functionality would also be useful for any other application or future network design that employs recursive concatenations. Just in case I didn't find it in my search, perhaps a mechanism already exists that can meet these goals?