TensorFlow 60-80% slower than PyTorch on training Wide ResNet

cc @tfboyd
From #7187 (comment)
On an AWS p2.xlarge, using the tensorflow/tensorflow:1.0.1-devel-gpu Docker image as a base, I see ~270 ms per epoch while training a WRN-16-4 without dropout on CIFAR-10.
Using a PyTorch implementation from https://github.com/xternalz/WideResNet-pytorch, I see instead ~150 ms per epoch for the same.
My implementation of Wide ResNets uses NCHW and fused batch norm. It does use feed_dict for data loading, but I've observed with nvidia-smi that my GPU utilization stays near 100%.

To reproduce:

Clone https://github.com/4Catalyzer/dl-papers, and go to that directory.
Check out the benchmark branch.
Build the Docker image, which is based on the Docker hub image above:

$ docker build -t dl-papers .


Run the Docker image using NVIDIA Docker:

$ nvidia-docker run --rm -it dl-papers /bin/bash


Run the TF WRN-16-4 training:

# python -m dl_papers.wide_resnet.train cifar10


Observe the logged batch timings, then kill the process.
In the same Docker container up the PyTorch Wide ResNet example:

# cd ..
# pip install http://download.pytorch.org/whl/cu80/torch-0.1.11.post5-cp27-none-linux_x86_64.whl
# pip install torchvision tensorboard_logger
# git clone https://github.com/xternalz/WideResNet-pytorch.git
# cd WideResNet-pytorch


Run PyTorch training:

# python train.py --dataset cifar10 --layers 16 --widen-factor 4 -p 1


Observe logged batch timings.