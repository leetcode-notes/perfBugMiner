[tf.keras] Bug with Stateful Metrics & Fit Generator

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.8.0
Python version:  2.7
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: n/a

The stateful metrics integration with fit generator is not working. This can be demonstrated by taking the Keras metrics tests from the latest release and changing the imports to TensorFlow.keras (see https://github.com/keras-team/keras/blob/master/tests/keras/metrics_test.py):
import tensorflow as tf
from tensorflow.python.keras import backend as K

import numpy as np

class BinaryTruePositives(tf.keras.layers.Layer):
    """Stateful Metric to count the total true positives over all batches.
    Assumes predictions and targets of shape `(samples, 1)`.
    # Arguments
        name: String, name for the metric.
    """

    def __init__(self, name='true_positives', **kwargs):
        super(BinaryTruePositives, self).__init__(name=name, **kwargs)
        self.stateful = True
        self.true_positives = K.variable(value=0, dtype='int32')

    def reset_states(self):
        K.set_value(self.true_positives, 0)

    def __call__(self, y_true, y_pred):
        """Computes the number of true positives in a batch.
        # Arguments
            y_true: Tensor, batch_wise labels
            y_pred: Tensor, batch_wise predictions
        # Returns
            The total number of true positives seen this epoch at the
                completion of the batch.
        """
        y_true = K.cast(y_true, 'int32')
        y_pred = K.cast(K.round(y_pred), 'int32')
        correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')
        true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')
        current_true_pos = self.true_positives * 1
        self.add_update(K.update_add(self.true_positives,
                                     true_pos),
                        inputs=[y_true, y_pred])
        return current_true_pos + true_pos

# Test on simple model
inputs = tf.keras.Input(shape=(2,))
outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)
model = tf.keras.Model(inputs, outputs)

model.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['acc', BinaryTruePositives()])

samples = 1000
x = np.random.random((samples, 2))
y = np.random.randint(2, size=(samples, 1))

val_samples = 10
val_x = np.random.random((val_samples, 2))
val_y = np.random.randint(2, size=(val_samples, 1))

# Test fit and evaluate
history = model.fit(x, y, validation_data=(val_x, val_y), epochs=2, batch_size=10)
outs = model.evaluate(x, y, batch_size=10)
preds = model.predict(x)

def ref_true_pos(y_true, y_pred):
    return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))

# Test correctness (e.g. updates should have been run)
np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)

# Test correctness of the validation metric computation
val_preds = model.predict(val_x)
val_outs = model.evaluate(val_x, val_y, batch_size=10)
np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)
np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)

# Test with generators
gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(x, y)]
val_gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(val_x, val_y)]
history = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,
                              validation_data=iter(val_gen), validation_steps=val_samples)
outs = model.evaluate_generator(iter(gen), steps=samples)
preds = model.predict_generator(iter(gen), steps=samples)

# Test correctness of the metric re ref_true_pos()
np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)

# Test correctness of the validation metric computation
val_preds = model.predict_generator(iter(val_gen), steps=val_samples)
val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples)
np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)
np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)

In addition the progress bar is not working with fit generator. @fchollet