About Deterministic Behaviour of GPU implementation of tensorflow

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
TensorFlow version (use command below):('v1.3.0-rc2-20-g0787eee', '1.3.0')
CUDA/cuDNN version: 8.0
GPU model and memory: GeForce GTX 950

Dear experts,
I have just written a python code that implements a CNN with tensorflow. Despite using tf.set_random_seed(0) throughout my code, I get different results for each different run using a GPU, while this is not happening when switching to CPU. I read different threads about stochastic behaviour of GPUs and I could find that the stochastic behaviour is happening when using AdamOptimizer. My code now uses a CPU just for the optimizer, and a GPU for all the others operations, so now the time performance of the GPU is 25% better than CPU despite the 700% of the full GPU implementation. My question is: is this avoidable? Should I really give up in having a deterministic code when using GPUs? How can I tune my hyperparameters and use a full GPU code (some forums say to make an average of different runs.. but this kills the time advantage of using GPUs).
Thanks in advance for all your kind support and for the wonderful job