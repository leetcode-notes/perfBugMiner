Bijector caching breaks when used with TransformedDistribution

Currently, the caching that Bijector objects do to avoid unnecessary calculations does not work when the bijector is used in a TransformedDistribution object. I believe the culprit is the reshaping that the distribution object does here; when we call Bijector.inverse on the output, the Bijector object cannot tell that this is merely a reshaped version of what it calculated previously.
My use case is to sample from a TransformedDistribution and then later calculate the log probability of that sample.
This issue is particularly a problem when using a bijector whose inverse is numerically delicate (in my case, I'm chaining together softplus bijectors and my own custom affine bijector).
I'm willing to work on a fix for this problem, but I'm not sure what the best way to do it is (adding caching to the TransformedDistribution code might work, but that seems like code duplication).
System information

OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: Source
TensorFlow version: v1.1.0-rc2-773-g7fa0cf3 (commit 7fa0cf3)
Bazel version: 0.4.5