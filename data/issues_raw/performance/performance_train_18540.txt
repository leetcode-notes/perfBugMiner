Tensorflow leaks 1280 bytes with each session opened and closed? (Python API)

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 17.10
TensorFlow installed from (source or binary):
Binary
TensorFlow version (use command below):
1.6 (also tested on 1.7)
Python version:
3.6
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
9.0/7.0
GPU model and memory:
Titan XP, 12GB
Exact command to reproduce:
To reproduce, save the following python script as memory_test.py:

    import tensorflow as tf
    import sys
    n_Iterations=int(sys.argv[1])
    def open_and_close_session():
       with tf.Session() as sess:
          pass
    for _ in range(n_Iterations):
       open_and_close_session()
    with tf.Session() as sess:
       print("bytes used=",sess.run(tf.contrib.memory_stats.BytesInUse()))

Then run it from the command line using different number of iterations:
python memory_test.py 0, python memory_test.py 1, python memory_test.py 10, and so on.
Describe the problem
It seems that each Tensorflow session I open and close consumes 1280 bytes from the GPU memory, which are not released until the python kernel is terminated.
Running the script given above, which simply opens and closes sessions without any further operation, yields these results:

python memory_test.py 0 yields bytes used= 1280
python memory_test.py 1 yields bytes used= 2560.
python memory_test.py 10 yields bytes used= 14080.
python memory_test.py 100 yields bytes used= 129280.
python memory_test.py 1000 yields bytes used= 1281280.

The math is easy - each session opened and closed leaks 1280 bytes. I tested this script on two different ubuntu 17.10 workstations with tensorflow-gpu 1.6 and 1.7 and different NVIDIA GPUs.
(here's a related stackoverflow question, at least one user was able to reproduce).