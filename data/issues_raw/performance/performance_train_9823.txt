Tensorflow consumes much more memory than expected

My model has four CPU variables:
[500M, 3] tf.int32
[500M] tf.float32
[500M] tf.float32 (FTRL accumulate slot)
[500M] tf.float32 (FTRL linear slot)
expected memory consumption should be (500M * 6) * 4 = 12G, however tensorflow used 20G memory.
When I increased 500M to 1B, total memory usage is 40G, seems tensorflow do allocate much more memory than needed, any idea? By the way I am not using any tcmalloc stuff.
I also used timeline show_memory to print allocated tensor size, everything is consistent with my calculating.