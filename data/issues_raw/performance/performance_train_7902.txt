Periodically evaluating on a validation set

A common training pattern is to run an epoch, or a few epochs, of training and then evaluate on a development set. As far as I can tell, there does not seem to be an easy way to do this while also using the TFRecords format. While it is possible to run through a data split by setting the num_epochs parameter in the functions to read in single examples, doing so would require the computation graph to be rebuilt before every validation phase. Instead, it would be preferable to have a symbolic example that could be reset to the beginning of the dataset using a related operation.
I feel that the above pattern is sufficiently widely used to make a new feature worthwhile. Further, having control over the number of epochs also becomes important when trying to replicate the performance numbers of other implementations.

The same training-validation pattern using feed dicts:
for epoch in num_epochs:
for batch in get_batches(train, epoch):
sess.run(train_op, feed_dict={"batch": batch)
num_correct, total = 0, 0
for batch in get_batches(val, epoch):
acc = sess.run(acc_op, feed_dict={"batch", batch)
total += batch.shape[0]
num_correct += batch.shape[0] * acc
print "Dev accuracy after %s epochs: %s" % (epoch + 1, num_correct / total)