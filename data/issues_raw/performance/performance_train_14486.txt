TowerLoss(Multiple GPU) will hurt final accuracy/performance so much when doing image finetune, is it a bug?

From the paper "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge"
Writer say "Training was
done using a single GPU (Nvidia K20), and step time was
about 3 seconds. Thus, training took over 3 weeks â€“ parallelizing
training yielded somewhat worse results, though it
increased the speed to convergence."
For my applications, I find tower loss is ok when you do anything without finetune image mode, even
if you use image model(inception resnet nasnet etc.) is fine.
But if you do finetune, either for image caption or image classification, the performance will hurt
a lot(using mulitple gpu wether increase total batch size or keep total batch size the same as single gpu), might not convergent.  Is it a known bug, can we avoid this ?