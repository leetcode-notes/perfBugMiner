`tensorflow.python.client.device_lib.list_local_devices()` Bug

I am trying to set up GPU configuration for Tensorflow. The step is very simple - Call tensorflow.python.client.device_lib.list_local_devices() to detect the number of gpu devices on the machine, and then set config for Tensorflow.  The following is the code for reproducing:
from logging import getLogger

import tensorflow as tf
from tensorflow.python.client import device_lib


log = getLogger(__name__)


def get_available_gpus():
    """ Get available GPU devices info. """
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']


def test_gpu_memory_usage():
    # Detect available GPU devices info.
    log.info("On this machine, GPU devices: ", get_available_gpus())

    # Set Tensorflow GPU configuration.
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)
    tf_config=tf.ConfigProto(
        allow_soft_placement=True,
        device_count={'GPU': len(get_available_gpus())},
        gpu_options=gpu_options,
        log_device_placement=True)
    session = tf.Session(config=tf_config)

    # Mimick training process.
    while True:
        pass
        

test_gpu_memory_usage()

If you run the above code, you could notice that even though you set GPU memory fraction per process to 0.1, it still allocates the whole GPU memory by looking at command nvidia-smi. However, if you don't call get_available_gpus(), the memory allocation works fine. That means, there might be a bug in device_lib.list_local_devices() to prevent setting up Tensorflow GPU memory usage.
PS. My code runs on machine with GPU GeForce GTX 1080, CUDA 8.0, OS Ubuntu 16.04 and Python 3.5, and the above issue could be reproduced using either Tensorflow v.0.12, v.1.0 or v.1.1.