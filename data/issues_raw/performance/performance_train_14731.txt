Tensorflow lite - object detection - ssd-mobilenet-v1

Hi guys,
I have trained a custom ssd-mobilenet-v1 (300x300 input) and currently running it via Tensorflow Android demo (Tensorflow mobile). I would love to convert this model to the lite format and possibly quantize it and run it via Tensorflow Lite to see how much has the performance improved. Currently the inference takes around 400-500ms on Google Pixel (version 1).
Could you please let me know what's the best way to deploy my custom model for object detection?
Thank you very much in advance!
Martin Peniak