Suport cuDNN v6.0

cuDNN v6.0 has been released. There are some cool new features:

Dilated Convolutions: Dilated Convolutions are now supported in cuDNN without a
change in API.
cudnnConvolutionBiasActivationForward allows for the execution of a single kernel fusing convolution, bias and activation operations

Full release notes:

Dilated Convolutions: Dilated Convolutions are now supported in cuDNN without a
change in API. Previously unused “upscale” fields in the Convolution Descriptor
have been repurposed to allow user specification of dilation factors along each
dimension. Support for dilation is present in the following code paths :
Forward : CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,
Backward Data: CUDNN_CONVOLUTION_BWD_DATA_ALGO_0 and
Backward Filter: CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0
 The new API cudnnConvolutionBiasActivationForward allows for the execution of
a single kernel fusing convolution, bias and activation operations. At present, only
per channel bias and RELU activation are supported.
 Inference on 8 bit integer data is now supported, leveraging the 4 element dot
product instruction (IDP4A) of Pascal GPUs with CUDA capabilities 6.1. Two tensor
layouts are supported for this feature: CUDNN_TENSOR_NHWC with INT8 data
type and CUDNN_TENSOR_NCHW_VECT with INT8x4 data type.
 RNN now supports 3 algorithms
o CUDNN_RNN_ALGO_STANDARD :
 Same functionality as in CUDNN v5.1
o CUDNN_RNN_ALGO_PERSIST_STATIC :
 This algorithm relies on the usage of persistent CUDA kernels which
are pre-compiled to fit different GPUs.
 This algorithm is available only on Pascal GPUs.
o CUDNN_RNN_ALGO_PERSIST_DYNAMIC :
This algorithm also relies on the usage of persistent CUDA kernels
but these kernels are compiled at runtime using nvrtc. In some cases
this results in a significant performance benefit.
 This algorithm is also available only on Pascal GPUs and is supported
only on Linux and Windows.
 Support for 1D-FFT convolutions has been added
 New API routine cudnnReduceTensor has been added, supporting 8 reduction
operations
 Activation mode CUDNN_ACTIVATION_ELU is now supported.
 A deterministic max pooling mode CUDNN_POOLING_MAX_DETERMINISTIC
has been added.
 Significant performance improvement for softmax layers for mode
CUDNN_SOFTMAX_MODE_CHANNEL has been achieved when low batch
number is used.
 Significant performance improvements have been added for cudnnAddTensor
when spatial dimensions are set to 1.