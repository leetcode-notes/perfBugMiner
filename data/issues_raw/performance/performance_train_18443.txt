In Matmul, is FP16 x FP16 accumulated to FP16 or FP32?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
1.7
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
9.0
GPU model and memory:
V100 16GB
Exact command to reproduce:

Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is not clear in documents that, in Matmul, FP16xFP16 is accumulated to FP16 or FP32.
This choice affects not only training accuracy, but also TensorCore computing performance on Volta GPU.