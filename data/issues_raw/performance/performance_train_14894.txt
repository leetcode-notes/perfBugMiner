'No gradients provided for any variable, check your graph for ops that do not support gradients'

I write the code like following
import tensorflow as tf
input1=tf.Variable([1.0,2.0,3.0,4.0,5.0,6.0],name='input1')
input2=tf.Variable([2.0,3.0,4.0,6.0,8.0,9.0],name='input2')
values_range = tf.constant([0., 10.], dtype = tf.float32)
source_hist = tf.histogram_fixed_width(tf.to_float(input1), values_range, 11)
template_hist = tf.histogram_fixed_width(tf.to_float(input2), values_range, 11)
source_hist=tf.cast(source_hist,tf.float32)
template_hist=tf.cast(template_hist,tf.float32)
loss=2*tf.nn.l2_loss(source_hist-template_hist)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    writer=tf.summary.FileWriter('./',sess.graph)
    train_step=tf.train.AdamOptimizer(0.001).minimize(loss)
    for i in range(0,10000,1):
        sess.run(train_step)
        print('input1_value',input1.eval())
        print('input2_value',input2.eval())
    writer.close()

Tensorflow throws an error and shows
''ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ["<tf.Variable 'input1:0' shape=(6,) dtype=float32_ref>", "<tf.Variable 'input2:0' shape=(6,) dtype=float32_ref>"] and loss Tensor("mul:0", shape=(), dtype=float32).''
I know the op  tf.histogram_fixed_width doesn't support backpropagation and is not differentiable. While
the op tf.floor has the same attribute as  tf.histogram_fixed_width. And the code below can run  without any error which surprises me a lot.
import tensorflow as tf
cst=tf.constant([1.2,1.4,2.8,4.6,6.8], dtype=tf.float32)
input=tf.Variable(cst)
new=tf.floor(input)
loss=2*tf.nn.l2_loss(input-new)
train_step=tf.train.AdamOptimizer(0.001).minimize(loss)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(0,10000,1):
        sess.run(train_step)
        print('input_value',input.eval())
        print('new_value',new.eval())

I could not figure it out for days, please help