Gradient Inconsistency

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04.5
TensorFlow installed from (source or binary): pip install from binary
TensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1
Python version: Python3.5 (Anaconda)
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: CUDA-8.0
GPU model and memory: GeForce GTX 1070(8GB) & GeForce GTX 770(4GB)
Exact command to reproduce: N/A

Describe the problem
When use operation tf.transpose(), tf.gather(), tf.cholesky() all together in a row over instance of tf.Variable(), the backward gradient computation may seems inconsistent. By using 'inconsistent', I mean that after run the same script multiple times with fixed random seeds, the computed gradient of result of tf.cholesky() over input of tf.transpose() are not always identical.
Source code / logs
import tensorflow as tf
import numpy as np

np.random.seed(1024)
tf.set_random_seed(1024)

N = 10

# indices for `tf.gather()`
indices_for_gather = tf.constant(np.concatenate((np.zeros(N), np.ones(N))).reshape(-1,).astype(np.int32))

# build a 2-by-2 PSD matrix as `param` for 'tf.gather()'
W = tf.constant(np.random.rand(2, 1), dtype=tf.float32)
W = tf.Variable(W)

PSD2x2 = tf.matmul(W, W, transpose_b=True)

# do the `tf.gather()`
M_temp = tf.gather(PSD2x2, indices_for_gather)

# Then transpose the M_Temp and tf.gather() again (to ensure the result is a PSD for cholesky)
M_temp_T = tf.transpose(M_temp)

PSD = tf.gather(M_temp_T, indices_for_gather) + tf.eye(2*N, dtype=tf.float32) * 0.01

# cholesky
L = tf.cholesky(PSD)

# compute the gradient of `L` over `M_temp`
grad = tf.gradients(L, M_temp)

# build a session and compute the gradient
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
results = []
with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(10):
        result = sess.run(grad)
        results.append(result[0]) 
        
# check the results are whether identical or not
for i in range(len(results) - 1): 
    if isinstance(results[i], np.ndarray):
        print(np.all(np.equal(results[i], results[i+1])))
    else:
        print(np.all(np.equal(results[i].values, results[i+1].values)))
The output result is
False
False
False
False
False
False
False
False
False

If I comment the line W = tf.Variable(W) making the W a constant tensor, the results are ideally all True. And I've also tried to compute the gradient ofL over M_temp_T and PSD, both of them are all True. So I think the problem lies in the using of tf.transpose(), tf.gather(), tf.cholesky() all together over instance of tf.Variable().