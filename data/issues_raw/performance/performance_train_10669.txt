Running session using c++ api is significantly slower than using python

System information

OS Platform and Distribution: Linux 13.1 (Bottle)
TensorFlow installed from: source
TensorFlow version: 1.1.0 (git version: v1.1.0-1-g10ec24a, compiler version: v1.1.0-1-g10ec24a)
Compiler: c++ (SUSE Linux) 4.8.1 20130909 [gcc-4_8-branch revision 202388]
Bazel version: 0.4.5
Packages: numpy (1.11.3), numpydoc (0.6.0), protobuf (3.1.0)

No docker, no virtual environment.
All tests were done using CPU only.
All optimization flags are set, no warnings are shown.
tcmalloc is also used.
Batching also helps to increase the performance both for c++ and python, but the gap stays the same.
Describe the problem
I have written a simple benchmark based on official Deep MNIST example (https://www.tensorflow.org/get_started/mnist/pros). I create a simple convolutional net, train it on MNIST (number of training steps is small as we are interested in speed, not accuracy), freeze the graph and load it to c++. Then I do tests using python and using c++ and measure average time that it takes to run a tensorflow session. My tests show that running session in python takes ~2ms, while doing the same using c++ api is slower: ~3ms.
The code of the benchmark can be found here:
https://github.com/July-Morning/MNIST_convnet_tensorflow
It should also be mentioned that the same tests were done for the multilayer perceptron. In that case c++ api was significantly (~7-10x times) faster than python just as was expected.