ssd_mobilenet_v2 was slower than ssd_mobilenet_v1 in the tflite ?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
TensorFlow installed from (source or binary):Source
TensorFlow version (use command below):1.8.0rc0
Python version: 2.7.12
Bazel version (if compiling from source): 0.12.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version:cuda-9.0/7.0
GPU model and memory:GeForce GTX 1080/8105MiB
Phone: xiaomi5 (Snapdragon 820)

Describe the problem
Using tflite's benchmark_model tool, I tested the performance of the ssd_mobilenet_v1 and ssd_mobilenet_v2  models , and found that ssd_mobilenet_v2 was slower than ssd_mobilenet_v1, about 10ms, when setting to 4 threads.
But SSDLite presented in MobileNetV2: Inverted Residuals and Linear Bottlenecks is 35% faster than Mobilenet V1 SSD on a Google Pixel phone CPU (200ms vs. 270ms) at the same accuracy.
Who can explain why ?
Source code / logs
ssd_mobilenet_v1 :
 ./benchmark_model \
>   --graph="mobilenet_ssd_v1_quan.tflite" \
>   --input_layer="Preprocessor/sub" \
>   --input_layer_shape="1,300,300,3" \
>   --input_layer_type="uint8" \
>   --run_delay="-1.0" \
>   --output_layer="concat,concat_1" \
>   --num_runs=50 \
>   --num_threads=4 \
>   --warmup_runs=1 \
>   --use_nnapi=false
WARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e
Graph: [mobilenet_ssd_v1_quan.tflite]
Input layers: [Preprocessor/sub]
Input shapes: [1,300,300,3]
Input types: [uint8]
Output layers: [concat,concat_1]
Num runs: [50]
Inter-run delay (seconds): [-1.0]
Num threads: [4]
Warmup runs: [1]
Use nnapi : [0]
Enable profiling : [0]
nnapi error: unable to open library libneuralnetworks.so
Initialized session in 0.02646s
Running benchmark for 1 iterations: 
count=1 min=220752 max=220752 avg=220752 std=0
Running benchmark for 50 iterations: 
count=50 min=75066 max=193842 avg=82848.9 std=19942
Average inference timings in us: 82848 , Warmup: 220752,

ssd_mobilenet_v2 :
./benchmark_model \
>   --graph="mobilenet_ssd_v2_quan.tflite" \
>   --input_layer="Preprocessor/sub" \
>   --input_layer_shape="1,300,300,3" \
>   --input_layer_type="uint8" \
>   --run_delay="-1.0" \
>   --output_layer="concat,concat_1" \
>   --num_runs=50 \
>   --num_threads=4 \
>   --warmup_runs=1 \
>   --use_nnapi=false
WARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e
Graph: [mobilenet_ssd_v2_quan.tflite]
Input layers: [Preprocessor/sub]
Input shapes: [1,300,300,3]
Input types: [uint8]
Output layers: [concat,concat_1]
Num runs: [50]
Inter-run delay (seconds): [-1.0]
Num threads: [4]
Warmup runs: [1]
Use nnapi : [0]
Enable profiling : [0]
nnapi error: unable to open library libneuralnetworks.so
Initialized session in 0.033135s
Running benchmark for 1 iterations: 
count=1 min=110434 max=110434 avg=110434 std=0
Running benchmark for 50 iterations: 
count=50 min=78189 max=285788 avg=90162.9 std=29644
Average inference timings in us: 90162 , Warmup: 110434,