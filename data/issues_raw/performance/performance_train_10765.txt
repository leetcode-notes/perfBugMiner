[Performance] contirb.seq2seq.attention_wrapper slower due to using matmul instead of reduce_sum

tf version '1.2.0-rc0' contirb.seq2seq.attention_wrapper is great, it make using attention much easier.
However I found using attention_wrapper will be much slower then tf version 1.0.
After some experiment I found it is due to using matmul instead of reduce_sum.
from attetntion_wrapper.py 731
  expanded_alignments = array_ops.expand_dims(alignments, 1)
  attention_mechanism_values = self._attention_mechanism.values
  context = math_ops.matmul(expanded_alignments, attention_mechanism_values)
  context = array_ops.squeeze(context, [1])

Using above code for one of my application got 2.2 batch/s, after changing to use reduce_sum(as tf version 1.0 did), the speed is 3.4 batch/s, improve a lot.
  expanded_alignments = array_ops.expand_dims(alignments, 2)
  attention_mechanism_values = self._attention_mechanism.values
  context = math_ops.reduce_sum(expanded_alignments * attention_mechanism_values, [1])