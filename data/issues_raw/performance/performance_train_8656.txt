Inconsistent behaviour between CPU and GPU gradient step operation

What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None
Environment info
Operating System: Linux Mint 17.2 Rafaela
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
ls -l /usr/local/cuda/lib64/libcud*
/usr/local/cuda/lib64/libcudadevrt.a
/usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
/usr/local/cuda/lib64/libcudart.so.8.0.61
/usr/local/cuda/lib64/libcudart_static.a
/usr/local/cuda/lib64/libcudnn.so
/usr/local/cuda/lib64/libcudnn.so.5
/usr/local/cuda/lib64/libcudnn.so.5.1.10
/usr/local/cuda/lib64/libcudnn_static.a

The output from python -c "import tensorflow; print(tensorflow.__version__)".
1.1.0-rc0
This bug appeared also on the current TF 1.0 Release when installed via pip install tensorflow-gpu

If installed from source, provide

The commit hash (git rev-parse HEAD)
git rev-parse HEAD: 49380d6
The output of bazel version
Build label: 0.4.5

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import tensorflow as tf
a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([train_op,a]))
print(sess.run(a))

The two print statements evaluate to
[None, 3.0]
3.0

When allowing GPU computation to happen by commenting out the second line above, the two print statements evaluate to:
[None, 1.0]
3.0

So apparently when using the GPU, the Variable a is evaluated before the gradient op is executed - and the other way around on CPU. I am not entirely sure what the desired behaviour is supposed to be, but I'm pretty sure they should not be inconsistent.
What other attempted solutions have you tried?
A few things I have observed:
import os
# os.environ["CUDA_VISIBLE_DEVICES"] = ""

import tensorflow as tf

with tf.device("/cpu:0"):
    a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)
init_op = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init_op)
print(sess.run([train_op,a]))
print(sess.run(a))

evaluates to
[None, 3.0]
3.0

The following code
import os
# os.environ["CUDA_VISIBLE_DEVICES"] = ""

import tensorflow as tf

a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)
init_op = tf.global_variables_initializer()
with tf.control_dependencies([train_op]):
    a = tf.identity(a)
sess = tf.Session()
sess.run(init_op)
print(sess.run(a))

evaluates to
3.0. This seems to "enforce" the behaviour of CPU-only computation when using the GPU.
Also the initial example has been run on three different machines, all with the same TitanX GPU Model.
What am I missing? Any help would be greatly appreciated.
Matthias