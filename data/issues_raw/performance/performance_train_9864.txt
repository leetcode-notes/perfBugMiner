MPI based communication path for tensor exchange operations

This pull request adds an additional communication path to TensorFlow that allows Tensors to be exchanged using MPI based Send and Recieve routines when running distributed TensorFlow. T
The used MPI implementation will pick the most optimal path available between the two communication points allowing the user to take advantage of high performance networks such as Infiniband.
Certain MPI implementations allow direct data-transfers from GPU buffers (CUDA Aware / GPUDirect RDMA), this option can be enabled using an environment variable. See the README for more details.
Note: Be aware of the known problems that results in unpredictable behavior for certain complex networks. Any help/insight on that from other MPI experts would be appreciated.