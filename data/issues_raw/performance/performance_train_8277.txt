Different Code Path Taken in conv2d for Constant vs Variable Filter

It appears that a different code path is taken in conv2d for Constant vs Variable filters.
On a box with GPU, this works:
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))
filters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
      data_format='NCHW',
  ).eval()
however this fails with Check failed: data_format == FORMAT_NHWC Generic conv implementation only supports NHWC tensor format for now.:
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))
filters = tf.constant(1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
      data_format='NCHW',
  ).eval()
Both of these work (the data_format is supported):
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))
filters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
  ).eval()
and
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))
filters = tf.constant(1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
  ).eval()
I don't see a reason for the Constant filter to take what I am guessing is a less efficient code path (not using GPU op) than the Variable Filter.
To make things even weirder, it seems like even though the GPU is not being used for the Constant, there is still cuda memcpy. Run on 20 calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 52.45%  90.108us        20  4.5050us  4.0950us  6.0800us  [CUDA memcpy HtoD]
 42.91%  73.726us        20  3.6860us  3.5520us  4.5120us  [CUDA memcpy DtoH]
  4.64%  7.9680us         1  7.9680us  7.9680us  7.9680us  [CUDA memset]