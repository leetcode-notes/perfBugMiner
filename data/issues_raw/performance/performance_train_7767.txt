ParameterServer restart crashes distributed training process

I've tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted.  Code is available at http://pastebin.com/HBUicRp2.  I'm using TensorFlow freshly built from r1.0.
At first, this error happens which is OK:
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnavailableError'>, {"created":"@1487751897.191020037","description":"EOF","file":"ex
ternal/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":235,"grpc_status":14}
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":
235,"grpc_status":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:ps/replica:0/task:0/cpu:0", send_
device_incarnation=8036561443230364107, tensor_name="edge_30_Assign_1", tensor_type=DT_INT64, _device="/job:worker/replica:0/task:0/cpu:0"]()]]
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":
235,"grpc_status":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:ps/replica:0/task:0/cpu:0", send_
device_incarnation=8036561443230364107, tensor_name="edge_30_Assign_1", tensor_type=DT_INT64, _device="/job:worker/replica:0/task:0/cpu:0"]()]]
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":
235,"grpc_status":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:ps/replica:0/task:0/cpu:0", send_
device_incarnation=8036561443230364107, tensor_name="edge_30_Assign_1", tensor_type=DT_INT64, _device="/job:worker/replica:0/task:0/cpu:0"]()]]

However, after Parameter Server is restarted, I see this error in logs and training worker crashes:
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000729. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000727. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000731. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000723. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000072b. Possibly, this worker just restarted.
Traceback (most recent call last):
  File "tf_dist_mnist.py", line 140, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "tf_dist_mnist.py", line 107, in main
    mon_sess.run(train_op, feed_dict={image: image_, label: label_})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 478, in __exit__
    self._close_internal(exception_type)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 511, in _close_internal
    self._sess.close()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 739, in close
    self._sess.close()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 827, in close
    self._coord.join()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py", line 234, in _run
    sess.run(enqueue_op)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":235,"grpc_status":14}

I expect it to recover from latest checkpoint instead.