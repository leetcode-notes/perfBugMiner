tfrecords giving parsing error when saved examples are too big

When I try to save a example to a tfrecord that is too large a parsing error occurs. I need to save a large sequence of float valued images (states of a fluid flow simulation) and when the states and sequences get too large a parsing error occurs.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I think that this might be the same error and problem that other people are getting when saving long sequences to tfrecords like seen here #5234 and OliviaMG/xiaomeng#1. While both of these threads found solutions the underlining problem does not seem to be found.
Environment info
Operating System:
Ubuntu 16.04
CUDA 8.0
CUDNN 5.1
TensorFlow 0.12.1
If possible, provide a minimal reproducible example
Running this minimal script will cause the error. Lowering the SIZE_RECORD value will cause the script to run with out error.
from tqdm import tqdm
import numpy as np
import tensorflow as tf

# this will kill it
SIZE_RECORD=20000000
# this will run just fine
# SIZE_RECORD=2000000

writer = tf.python_io.TFRecordWriter("test.tfrecords")
# iterate over 10 times
# wrap with tqdm for a progress bar
for example_idx in tqdm(xrange(2)):
    features = np.zeros((SIZE_RECORD))

    # construct the Example proto boject
    example = tf.train.Example(
        # Example contains a Features proto object
        features=tf.train.Features(
          # Features contains a map of string to Feature proto objects
          feature={
            # A Feature contains one of either a int64_list,
            # float_list, or bytes_list
            'image': tf.train.Feature(
                float_list=tf.train.FloatList(value=features.astype("float"))),
    }))
    # use the proto object to serialize the example to a string
    serialized = example.SerializeToString()
    # write the serialized object to disk
    writer.write(serialized)
writer.close()

def read_and_decode_single_example(filename):
    # first construct a queue containing a list of filenames.
    # this lets a user split up there dataset in multiple files to keep
    # size down
    filename_queue = tf.train.string_input_producer([filename],
                                                    num_epochs=None)
    # Unlike the TFRecordWriter, the TFRecordReader is symbolic
    reader = tf.TFRecordReader()
    # One can read a single serialized example from a filename
    # serialized_example is a Tensor of type string.
    _, serialized_example = reader.read(filename_queue)
    # The serialized example is converted back to actual values.
    # One needs to describe the format of the objects to be returned
    features = tf.parse_single_example(
        serialized_example,
        features={
            # We know the length of both fields. If not the
            # tf.VarLenFeature could be used
            'image': tf.FixedLenFeature([SIZE_RECORD], tf.float32)
        })
    # now return the converted data
    image = features['image']
    return image

# get single examples
image = read_and_decode_single_example("test.tfrecords")
# groups examples into batches randomly
images_batch = tf.train.shuffle_batch(
    [image], batch_size=1,
    capacity=3,
    min_after_dequeue=2)

# simple model
w = tf.get_variable("w1", [SIZE_RECORD, 1])
y_pred = tf.matmul(images_batch, w)
loss = tf.nn.l2_loss(y_pred - 1.0)

# for monitoring
loss_mean = tf.reduce_mean(loss)

train_op = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)
tf.train.start_queue_runners(sess=sess)

_, loss_val = sess.run([train_op, loss_mean])
print loss_val
print("worked!!!")

What other attempted solutions have you tried?
I have tried saving the example in a variety of different ways including converting it to a string and breaking up the vector into multiple features.
Logs or other output that would be helpful
This is the output when the tfrecord is too big
W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Could not parse example input, value: '
���&
���&
image����&����&
���&
ERROR:tensorflow:Exception in QueueRunner: 'utf8' codec can't decode byte 0x9b in position 40: invalid start byte
Exception in thread Thread-3:
Traceback (most recent call last):
File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner
self.run()
File "/usr/lib/python2.7/threading.py", line 754, in run
self.__target(*self.__args, **self.__kwargs)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py", line 234, in _run
sess.run(enqueue_op)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 766, in run
run_metadata_ptr)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 964, in _run
feed_dict_string, options, run_metadata)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1014, in _do_run
target_list, options, run_metadata)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1021, in _do_call
return fn(*args)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1003, in _run_fn
status, run_metadata)
File "/usr/lib/python2.7/contextlib.py", line 24, in exit
self.gen.next()
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py", line 468, in raise_exception_on_not_ok_status
compat.as_text(pywrap_tensorflow.TF_Message(status)),
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.py", line 84, in as_text
return bytes_or_text.decode(encoding)
File "/usr/lib/python2.7/encodings/utf_8.py", line 16, in decode
return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x9b in position 40: invalid start byte
Thank!!!