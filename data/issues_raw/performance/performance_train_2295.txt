pow() gradients create +inf at 0.0

Hi,
I had a pipeline where I was first clamping to [0..1] and then doing x^2.4 (converting to/from gamma RGB to linear RGB):
y = tf.minimum(tf.maximum(y, 0.0), 1.0)
gamma = tf.constant(1.0/2.4, tf.float32)
y = tf.pow(y, gamma)

However, this instantly created inf values (and NaNs in the next step), seemingly due to the use of log() to compute the gradients. The gradient of pow() in 0.0 should be pretty well defined as long as the exponent is nonzero; at the very least, it is not infinity.
As a workaround, clamping to 1e-5 instead of 0.0 caused the NaNs to disappear.