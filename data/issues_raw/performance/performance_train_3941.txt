cifar10_train can't use all CPUs

Environment info
Operating System: Linux 14.04
Installed version of CUDA and cuDNN:  No
If installed from binary pip package, provide:

Which pip package you installed.

https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl

The output from python -c "import tensorflow; print(tensorflow.__version__)".

0.10.0rc
Steps to reproduce

clone the tensorflow main repo
run python cifar10_train.py
This will only occupy ~6 CPUs on a machine with 8 CPUs (top command shows the CPU usage is ~600%)

What have you tried?

set  intra_op_parallelism_threads and  inter_op_parallelism_threads in the ConfigProto when creating the session.  This doesn't help at all.
use multi-thread training like:

class Cifar10(object):
  def __init__(self, sess):
    print("In construction")
    self.global_step = tf.Variable(0, trainable=False)
    self.images, self.labels = cifar10.distorted_inputs()
    self.logits = cifar10.inference(self.images)
    self.loss = cifar10.loss(self.logits, self.labels)
    self.train_op = cifar10.train(self.loss, self.global_step)

    self.sess = sess
    init = tf.initialize_all_variables()
    self.sess.run(init)
    self.start = time.time()
    tf.train.start_queue_runners(sess=self.sess)
    print("Construction Done")
    self.saver = tf.train.Saver()

  def train(self):
    for step in xrange(FLAGS.max_steps):
      self.sess.run(self.train_op)

  def run(self, sess):
    workers = []
    for _ in xrange(4):
      t = threading.Thread(target=self.train)
      t.start()
      workers.append(t)

    for t in workers:
      t.join()

def main(argv=None):  # pylint: disable=unused-argument
  cifar10.maybe_download_and_extract()
  with tf.Graph().as_default(): 
    with tf.Session() as sess:
      model = Cifar10(sess)
      for _ in xrange(10000):
        gs = sess.run(model.global_step)
        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')
        model.saver.save(sess, checkpoint_path, global_step=gs)
        model.run(sess)

if __name__ == '__main__':
  tf.app.run()

The complete code:
cifar10_multi_thread.txt
This did help. The CPU usage became 800% and the training process did go faster.
I assume that the data feeding process should not be the bottleneck because in the multi-thread training the input queue is not adjusted. Then how does TensorFlow parallelize the training process? Is there any parameter I should try to increase the CPU usage with the original code?
I also observed that if I train the multi-threads version in a distributed setting (I'm running 3 workers async, each runs a multi-threads training process), the precision on testing data looks like:

But if I use the original version, the performance looks like:

I guess that might be caused by the async updates, is there any suggestion to avoid this?
Thanks