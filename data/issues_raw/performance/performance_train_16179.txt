ProfilerHook and loading libcupti.so cause Ubuntu to completely freeze

OS Platform and Distribution: Ubuntu 14.04 LTE
TensorFlow version: 1.14
Bazel version: 0.9.0
CUDA/cuDNN version: 8.0/7.0.5
GPU model and memory: GeForce GTX1060 - 6070MB
Exact command to reproduce: python3.4 -m music_modeling
--

I added "ProfilerHook" to Estimator for recording GPU memory consumption; but it always causes my Ubuntu to freeze indefinitely and Ubuntu never makes away with it.
Here is the source code:
import tensorflow as tf

from tensorflow.python.layers.core import dense
from tensorflow.python import debug as tf_debug

from data.music_data_reader import MusicDataReader
from model.tf_msa_rnn import dynamic_msa_rnn


FLAGS = tf.app.flags.FLAGS
tf.app.flags.DEFINE_string("mode", tf.estimator.ModeKeys.TRAIN,
                           """"Is training or testing mode""")
tf.app.flags.DEFINE_string("model_dir", './msa_model',
                           """"Directory in where checkpoints are stored""")
tf.app.flags.DEFINE_integer("batch_size", 20,
                            """Number of samples in a batch""")
tf.app.flags.DEFINE_integer("num_epochs", 100,
                            """"How many times the whole training set has to be fed into network""")
tf.app.flags.DEFINE_string("log_directory", './log_dir',
                           """"Directory in where logs and checkpoints are stored""")
# *************************************
# *************************************Configuration options for the network
# *************************************
tf.app.flags.DEFINE_integer("num_units", 30,
                            """"# of hidden units in an LSTM cell""")
tf.app.flags.DEFINE_integer("num_msa_feats", 10,
                            """"# of MS features to be learned""")
tf.app.flags.DEFINE_integer("signal_len", 100,
                            """"Length of the signal at a time to be processed by 
                                multi-scale analyzer
                            """)
tf.app.flags.DEFINE_integer("dim_pitch", 88,
                            """"# of hidden units in an LSTM cell""")
# *************************************
# *************************************Configuration options for dataset
# *************************************
tf.app.flags.DEFINE_string("dir_path",
                           './music_samples/MuseData',
                           """"Absolute path to the music files for reading training/testing samples""")
tf.app.flags.DEFINE_integer("pitch_low",
                            21,
                            """"Low pitch value""")
tf.app.flags.DEFINE_integer("pitch_high",
                            109,
                            """"High pitch value""")
tf.app.flags.DEFINE_float("dt",
                          0.3,
                          """"Not sure yet...""")


def loss_fn(y_pred, y_true):
    '''

    :param y_pred: Logits predicted by the model
    :param y_true: Correct values corresponding each prediction
    :return:
    '''

    y_pred = tf.log(tf.nn.softmax(y_pred, name="probs_tensor"))  # [BSxMTxOS] -- Probabilities
    p_trun = y_pred[:, 0:-1, ...]  # x'[1], x'[2], ..., x'[N-1]
    t_trun = y_true[:, 1:, ...]  # x[1], x[2], ..., x[N-1]
    loss = tf.reduce_sum(p_trun*t_trun, axis=2)  # [BSxMT] -- Dot product between 3rd dimensions
    loss = tf.reduce_mean(loss, name="piano_roll_loss")  # loss function -- returns a scalar
    tf.summary.scalar("loss_fn", loss)  # Add summary for the loss
    loss = tf.Print(loss, [loss], "Loss: ")
    return loss


def model_fn(features,
             mode=tf.estimator.ModeKeys.TRAIN,
             params=None):

    print("Creating Model...")
    input_ph = tf.reshape(features['input_ph'], [FLAGS.batch_size, params['max_seq'], FLAGS.dim_pitch])
    seq_len_ph = tf.reshape(features['seq_len_ph'], [FLAGS.batch_size])
    outputs, state = dynamic_msa_rnn(FLAGS.batch_size,
                                     input_ph,
                                     seq_len_ph,
                                     params['max_seq'],
                                     FLAGS.signal_len,
                                     [20, 10],  # Number of filters per layer
                                     [11, 13],  # Kernel size for each layer
                                     [3],  # Pooling size for each layer
                                     FLAGS.num_msa_feats,
                                     FLAGS.num_units,
                                     activation=tf.nn.tanh,
                                     initializer=tf.glorot_normal_initializer())  # [BSxMTxOS], [BSxSS]
    # Create fully connected layer to generate output for piano keys
    outs = dense(outputs,
                 FLAGS.dim_pitch,
                 kernel_initializer=tf.glorot_normal_initializer())  # BSxMTxID
    loss = loss_fn(outs, features['input_ph'])
    print("Creating Estimator Spec for %s ..." % mode)
    # For training
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer()
        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op)
    # For evaluation
    eval_metric_ops = {
        "accuracy": tf.metrics.accuracy(
            labels=tf.argmax(features['input_ph'][:, 1:, ...], axis=-1),
            predictions=tf.argmax(outs[:, 0:-1, ...], axis=-1)
        )
    }
    return tf.estimator.EstimatorSpec(mode=mode,
                                      loss=loss,
                                      eval_metric_ops=eval_metric_ops)


def do_train(tr_data, vl_data):

    # Create Estimator
    sess_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)
    sess_conf.gpu_options.allow_growth = True
    config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,  # CheckpointSaverHook
                                    save_checkpoints_steps=100,  # CheckpointSaverHook
                                    log_step_count_steps=10,  # SummarySaverHook
                                    session_config=sess_conf)
    music_classifier = tf.estimator.Estimator(model_fn,
                                              config=config,
                                              params={'max_seq': tr_data.max_seq})
    # Prepare input data
    tr_input_fn = tf.estimator.inputs.numpy_input_fn(
        {'input_ph': tr_data.data, 'seq_len_ph': tr_data.seq_len},
        batch_size=FLAGS.batch_size,
        num_epochs=FLAGS.num_epochs,
        num_threads=1,
        shuffle=True
    )
    # Extra Hooks
    logging_hook = tf.train.LoggingTensorHook(
        tensors={'probabilities': 'probs_tensor'},
        every_n_secs=60
    )
    # debugging_hook = tf_debug.LocalCLIDebugHook(thread_name_filter="MainThread$", dump_root="./dump")
    profiler_hook = tf.train.ProfilerHook(save_steps=1,
                                          output_dir="./profile",
                                          show_dataflow=False,
                                          show_memory=True)
    # Train
    music_classifier.train(tr_input_fn, hooks=[profiler_hook])
    print("Training is over...")


def do_test(te_data):

    print("Start testing...")


def main(_):

    if FLAGS.mode == tf.estimator.ModeKeys.TRAIN:
        tr_data = MusicDataReader(FLAGS.dir_path,
                                  'train',
                                  (FLAGS.pitch_low, FLAGS.pitch_high),
                                  FLAGS.dt)
        vl_data = MusicDataReader(FLAGS.dir_path,
                                  'valid',
                                  (FLAGS.pitch_low, FLAGS.pitch_high),
                                  FLAGS.dt)
        print("Number of training samples: %d" % tr_data.data_num)
        print("Number of validation samples: %d" % vl_data.data_num)
        do_train(tr_data, vl_data)
    else:
        te_data = MusicDataReader(FLAGS.dir_path,
                                  'test',
                                  (FLAGS.pitch_low, FLAGS.pitch_high),
                                  FLAGS.dt,)
        print("Number of testing samples: %d" % te_data.data_num)
        do_test(te_data)


if __name__ == "__main__":
    tf.app.run(main=main)

I cannot attach the output of my console for it is impossible due to the indefinite freezing of my Ubuntu. All I can say is that it tells me that it loads libcupti.so and computes a step of training process. Then, everything totally messes up.
Thank you for your support in advance.