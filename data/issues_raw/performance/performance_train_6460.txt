Slow Adam sparse updates in distributed TF

I am trying to train a model with the tf.nn.embedding_lookup_sparse operation. Small example: https://gist.github.com/Bobrosoft98/2d639d3924dfbc4ec7bc620fd5a4d480
When I run this code with NUM_WORKERS = 1, the output is as follows
0 calc in 0.0176000595093
0 apply in 0.184364080429
0 calc in 0.0167639255524
0 apply in 0.189659118652
...

However, when I increase the number of workers to 30, every single process works more than 30 times slower:
6 calc in 0.432843923569
11 calc in 0.787642002106
3 calc in 0.440953016281
14 calc in 0.377243995667
20 calc in 0.569782018661
...
6 apply in 5.63959908485

The CPU load is only ~50%, so there is a lot of resources available for the computation. This makes me think that sparse updates use locking, even though the use_locking flag is set to False by default. There is no such problem with other optimizers (I tried GradientDescentOptimizer and AdadeltaOptimizer). Also if I exclude sparse operations from the graph (commented lines), the problem disappears.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#464 - there was mentioned that Adam was slower on sparse updates in general
Environment info
Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: I used the CPU version
If installed from binary pip package, provide:

A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0-cp27-none-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".

$ python -c "import tensorflow; print(tensorflow.__version__)"
0.12.0-rc1