[feature] Mobile Integration with NNPACK

Caffe2 use can use NNPACK for which it says:

NNPACK, which specifically optimizes convolutions on ARM


Currently Caffe2 is optimized for ARM CPUs with NEON (basically any ARM CPU since 2012). Perhaps surprisingly, ARM CPUs outperform the on-board GPUs (our NNPACK ARM CPU implementation outperforms Apple’s MPSCNNConvolution for all devices except the iPhone 7).


For a convolutional implementation, it is recommended to use NNPACK since that’s substantially faster (~2x-3x) than the standard im2col/sgemm implementation used in most frameworks.

The readme for NNPACK lists Tensorflow as a framework that could potentially use it, though that has not yet happened.
I believe that TF also avoids using the im2col/sgemm approach on mobile and instead uses the Eigen TensorConvolution. It would be good to benchmark these two options against each other and see if TF performance can be improved by using the NNPACK conv instead of the eigen conv. There is an open ticket to do this benchmarking: Maratyszcza/NNPACK#30.
As a feature I suggest offering an NNPACK backed kernel to allow comparing vs Eigen.