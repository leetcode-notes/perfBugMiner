Floating point exception when computing gradients

Hi tensorflow developer team!
I just wanted to first let you know that I really appreciate all the work you are putting into this amazing open source system. It has been a terrific system for me so far. However, I have come into a roadblock when using the tf.gather function in gradients.
I am using
Environment info
Operating System: CentOS
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
CUDA version 7.5
cuDNN version 7.0 (64 bit)
tensorflow/0.8.0-gpu version
Steps to reproduce
I obtain a floating point exception when I run the following code
A_ph = tf.placeholder(tf.float32, shape=(None, 2))
ind_ph = tf.placeholder(tf.int32, shape=(None, 2))
gather = tf.gather(A_ph, ind_ph)
redsum = tf.reduce_sum(gather, 1)
l2 = tf.reduce_sum(redsum)
A = np.array([[0,0],[0,0],[0,0]])
ind = np.zeros((0,2))
grad_op = tf.gradients(l2, A_ph)
out = sess.run(grad_op, feed_dict={A_ph:A, ind_ph:ind})
What have you tried?
If you replace ind with a non-empty array, such as [[0,1]] it works fine.
I suspect that the output of redsum = [], which makes l2 disconnected from A_ph for the given ind. Fed forward to computing l2, I get the correct value of 0.0 (since there is nothing to be summed). However, for backpropagation of gradients, I think the output of redsum=[] chokes the algorithm, and causes it to evaluate some illegal operation using an empty tensor, instead of just backpropagating 0.0.
Are there any solutions to this problem currently? In my use of this computation, I won't know ahead of time whether or not the intermediate computations produce [](empty tensor), but I also want to be able to get the correct gradients when this happens. Thank you so much for your consideration!