The keys in end_points of slim are not unified for different layers.

Look at the code:
  with tf.name_scope('xx'):
    end_points_collection = 'dd'
    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                      outputs_collections=end_points_collection):
      y = slim.conv2d(np.zeros([1,20,20,3],dtype=np.float32), 10, [2, 2])
      x = slim.max_pool2d(np.zeros([1,20,20,3],dtype=np.float32), [2, 2])
      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
  print(end_points)
It output
OrderedDict([('Conv', <tf.Tensor 'xx/Conv/Relu:0' shape=(1, 20, 20, 10) dtype=fl
oat32>), ('xx/MaxPool2D', <tf.Tensor 'xx/MaxPool2D/MaxPool:0' shape=(1, 10, 10,
3) dtype=float32>)])

For max_pool2d layer, the key has prefix xx, but for conv2d layer, it don't have prefix xx. Because in conv2d it uses variable_scope but in max_pool2d it uses name_scope. So the behavior looks inconsistent, and may cause the code make mistake. Do we need to unify the behavior ?
For example, if we use multi-gpu to train the network, and we have many clones of network, which name_scope's prefix  are clone_1,  clone_2 and so on. If we want to use the key of end_points to get the output of layer on different gpu. We should deal with max_pool2d and conv2d  differently.