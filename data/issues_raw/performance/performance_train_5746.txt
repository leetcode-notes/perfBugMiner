Optimization occurs after graph is partitioned across devices

The optimization (sub-expression, constant folding) of the graphs takes place after partitioning. While this is important, it prevents optimization of logic that is split across devices by an unfortunate placement decision.
In my simple example, constants are assigned to my device graph, and the downstream maths nodes are assigned to the CPU.  Consequently, parts of the graph where constant folding should occur are not eliminated, resulting in substantially larger graphs than is necessary.
I propose that there should be an optmization pass before the graph is partitioned.  I realize that this is awkward, because the optimization takes place on a Graph, not a GraphDef.