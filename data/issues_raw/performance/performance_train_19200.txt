Difference in output between CPU and GPU

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): TF 1.7 and TF 1.8 (v1.8.0-0-g93bc2e2072 1.8.0)
Python version: Python 3.5.4
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: CUDA 9.0, cudnn 7.0.5
GPU model and memory: GPU GTX1080
Exact command to reproduce: n/a

Problem
I see a very different output when executing a training on CPU vs. GPU. The problem does not seem to be related to initialization or numerical precision (see investigation): same code, same data, same machine, just switching between GPU and CPU execution by adding os.environ["CUDA_VISIBLE_DEVICES"]="" gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).
Investigation
I gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a tf.multiply (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.
Unfortunately, I am not able to reproduce the problem in a unit test (but am still working on it). If I feed the same inputs to tf.multiply in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.
Seeding and reproducibility
Regarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.
The problem is reproducible from one run to the other.
Environment dependency
The problem occurs with both TF 1.7 and TF 1.8. However, if I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.
Any idea or debugging experiment suggestion is very welcome.