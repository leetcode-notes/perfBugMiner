Major performance hit when running two processes on same GPU.

I've observed a major performance hit when running two identical models on the same GPU, with allow_growth set to True, so that each process, which only needs a small fraction of the GPU memory (~1gb/11gb are used when allow_growth is set to true). When running a single model on a single process, it consistently finished going through the data I have available in approximately 170-174 seconds, and never exceeds 50% Volatile GPU-Utilization according to nvidia-smi . However, when running with two separate, concurrent processes, (each identical to the first), they both finish in approximately 320-340 seconds, which is unexpected, since GPU utilization was not even half in the first scenario, and running two concurrently effectively slows them down to running them sequentially, despite the seemingly available processing power and memory.
Is this intentional, or is there a better way to do this? (Currently launching two workers via Celery, each of which create their own TF session and load the model into it separately, and run the data through it). I would love to make maximal use of available hardware, and this seems like a very counter-intuitive outcome.
I can observe each process allocate roughly 1GiB GPU memory, and each adds approximately 45-50% GPU utilization. For testing purposes, data and model used in both parallel runs is completely identical.
Any thoughts? Am I misusing TF?
System information

Using Keras to load model
Ubuntu 16.04
CUDA/CUDNN 8.0/5.1
TF version 1.0.1
GTX 1080 ti (not being used to render screen, second 1080 ti is doing that)
Running in nvidia-docker with a single GPU available to worker process (the one not rendering the screen).
Running two separate sessions initiated via Celery which both create their own session, set allow_growth=True, each load the model into memory separately, and each run with data separately.