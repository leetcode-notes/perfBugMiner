Memory Leak from Training Step

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, although my code is somewhat based on the MNIST deep learning tutorial.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 14.04 VERSION="14.04.5 LTS, Trusty Tahr"
TensorFlow installed from (source or binary):
Installed via the VirtualEnv method
TensorFlow version (use command below):
1.1 and 1.2
Bazel version (if compiling from source):
CUDA/cuDNN version:
CUDA Version 8.0.44
GPU model and memory:
GeForce GTX 780M 4GB
Exact command to reproduce:
self.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})

Describe the problem
This is very similar to the bug report I submitted here, but is a bit of a slower leak and is present in both TF 1.1 and 1.2.  I have finalized my graph.  Using the architecture described by Zeiler et al., 2013 (ZF Net), batch sizes of 64, and 224x224 grayscale (1 channel) input, it leaks approximately 4GB after approximately 3000 batches.  This makes it unworkable, for say, 80 epochs of ImageNet training.  I have confirmed that the leak either does not occur or is much less severe (hard to tell which) if I comment out the training line (i.e. still do all of my preprocessing and loading).
As directed in that last linked issue, I tried to call sess.run with
options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata
and start the program with
env TF_CPP_MIN_VLOG_LEVEL=1 python deep_learning_main.py
but the amount of spew was enormous, and it won't respond to keyboard interrupts (I have to kill the job).  If that info would be helpful, how do I go about recording/saving this information properly to upload and help you all debug?
1_08.zip