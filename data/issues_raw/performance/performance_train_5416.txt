Possible Bug - Varying session run times when parallelized across GPUs on one node

Hello tensorflow team!
I have been trying to parallelize my 3D convolution network across 2 GPUs on the same node (using data parallelism), and have found that some of my session runs took significantly longer than others.  Upon closer investigation using your timeline feature, I found that sometimes GPU1 would wait a certain amount of time before starting to run its convolutions even as GPU0 was happily chugging along. In the most extreme case, execution would happen essentially sequentially, doubling my runtime!
I am attaching screenshots of my timelines from three sessions to show you what I mean...
In the first one, everything is fine, it is executing both towers in parallel:

In the second, I see the "sequential" behaviour:

In the last one, I see something in between:

I know this happens even when I am not logging metadata because I still see a large spread of runtime distributions without logging on (see parallel-nometadata.txt at the bottom).  Also, dumping the GraphDef proto at each step tells me that the ops are correctly assigned to the different GPUs (I have attached those, as well as my timeline files, at the bottom).
I actually briefly talked about this with @mrry in person a little over a week ago, and we did not find anything obviously wrong at first glance.  Any help you could provide would be much appreciated!
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I have not found any related Github or StackOverflow threads.  I have been using the Timeline profiling feature as described here.
Environment info
Operating System: Ubuntu 14.04.5 LTS (running in a singularity container on a CentOS 6.7 host).
Installed version of CUDA and cuDNN:
I am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 .
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
libOpenCL.so
libOpenCL.so.1
libOpenCL.so.1.0
libOpenCL.so.1.0.0
libcublas.so
libcublas.so.8.0
libcublas.so.8.0.45
libcublas_device.a
libcublas_static.a
libcudadevrt.a
libcudart.so
libcudart.so.8.0
libcudart.so.8.0.44
libcudart_static.a
libcudnn.so
libcudnn.so.5
libcudnn.so.5.1.5
libcudnn_static.a
libcufft.so
libcufft.so.8.0
libcufft.so.8.0.44
libcufft_static.a
libcufftw.so
libcufftw.so.8.0
libcufftw.so.8.0.44
libcufftw_static.a
libcuinj64.so
libcuinj64.so.8.0
libcuinj64.so.8.0.44
libculibos.a
libcurand.so
libcurand.so.8.0
libcurand.so.8.0.44
libcurand_static.a
libcusolver.so
libcusolver.so.8.0
libcusolver.so.8.0.44
libcusolver_static.a
libcusparse.so
libcusparse.so.8.0
libcusparse.so.8.0.44
libcusparse_static.a
libnppc.so
libnppc.so.8.0
libnppc.so.8.0.44
libnppc_static.a
libnppi.so
libnppi.so.8.0
libnppi.so.8.0.44
libnppi_static.a
libnppial.so
libnppial.so.8.0
libnppial.so.8.0.44
libnppicc.so
libnppicc.so.8.0
libnppicc.so.8.0.44
libnppicom.so
libnppicom.so.8.0
libnppicom.so.8.0.44
libnppidei.so
libnppidei.so.8.0
libnppidei.so.8.0.44
libnppif.so
libnppif.so.8.0
libnppif.so.8.0.44
libnppig.so
libnppig.so.8.0
libnppig.so.8.0.44
libnppim.so
libnppim.so.8.0
libnppim.so.8.0.44
libnppist.so
libnppist.so.8.0
libnppist.so.8.0.44
libnppisu.so
libnppisu.so.8.0
libnppisu.so.8.0.44
libnppitc.so
libnppitc.so.8.0
libnppitc.so.8.0.44
libnpps.so
libnpps.so.8.0
libnpps.so.8.0.44
libnpps_static.a
libnvToolsExt.so
libnvToolsExt.so.1
libnvToolsExt.so.1.0.0
libnvblas.so
libnvblas.so.8.0
libnvblas.so.8.0.44
libnvgraph.so
libnvgraph.so.8.0
libnvgraph.so.8.0.44
libnvgraph_static.a
libnvrtc-builtins.so
libnvrtc-builtins.so.8.0
libnvrtc-builtins.so.8.0.44
libnvrtc.so
libnvrtc.so.8.0
libnvrtc.so.8.0.44
stubs

I installed tensorflow using the 0.11.0rc2-gpu tag on docker hub.
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Here is the code I used to generate all the data I am sharing.
import os
import time
import timeit

import google
import numpy as np
import tensorflow as tf

grid_size = 9
channel_size = 4
batch_size = 2**9
num_convs = 8
log_metadata = True
towers = 2
num_train_batches = 100


def define_model(towers):
    tower_outs = []
    for i in range(0, towers):
        with tf.device('/gpu:{:}'.format(i)):
            with tf.name_scope('TOWER_{:}'.format(i)):
                tower_outs.append(define_tower(i))
                tf.get_variable_scope().reuse_variables()
    return tower_outs


def define_tower(gpu_num):
    x = tf.placeholder(
        tf.float32,
        shape=[None, grid_size, grid_size, grid_size, channel_size],
        name='grid')

    num_inputs = channel_size
    for i in range(num_convs):
        with tf.variable_scope("conv{:d}".format(i)):
            weights = tf.get_variable(
                "weights",
                [3, 3, 3, num_inputs, 32])
            x = tf.nn.conv3d(x, weights, [1, 1, 1, 1, 1], 'SAME')
            num_inputs = 32

    return x


if __name__ == "__main__":

    # Define model.
    tower_outs = define_model(towers)

    # Create inputs.
    feed_dict = {}
    for i in range(towers):
        feed_dict['TOWER_{:}/grid:0'.format(i)] = np.random.rand(
            batch_size, grid_size, grid_size, grid_size, channel_size)

    # Prepare for logging.
    if log_metadata:
        run_options = tf.RunOptions(
            trace_level=tf.RunOptions.FULL_TRACE,
            output_partition_graphs=True)
        run_metadata = tf.RunMetadata()
        kwargs = {'options': run_options,
                  'run_metadata': run_metadata}

        curr_time = time.strftime("%Y-%m-%d-%H-%M-%S")
        if log_metadata:
            out_dir = curr_time
            os.mkdir(out_dir)
    else:
        kwargs = {}

    # Run model.
    train_learning_times = []
    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        tf.initialize_all_variables().run()
        for i in range(num_train_batches):
            learning_time_start = timeit.default_timer()
            _ = sess.run(tower_outs, feed_dict=feed_dict, **kwargs)
            train_learning_times.append(timeit.default_timer() -
                                        learning_time_start)

            if log_metadata:
                from tensorflow.python.client import timeline
                tl = timeline.Timeline(run_metadata.step_stats)
                ctf = tl.generate_chrome_trace_format()
                with open('{}/timeline_parallel_{}.json'
                          .format(curr_time, i), 'w') as f:
                    f.write(ctf)
                with open('{}/run_metadata_{}.txt'
                          .format(curr_time, i), 'w') as f:
                    f.write(
                        google.protobuf.text_format.MessageToString(
                            run_metadata))

    # Print stats
    print 'Histogram counts: {}'.format(
        np.histogram(train_learning_times[2:])[0])
    print 'Histogram edges: {}'.format(
        np.histogram(train_learning_times[2:])[1])
    print 'Median time for learning: {:5.2f}'.format(
        np.median(train_learning_times))

What other attempted solutions have you tried?
No other solutions tried.
Logs or other output that would be helpful
Dump of GraphDef and timeline json for each of 100 runs.
2016-11-05-18-52-32.zip
Output of code with log_metadata set to True:
parallel.txt
Output of code with log_metadata set to False:
parallel-nometadata.txt