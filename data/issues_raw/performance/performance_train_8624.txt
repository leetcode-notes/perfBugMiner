Constant folding does not work across devices?

I've been trying to understand tensorflow internals recently. I found in tensorflow/core/common_runtime/direct_session.cc, if I understand it correctly, that constant folding only take place at #L1051 after graph partitioning, so constants won't propagate through device boundary.
This is also evidenced by a simple experiment:
with tf.device('gpu'):
    a = tf.constant(0)
with tf.device('cpu'):
    b = a + 1
    c = b + 1

Resulting computation time graph is

Whereas placing all ops on GPU gives a fully shaded graph.
Did I miss something? Or is there any consideration not to run constant folding before partitioning the graph?