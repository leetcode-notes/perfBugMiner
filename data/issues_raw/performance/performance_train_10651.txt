Different results using tf.train.batch and tf.train.shuffle_batch

System information

Custom code:
Yes
OS Platform and Distribution:
Linux Ubuntu 16.04
TensorFlow installed from:
binary
TensorFlow version (use command below):
v1.1.0-rc0-61-g1ec6ed5
CUDA/cuDNN version:
CUDA 8 and cuDNN 5.1
GPU model and memory:
GTX Titan X (Maxwell) with 12GB of RAM

Hello,
I am currently working on sequential data with variable length and would like to build a dynamic graph with respect to the "time" of the sequence. Because tf.train.shuffle_batch does not handle dynamic padding, the only official solution is to work with tf.train.batch. To assert the problem, I am preprocessing the data before batching to get sequences with constant length in order to be able to use shuffle_batch and tf.train.batch to compare them. So, the only difference between the two compared pipelines is the swap between tf.train.batch and tf.train.shuffle_batch. Problem : the results drop significantly at test time using tf.train.batch instead of tf.train.shuffle_batch.

My database is self made and I believe properly shuffled initially : the database is cross subject and all occurrences of a user are shuffled using Python's random.shuffle (the size of the lists is small enough for the pseudo random to be relevant), then added to the TFRecords.
Queue randomization doesn't seem to be the problem either : using standy66's workaround showed in #5147 doesn't change the outcome of the tf.train.batch pipeline.
Should we expect different behaviors from these two functions (except for the random queue shuffling) or is this behavior abnormal?
Thanks,
Quentin