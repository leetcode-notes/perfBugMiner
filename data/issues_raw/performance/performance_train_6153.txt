Training on GPU and CPU with the same code differs significantly.

I am training a TF network on two machines. It is the same code, which is basically a modification of the MNIST example. In one case, I use GPU on Amazon Cloud, in the other I use a standard CPU Intel Xeon. I have the same version of TF, the same code, the same parameters, the same packages the and same Ubuntu distribution.
In one case on GPU the learning process is significantly different, and particularly slower. It reaches only 0.60 performance while CPU one reaches 0.90. Increasing learning rate by 10 I managed to get the GPU result to 0.90 too. Have you encountered this problem before? Is this intended?