Cannot use AdamOptimizer in C++ when running graph defined from Python

Running Ubuntu 14.04, with r1.2 Tensorflow
I have defined a graph in Python
features = 784
output_dim = 10
graph_name = "graph.pb"
graph_folder = "/home/fran/Repositories/tensorflow/bazel-bin/tensorflow/test"
input_node_name = "input"
output_node_name = "output"

with tf.Session() as session:
    x = tf.placeholder(tf.float32, [None, features], name="input")
    W = tf.Variable(tf.zeros([features, output_dim]), dtype=tf.float32, name="Weight")
    b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32, name="bias")
    y_pred = tf.nn.softmax(tf.matmul(x, W)+b, name=output_node_name)

    y_true = tf.placeholder(tf.float32, [None, 10], name="y_true")
    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]), name="loss")
    optimizer = tf.train.AdamOptimizer(0.00001)
    train_step = optimizer.minimize(cross_entropy, name="train")
    init = tf.variables_initializer(tf.global_variables(), name='init_all_vars_op')
    saver = tf.train.Saver()
    tf.train.write_graph(session.graph_def, graph_folder, graph_name, as_text=False)

If I run this graph in a C++ program, where I load the graph, initialize the variables and feed X and Y tensors to "train", I get the following error (works fine with other optimizers like GradientDescent or RMSPropOptimizer):
Training.
Step: 0; Loss: 9.25913
2017-07-03 09:54:43.264264: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]
2017-07-03 09:54:43.264468: F tensorflow/driving_school/training.cc:102] Non-OK-status: session->Run(inputs, {}, {"train"}, nullptr) status: Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]
Aborted (core dumped)

Is this a bug?
For reference, the code I use to load the graph definition and train is the following:
#include "tensorflow/core/public/session.h"
#include "tensorflow/core/platform/env.h"

using namespace tensorflow;

int main(int argc, char* argv[]) {

    Session* session;
    std::cout << "Initializing Tensorflow session." << std::endl;
    TF_CHECK_OK(NewSession(SessionOptions(), &session));

    // Read in the protobuf graph we previously exported
    // (The path is relative to the cwd. Keep this in mind
    // when using `bazel run` since the cwd isn't where you call
    // `bazel run` but from inside a temp folder.)
    GraphDef graph_def;
    std::cout << "Reading protobuf graph." << std::endl;
    TF_CHECK_OK(ReadBinaryProto(Env::Default(), "graph.pb", &graph_def));

    std::cout << "Loading graph in Tensorflow session." << std::endl;
    TF_CHECK_OK(session->Create(graph_def));

    int batch_size = 64;
    int input_feature_len = 784;
    int output_feature_len = 10;
    int training_steps = 1000;
    int save_interval = 100;
    std::string save_path = "/home/fran/Repositories/tensorflow/tensorflow/test/";

    std::cout << "Initializing Tensorflow I/O tensors." << std::endl;
    Tensor X(DT_FLOAT, TensorShape({batch_size, input_feature_len}));
    Tensor Y(DT_FLOAT, TensorShape({batch_size, output_feature_len}));

    // Initialize our variables for training
    std::cout << "Initializing Tensorflow variables." << std::endl;
    TF_CHECK_OK(session->Run({}, {}, {"init_all_vars_op"}, nullptr));

    auto _XTensor = X.matrix<float>();
    auto _YTensor = Y.matrix<float>();
    _XTensor.setRandom();
    _YTensor.setRandom();

    // Assuming graph contains single input X, with node name "input"
    // and single ground truth placeholder, with node name "y_true"
    std::vector<std::pair<string, Tensor>> inputs = {
            { "input", X },
            { "y_true", Y }
    };

    // for checkpoint generation
    Tensor model_string(DT_STRING, TensorShape( { 1, 1 } ) );

    // Initialize output pointer for loss metric.
    // Assuming graph contains single loss, with node name "loss"
    // and single training node, with node name "train"
    std::cout << "Training." << std::endl;
    std::vector<Tensor> output;
    for (int i=0; i<training_steps; ++i) {
        TF_CHECK_OK(session->Run(inputs, {"loss"}, {}, &output)); // Get loss (for monitoring)
        float loss = output[0].scalar<float>()(0);
        std::cout << "Step: " << i << "; Loss: " << loss << std::endl;
        TF_CHECK_OK(session->Run(inputs, {}, {"train"}, nullptr)); // Train
        output.clear();

        // to save checkpoint feed name of checkpoint file as saver_def.filename_tensor_name
        // and fetch the value of saver_def.save_tensor_name, as per graph definition script.
        if (i % save_interval == 0){
            model_string.matrix< std::string >()( 0, 0 ) = save_path + "model_checkpoint_step" + std::to_string(i) + ".ckpt";
            TF_CHECK_OK(session->Run({{"save/Const:0", model_string}}, {}, {"save/control_dependency"}, nullptr));
        };
    };
    std::cout << "Training complete." << std::endl;
    
    std::cout << "Closing Tensorflow session." << std::endl;
    session->Close();
    delete session;
    return 0;
}