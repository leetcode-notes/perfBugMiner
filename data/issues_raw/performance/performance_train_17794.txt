how to write the objectives function roc_auc_score in tflearn by keras

Dear everyone,
I found the roc_auc_score function in  https://github.com/tflearn/tflearn/blob/master/tflearn/objectives.py. Now I want to write this function by keras. But failed. The following is the code:
def roc_auc_score(y_pred, y_true):
    """ ROC AUC Score.
    Approximates the Area Under Curve score, using approximation based on
    the Wilcoxon-Mann-Whitney U statistic.
    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).
    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.
    Measures overall performance for a full range of threshold levels.
    Arguments:
        y_pred: `Tensor`. Predicted values.
        y_true: `Tensor` . Targets (labels), a probability distribution.
    """
    with tf.name_scope("RocAucScore"):
        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))
        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))
        pos = tf.expand_dims(pos, 0)
        neg = tf.expand_dims(neg, 1)
        # original paper suggests performance is robust to exact parameter choice
        gamma = 0.2
        p     = 3
        difference = tf.zeros_like(pos * neg) + pos - neg - gamma
        masked = tf.boolean_mask(difference, difference < 0.0)
        return tf.reduce_sum(tf.pow(-masked, p))

The new code was changed to the following:
def roc_auc_score(y_pred, y_true):
    """ ROC AUC Score.
    Approximates the Area Under Curve score, using approximation based on
    the Wilcoxon-Mann-Whitney U statistic.
    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).
    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.
    Measures overall performance for a full range of threshold levels.
    Arguments:
        y_pred: `Tensor`. Predicted values.
        y_true: `Tensor` . Targets (labels), a probability distribution.
    """
    pos = tf.boolean_mask(y_pred, K.cast(y_true, tf.bool))
    neg = tf.boolean_mask(y_pred, ~K.cast(y_true, tf.bool))
    pos = K.expand_dims(pos, 0)
    neg = K.expand_dims(neg, 1)
    # original paper suggests performance is robust to exact parameter choice
    gamma = 0.2
    p     = 3
    difference = K.zeros_like(pos * neg) + pos - neg - gamma
    masked = tf.boolean_mask(difference, difference < 0.0)
    return K.sum(K.pow(-masked, p))

All the code are the following, it doesn't work,
The data in
https://pan.baidu.com/s/12yJCWdfvVW1tEKUfEU34RQ ,  password: nu98
# -*- coding: UTF-8 -*-
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import numpy as np
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras import optimizers
from keras import applications
from keras.models import Model
import keras
import numpy as np
from keras import backend as K
import tensorflow as tf


print("---------------------------AUC----------------------------------------")


# def roc_auc_score(y_pred, y_true):
#     """ ROC AUC Score.
#     Approximates the Area Under Curve score, using approximation based on
#     the Wilcoxon-Mann-Whitney U statistic.
#     Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).
#     Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.
#     Measures overall performance for a full range of threshold levels.
#     Arguments:
#         y_pred: `Tensor`. Predicted values.
#         y_true: `Tensor` . Targets (labels), a probability distribution.
#     """
#     with tf.name_scope("RocAucScore"):

#         pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))
#         neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))

#         pos = tf.expand_dims(pos, 0)
#         neg = tf.expand_dims(neg, 1)

#         # original paper suggests performance is robust to exact parameter choice
#         gamma = 0.2
#         p     = 3

#         difference = tf.zeros_like(pos * neg) + pos - neg - gamma

#         masked = tf.boolean_mask(difference, difference < 0.0)

#         return tf.reduce_sum(tf.pow(-masked, p))



def roc_auc_score(y_pred, y_true):
    """ ROC AUC Score.
    Approximates the Area Under Curve score, using approximation based on
    the Wilcoxon-Mann-Whitney U statistic.
    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).
    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.
    Measures overall performance for a full range of threshold levels.
    Arguments:
        y_pred: `Tensor`. Predicted values.
        y_true: `Tensor` . Targets (labels), a probability distribution.
    """
    pos = tf.boolean_mask(y_pred, K.cast(y_true, tf.bool))
    neg = tf.boolean_mask(y_pred, ~K.cast(y_true, tf.bool))
    pos = K.expand_dims(pos, 0)
    neg = K.expand_dims(neg, 1)
    # original paper suggests performance is robust to exact parameter choice
    gamma = 0.2
    p     = 3
    difference = K.zeros_like(pos * neg) + pos - neg - gamma
    masked = tf.boolean_mask(difference, difference < 0.0)
    return K.sum(K.pow(-masked, p))

def roc_auc_score_loss(y_true, y_pred):
    return roc_auc_score(y_true, y_pred)
print("---------------------------AUC----------------------------------------")


# dimensions of our images.
img_width, img_height = 512, 512

train_data_dir = 'data/train/'
validation_data_dir = 'data/validation/'


##preprocessing
# used to rescale the pixel values from [0, 255] to [0, 1] interval
datagen = ImageDataGenerator(rescale=1./255)
batch_size = 32

# automagically retrieve images and their classes for train and validation sets
train_generator = datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='binary')

validation_generator = datagen.flow_from_directory(
        validation_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='binary',shuffle=False)



# a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers.
model = Sequential()
model.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# model.compile(loss='binary_crossentropy',
#               optimizer='rmsprop',
#               metrics=['accuracy','mae'])

model.compile(loss=roc_auc_score_loss,
              optimizer='rmsprop',
              metrics=['accuracy','mae',roc_auc_score])
epochs = 5

train_samples = 2048
validation_samples = 832


model.fit_generator(
        train_generator,
        steps_per_epoch=train_samples // batch_size,
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps=validation_samples// batch_size)

model.save_weights('models/basic_cnn_30_epochs.h5')
print(model.summary())



The problem is the following :+1:
ValueError                                Traceback (most recent call last)
 in ()
149         epochs=epochs,
150         validation_data=validation_generator,
--> 151         validation_steps=validation_samples// batch_size)
152
153 model.save_weights('models/basic_cnn_30_epochs.h5')
/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
85                 warnings.warn('Update your ' + object_name + 86 ' call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
88         wrapper._original_function = func
89         return wrapper
/usr/local/lib/python2.7/dist-packages/keras/models.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)
1119                                         workers=workers,
1120                                         use_multiprocessing=use_multiprocessing,
-> 1121                                         initial_epoch=initial_epoch)
1122
1123     @interfaces.legacy_generator_methods_support
/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
85                 warnings.warn('Update your ' + object_name + 86 ' call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
88         wrapper._original_function = func
89         return wrapper
/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
1924
1925         do_validation = bool(validation_data)
-> 1926         self._make_train_function()
1927         if do_validation:
1928             self._make_test_function()
/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _make_train_function(self)
958                     training_updates = self.optimizer.get_updates(
959                         params=self._collected_trainable_weights,
--> 960                         loss=self.total_loss)
961                 updates = self.updates + training_updates
962                 # Gets loss and metrics. Updates weights at each call.
/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
85                 warnings.warn('Update your ' + object_name + 86 ' call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
88         wrapper._original_function = func
89         return wrapper
/usr/local/lib/python2.7/dist-packages/keras/optimizers.pyc in get_updates(self, loss, params)
235         for p, g, a in zip(params, grads, accumulators):
236             # update accumulator
--> 237             new_a = self.rho * a + (1. - self.rho) * K.square(g)
238             self.updates.append(K.update(a, new_a))
239             new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)
/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in square(x)
1356         A tensor.
1357     """
-> 1358     return tf.square(x)
1359
1360
/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc in square(x, name)
447           indices=x.indices, values=x_square, dense_shape=x.dense_shape)
448     else:
--> 449       return gen_math_ops.square(x, name=name)
450
451
/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc in square(x, name)
4565   if _ctx.in_graph_mode():
4566     _, _, _op = _op_def_lib._apply_op_helper(
-> 4567         "Square", x=x, name=name)
4568     _result = _op.outputs[:]
4569     _inputs_flat = _op.inputs
/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)
526               raise ValueError(
527                   "Tried to convert '%s' to a tensor and failed. Error: %s" %
--> 528                   (input_name, err))
529             prefix = ("Input '%s' of '%s' Op has type %s that does not match" %
530                       (input_name, op_type_name, observed))
ValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.
Finally, Anyone can check this problem. Looking forward to reply. Thanks advanced!
System information
Operating System: Ubuntu 16.04 LTS
Graphics card: Tesla K40
Installed version of CUDA: 8.0
Installed version of cuDNN: v5 , for CUDA 8.0
pip --version 9.0.1
pip 9.0.1 from /usr/local/lib/python2.7/dist-packages (python 2.7)
pip install tensorflow-gpu
Name: tensorflow-gpu, Version: 1.4.1