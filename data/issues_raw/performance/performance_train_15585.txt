FP16 slower than FP32

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Partly
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
RHEL 7
TensorFlow installed from (source or binary):
unknown
TensorFlow version (use command below):
('unknown', '1.4.0')
Python version:
python2.7
Bazel version (if compiling from source):
unknown
GCC/Compiler version (if compiling from source):
gcc 4.8
CUDA/cuDNN version:
9.0 / 7.0
GPU model and memory:
Pascal P-100
Exact command to reproduce:
N/A

Describe the problem
I tried running the tutorial/rnn/ptb and tutorial/images/mnist examples with the --use_fp16 option, to see if speedup was achievable by utilizing the new half-precision features.
It turned out that a single training step for MNIST with FP32 took 3.3ms, with FP16 it was 4ms. For PTB small (I had to use lstm_cell=basic, because other types are not yet supported in FP16), the WPS dropped from 24000 to 22000 when switching to FP16.
So the performance decreases when using FP16 in real-world examples.
To check this, I've created a small benchmark for myself (since I can't get the nightly tf build on my machine, I can't run the official benchmarks), which is basically one big matrix multiplication in Tensorflow.
I've run it with square matrices with a width of 8k, 16k and 32k. In each case, FP16 and FP32 yielded nearly the same runtime. Profiling with nvprof I found out that the used CUDA function is maxwell_fp16_segmemm_fp16_128x128_nn for FP16 and sgemm_128x128x8_NN_vec for FP32.
Since I wanted to double check if matrix multiplication in FP16 is really slower than in FP32 on my GPU, I tried to directly benchmark the GPU using cuBlas with a similar operation. It turns out that here, FP16 is nearly twice as fast as FP32. CuBlas internally uses maxwell_hgemm_256x128_nn for matrix multiplication of 16k x 16k square matrices in FP16. (again  according to the nvprof profiler)
So I'm wondering why Tensorflow is unable to achieve similar results in terms of speed, or if I'm doing something wrong in my tests.
Source code
Tensorflow Code snippet:
    graph = tf.Graph()
        with graph.as_default():
          tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))
          tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))
          tf_output = tf.matmul(tf_input1, tf_input2)
  
      with tf.Session(graph=graph) as session:
          tf.global_variables_initializer().run()
          print("Initialized")
          for i in range(FLAGS.times):
              out = session.run([tf_output])#, feed_dict=feed_dict)
          print("Done")

cuBlas Code FP16 (Snippet):
        uint16_t *d_a;          // d_a - a on the device
        uint16_t *d_b;          // d_b - b on the device
        uint16_t *d_c;          // d_c - c on the device
        cudaStat = cudaMalloc ((void **) &d_a, m * k * sizeof (*a));    // device memory alloc for a
        cudaStat = cudaMalloc ((void **) &d_b, k * n * sizeof (*b));    // device memory alloc for b
        cudaStat = cudaMalloc ((void **) &d_c, m * n * sizeof (*c));    // device memory alloc for c
        stat = cublasCreate (&handle);  // initialize CUBLAS context
        // copy matrices from the host to the device
        stat = cublasSetMatrix (m, k, sizeof (*a), a, m, d_a, m);   //a -> d_a
        stat = cublasSetMatrix (k, n, sizeof (*b), b, k, d_b, k);   //b -> d_b
        stat = cublasSetMatrix (m, n, sizeof (*c), c, m, d_c, m);   //c -> d_c
        uint16_t al = FP_16_ONE;       // al = 1 
        uint16_t bet = FP_16_ONE;      // bet =1
        // matrix - matrix multiplication : d_c = al*d_a *d_b + bet *d_c
        // d_a -mxk matrix , d_b -kxn matrix , d_c -mxn matrix ;
        // al ,bet -scalars

        stat = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &al, d_a, CUDA_R_16F, m, d_b, CUDA_R_16F, k, &bet, d_c, CUDA_R_16F, m, CUDA_R_16F, CUBLAS_GEMM_DEFAULT); 

        stat = cublasGetMatrix (m, n, sizeof (*c), d_c, m, c, m);   // cp d_c - >c