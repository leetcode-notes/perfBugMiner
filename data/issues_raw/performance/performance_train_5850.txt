adding zero-sized layer costs huge amount of memory, but without increasing total # of trainable parameters

In order to make code generic, my code does append a layer with a variable-sized layer which might be of length 0, i.e.
tv_concat = tf.concat(1, [tf.reshape(tf.slice(X,[0,time_step,0],[-1,Ncontext,-1]), [tp_current_batch_size,embed_size*Ncontext]), tv_hh])
In the above code, X is a tensor of shape [batch_size, max_time_step, state_size], tv_hh of shape [batch_size, state_size]. Depending on the value of Ncontext, a differently sized layer will be concatenated with the existing layer tv_hh. So when Ncontext=0, a zero-size layer will be concatenated so that tv_concat will be the same as tv_hh. I have explicitly checked and confirmed that tv_concat is indeed of the same shape as tv_hh. I have also checked that the total number of trainable parameters are the same.
However, when Ncontext=0, the graphics memory consumption is >3G, if I use tv_concat=tv_hh, the graphics memory consumption is only 2G.
So is this behaviour expected?