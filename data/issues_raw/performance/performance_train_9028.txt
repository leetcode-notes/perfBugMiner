XLA with tower parallelization

You must complete this information or else your issue will be closed

Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?: yes
TensorFlow installed from (source or binary)?: source
TensorFlow version: 1.1rc1
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: 8.0/5.1
GPU Model and Memory: Titan X (12 GB)
Exact command to reproduce: NA

Describe the problem clearly
Using the automatic XLA optionsess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1 causes an 8x performance improvement for my model (while training) when running with 1 GPU, amazing! I would like to parallelize this with a standard tower setup, without XLA this scales linearly (using 4 GPUs) but with XLA I see no difference. The timeline profiler (chrome://tracing) indicates that it just executes each tower one at the time on each GPU.
The example in tensorflow/models/neural_gpu/neural_gpu.py indicates that one can use the manual tf.contrib.compiler.jit.experimental_jit_scope context creator, using this causes a x6 times performance peanality compared to not using XLA, but does scale with multiple GPUs.
I would like a feature where experimental_jit_scope works similarly to the automatic sess_config XLA option or the sess_config works with multiple GPUs.
Source Code / Logs
My model is almost identical to the one in https://github.com/buriburisuri/ByteNet.