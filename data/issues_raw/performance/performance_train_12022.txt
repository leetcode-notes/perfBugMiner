Error exporting TensorForestEstimator model for serving

Problem
I am trying to host a TensorForestEstimator model on Google Cloud's ML Engine. Everything works right, but at the very end the model fails to export with stack trace:
Traceback (most recent call last):
[...]
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 502, in train_and_evaluate
  export_results = self._maybe_export(eval_result)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 597, in _maybe_export
  eval_result=eval_result))
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py", line 87, in export
  return self.export_fn(estimator, export_path, **kwargs)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 412, in export_fn
  checkpoint_path=checkpoint_path)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 1280, in export_savedmodel
  actual_default_output_alternative_key)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 252, in build_all_signature_defs
  for input_key, inputs in input_alternatives.items()
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 254, in <dictcomp>
  in output_alternatives.items()}
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 119, in build_standardized_signature_def
  input_tensors, output_tensors)
File "/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py", line 146, in predict_signature_def
  signature_constants.PREDICT_METHOD_NAME)
File "/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py", line 45, in build_signature_def
  signature_def.outputs[item].CopyFrom(outputs[item])
TypeError: None has type NoneType, but expected one of: bytes, unicode

Based on that trace, I'm thinking the error is in the make_export_strategy function with default_output_alternative_key=None. So what I did is set default_output_alternative_key='default' but then got the error:
ValueError: Requested default_output_alternative: default, but available output_alternatives are: [None]

So this shows that there are no output alternatives and my model is single-headed. Here is the code:
def serving_input_fn():
    feature_placeholders = {
    column['name']: tf.placeholder(dtype=column['dtype'], shape=[None])
    for column in columns_list if column['derived'] == 'N' and column['column_role'] != 'label'
    }

    features = {
        key: tf.expand_dims(tensor, -1)
        for key, tensor in feature_placeholders.items()
    }

    return InputFnOps(
        features=features,
        labels=None,
        default_inputs=feature_placeholders
    )

def get_experiment_fn(args):
    def _experiment(run_config, hparams):
        return Experiment(
            estimator=TensorForestEstimator(
                params=ForestHParams(
                    num_trees=args.num_trees,
                    max_nodes=10000,
                    min_split_samples=2,
                    num_features=7,
                    num_classes=args.num_projections,
                    regression=True
                ),
                model_dir=args.job_dir,
                graph_builder_class=RandomForestGraphs,
                config=run_config,
                report_feature_importances=True,
            ),
            train_input_fn=get_input_fn(
                project_name=args.project,
                data_location=args.train_data,
                dataset_size=args.train_size,
                batch_size=args.train_batch_size
            ),
            train_steps=args.train_steps,
            eval_input_fn=get_input_fn(
                project_name=args.project,
                data_location=args.eval_data,
                dataset_size=args.eval_size,
                batch_size=args.eval_batch_size
            ),
            eval_steps=args.eval_steps,
            eval_metrics=get_eval_metrics(),
            export_strategies=[
                make_export_strategy(
                    serving_input_fn,
                    default_output_alternative_key=None,
                    exports_to_keep=1
                )
            ]
        )
    return _experiment


def main():
    args = get_arg_parser().parse_args()

    learn_runner.run(
        experiment_fn=get_experiment_fn(args),
        run_config=RunConfig(model_dir=args.job_dir),
        hparams=HParams(**args.__dict__)
    )

if __name__ == '__main__':
    main()

This seems like a bug, but I could be wrong.
System information
== cat /etc/issue ===============================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
Mac OS X 10.12.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
protobuf (3.1.0.post1)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================