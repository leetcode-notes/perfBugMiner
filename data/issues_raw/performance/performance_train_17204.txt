HtoD takes 2.5x longer than D2H

I'm noticing that HtoD copies are taking significantly longer than DtoH.
IE, doing sess.run(gpu_var) vs sess.run(assign_gpu_var, feed_dict={gpu_var.initial_value: arr})
100MB tensor takes 8ms on V100 to fetch (12.5GB/sec), but 21.67ms to feed (4.5 GB/sec)
Benchmark
https://github.com/diux-dev/cluster/blob/db10c890530e7ded9e4a933596803e3ae0de1db0/yuxin_numpy/square_minimize_cpu_pipeline.py
Is there a way to make it faster? (something to do with memory pinning?)

TensorFlow:
version: 1.5.0
git_version: v1.5.0-0-g37aa430d84
37aa430