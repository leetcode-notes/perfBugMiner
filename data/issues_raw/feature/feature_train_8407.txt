Support for 'paged' tensor

I'm curious if it is technically feasible to support 'paged' tensors - by this I mean a tensor that seamlessly spills over into main memory when it overflows GPU memory and intelligently pages in and out from main memory.
My interest is in processing very large volumes of microscopy data - on the order of a couple thousand voxels to each axis. They don't even come close to fitting into GPU memory, one of the smaller volumes takes a full 250G to run on CPU.  My model in this case is fully spatially convolutional so a divide and conquer strategy into blocks works fine, however there's a lot of wasted computation in recalculating overlapping activations.
This particular use case may be somewhat rare, but I think this would be a really really useful thing to have around. Very curious if this is at all feasible and if automatic device allocation/management stuff like this in the works.