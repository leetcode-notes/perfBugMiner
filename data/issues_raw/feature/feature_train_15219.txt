tf.while_loop and tf.foldl do not support second order gradients

The example
import tensorflow as tf
x = tf.Variable(1.)
A = tf.Variable(tf.ones((3,3))) 
cost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))
tf.gradients(tf.gradients(cost, A), x)  
# TypeError: Second-order gradient for while loops not supported.
illustrates that despite applying tf.foldl to a static list, the internal implementation via while loops leads to a type error. The problem disappears if the fold operation is carried out manually using a for loop. While implementing foldl using the while loop clearly makes the operation  more widely applicable, it seems problematic if syntactic sugar can lead to code that has qualitative differences from a naive implementation using a static loop. I cannot help but wonder whether foldl could be more efficient in the static case as well, although that is more of a conjecture.
I think it would be nice if foldl (and other while loop derivatives) had a keyword that enabled or disabled the "dynamic mode" using while, or if, at the very least, the TypeError would occur at the foldloperation so that the error is easier to trace.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Custom code.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04 LTS
TensorFlow installed from (source or binary):
pip install.
TensorFlow version (use command below):
v1.4.0-rc1-11-g130a514 1.4.0
Python version:
3.5.4
Bazel version (if compiling from source):
Not applicable.
GCC/Compiler version (if compiling from source):
Not applicable.
CUDA/cuDNN version:
Did not use CUDA.
GPU model and memory:
Did not use GPU.
Exact command to reproduce:

import tensorflow as tf
x = tf.Variable(1.)
A = tf.Variable(tf.ones((3,3))) 
cost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))
tf.gradients(tf.gradients(cost, A), x)  
# TypeError: Second-order gradient for while loops not supported.