[idea] new Op: Conv2DWithBiasActivation for Backends that Support Higher Level Op

This is a repost of the comment here:
Along the lines of MklConv2DWithBias, I suggest defining a new primitive Op, Conv2DWithBiasActivation that can be used to define kernels on platforms that support a higher level primitive conv-bias-acitivation unit.
As far as the (new) fused Conv2D Op that I am using for my BNNS implementation, I used the MKL Conv2D Op as a predicate. In that case Intel has defined a new Op that includes both the convolution Op and the BiasAdd Op. My Conv Op actually isn't pulling in just the activation function, rather, my Op and the MKL both have the bias add has been fused as well.
I understand that the primitive Conv2D op in TF does not currently handle activations or bias addition, but this op may be "too primitive". In fact cuDNN is now moving in the direction of fusing these three into one kernel (see: #8828). Perhaps, a different FusedConv2D Op should be added that can be used by not just BNNS but also cuDNN v6 and any other underlying platform implementations that can take advantage of doing all three ops together. This would be  along the lines of batch norm, which now offers a fused version of the Op to reduce the number of primitive Ops used.
Addendum
There are even more implementations that can benefit from fusing Conv-Bias-Activation. Others include:

Metal Performance Shaders (#7958). Here it might be extra extra important because of 1. avoiding extra copies back and forth from GPU memory (ie using the tuned MPSTemporaryâ€‹Image and 2. because MPS uses a very weird memory layout and ideally transposing to this only needs to be done once.
MKL (as linked above, perhaps the MKL specific Op (MklConv2DWithBias) can be unified with this new Op.
cuDNN v6

/CC @drpngx @petewarden @flx42 @gunan