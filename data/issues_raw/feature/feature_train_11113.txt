Add cosine annealing for learning rate decay

SGDR: Stochastic Gradient Descent With Warm Restarts, proposes decaying the learning rate according to

where  is the minimum step length,  is the maximum step length,  is the global step and  is the maximum number of iterations.
I've personally found this strategy to  be easy to use given that the number of hyperparameters is relatively small and results are good.
Is this something we want added to tensorflow? Would you accept submissions?