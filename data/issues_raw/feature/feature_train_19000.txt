Feature Request: get Estimator from binary model (.pb file)

System information

Have I written custom code: No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra 10.13.4
TensorFlow installed from (source or binary): binary (through pip)
TensorFlow version (use command below): 1.7
Python version: 3.6
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: not using CUDA, just CPU for now
GPU model and memory: -
Exact command to reproduce: -

Describe the problem
I am using the Estimator interface to train and evaluate  my model. It is really cool.
However, since I am developing for mobile, after training I must also freeze, optimize, quantize the model and then port it to TFLite.
In these steps, my model must be in binary format (.pb).
It would be nice to build an Estimator from a .pb file to allow evaluation of binary models.
For instance, I need to check how much accuracy I am losing after the quantization step.
However, I believe that at the moment it is not possible to instantiate an Estimator from a binary model.
I have done some research before opening this issue:

I have searched the TF documentation but did not find anything about loading a binary model into an Estimator;
I have tried to ask this question on StackOverflow three weeks ago (here is the link), but got no answers.

As a consequence, at the moment I am just using the old GraphDef and feed_dict interface to perform evaluation and prediction with binary models.
However using different interfaces for literally the same tasks just seems a bit... off.
Estimators probably use GraphDef under the hood. So it should be really easy to allow instantiation from a binary model.
Am I missing something? If not, can you implement this feature please?
Thanks for your support!
Andrea