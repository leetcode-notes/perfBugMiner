Distributed TensorFlow without shared directory

It seems like there is an inherent assumption in Distributed TensorFlow that all nodes must share a common file system, such as google cloud or NFS.
I've found in testing that models will train just fine without a common file system, but the final trained model doesn't save properly when you try something like:
builder = tf.saved_model_builder.SavedModelBuilder(export_dir) ... builder.save()
The issue seems to be that the parameter server has the variables and the chief node has the graph.
It'd be great if TensorFlow added a function to allow us to consolidate the graph and variables at the end of training onto the chief node in order to save the trained model. Right now there doesn't seem to be an easy way to do this.
Thanks.