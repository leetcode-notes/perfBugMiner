GPU PoolAllocator never satisfied with the eviction rate. Can we limit its allocated size?

I have a tensorflow network where I call sess.run() with tensors of widly changing sizes. (to be more precise it takes two tensors as input a [None, E, None, None] and a [None, None, M]). Additionally, I use some tf.where calls which generate variable sized tensors to be transferred to the GPU.
Everything works fine and training goes nicely and quickly, but eventually it seems that the pool_allocator is never satisfied with the eviction rate it gets and constantly try to increase its size.
2017-03-20 18:24:07.895271: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 590087 get requests, put_count=590089 evicted_count=2000 eviction_rate=0.00338932 and unsatisfied allocation rate=0.0140911
2017-03-20 18:24:07.895342: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 69492 to 76441

Eventually, it fills the whole machine RAM (256GB....) and kills itself by running out-of-memory (in less than 1h...).
This behavior only happens when running on GPU. I checked that no ops are added during training through calling graph.finalize().
I also tried using tcmalloc as proposed here, and ran the profiler which was saying it was staying at a happy 500MB while the memory usage in top was multiple GB. As far as I understand, the pool_allocator uses his own malloc system so it would not show in tcmalloc profiler right?
tcmalloc profiler output
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

How to interpret PoolAllocator messages
How to debug a memory leak in TF
The question I asked here on SO, without much success.

Environment info
Operating System:
Ubuntu 14.04
CUDA 8.0 + cuDNN 5.1
python 3.5 nighty build
If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I honestly tried, but it seems that if I simplify the system the problem goes away.