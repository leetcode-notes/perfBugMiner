Feature: tf.contrib.rnn Downsampling Wrapper

Feature description

On sequential inputs with many timesteps (e.g. speech), it is common to down-sample the higher order representations in a multi-layer recurrent network to improve the computation time and eventually the attention weights learning [1]. The MultiRNNCell class in TensorFlow simplifies the construction of multi-layered RNNs but gives users no control to post-process the outputs of intermediate layers. Would it be possible to add a new cell wrapper implementing RNN layer output sub-sampling, or more general used-defined post-processing ? The simplest case could be down-sampling the outputs by a factor of 2, as in [1], section 3.1, but without concatenation.

Thanks,
George
[1] https://arxiv.org/abs/1508.01211