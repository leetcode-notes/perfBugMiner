Feature Request / Workaround for Variable size multi-label candidate sampling in TensorFlow.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): +
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS/Ubuntu 16.04
TensorFlow installed from (source or binary): both
TensorFlow version (use command below):  ('v1.3.0-rc2-20-g0787eee', '1.3.0')
Python version: 3.5
Bazel version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: n/a

Context:
Suppose we have a dataset with an arbitrary amount of labels per each training example (image segmentation, multi-label classification, etc.). Labels (classes) are NOT mutually exclusive, thus vary in size between examples.
Problem:
When trying to use provided standard nce_loss() -  a static int value for num_true option is required.

num_true: An int. The number of target classes per training example.

This probably works well for problems where we have same amount of labels per training examples and we know them in advance.
When labels have a variable shape [None], (and in our case, they are also being batched and bucketed by bucket size with .padded_batch() + .group_by_window()) it is necessary to supply a variable size num_true in order to accustom for all training examples. This is currently unsupported to my knowledge (correct me if I'm wrong).
Statement:
Is there a proper way to do this or any other workarounds? If not I would like to request a feature. If yes, I would appreciate a working example here or on stackoverflow.
Related stackoverflow question.
Desired behaviour:
nce_loss = tf.nn.nce_loss(
    weights=nce_weights,
    biases=nce_biases,
    labels=labels,
    inputs=inputs,
    num_sampled=num_sampled,
    num_true=(tf.shape(labels)[-1]), # or tf.placeholder("int32", [], name="num_trve")
    num_classes=self.num_classes)

Also, is it possible to add support for weighted losses to nce_loss() (specifically to _compute_sampled_logits() as it is partially generated from a .cc) in the same fashion as it is implemented in tf.losses.sparse_softmax_cross_entropy or tf.losses.sigmoid_cross_entropy?
Thanks in advance.