Feature Request: ADAG

It seems difficult to add ADAG  as an optimizer, because by default with tf.train.replica_device_setter(), all variables get assigned to a parameter server (ps).  Thus, it is difficult to update variables locally because the optimizer first pushes updates to the ps, allowing all other workers to see its updates.  If anyone know how to create local copies of variables, I can provide an example implementation.