Model Exporting Does not Use optimize_for_inference_lib

Looking at the code in master, It looks like neither tensorflow.python.training.saver nor tensorflow.contrib.session_bundle.exporter use optimize_for_inference_lib which is described as:

There are several common transformations ... that help reduce the amount of computation needed when the network is used only for inference.

Further these optimizations are not mentioned / suggested in the tensorflow serving docs:

https://tensorflow.github.io/serving/
https://github.com/tensorflow/tensorflow/blob/55cb1b37133e6c0409a708a763fccf566580a90a/tensorflow/contrib/session_bundle/README.md

Arguable the serving logic could be changed to use frozen models (see freeze_graph) which calls convert_variables_to_constants.