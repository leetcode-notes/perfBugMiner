Connect Apache Beam/Spark to TensorFlow (MonitoredTrainingSession) in a streaming manner?

Describe the problem
I have a lengthy question on SO about this. But in short, is there a way (or a best practice) to pipe big training datasets directly into a distributed setting (e.g. GKE),  especially if they are subjected to a heavy preprocessing?
I'm basically reaching the limit of what can be sanely stored in TFRecords (they are verbose and heavy).
The closest issue was this one (#12903) and this guide (https://github.com/GoogleCloudPlatform/dataflow-prediction-example) but I do not see a healthy way to implement it (last one with a @singleton looks like a hack and not usable with the tf.Dataset or MonitoredTrainingSession).
I believe this is a useful issue/feature request for a decent amount of Tensorflow users.