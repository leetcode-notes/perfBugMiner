reimplement core/util/cuda_kernel_helper.h?

Hi,
I'm trying to implement a GetCuda3DLaunchConfig into cuda_kernel_helper.h, but while reading the code, I feel it's a bit confusing and there might be a better way to implement it.
Here is the pointer:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h
In line 55-57, there is something like block_count = physical_thread_count / thread_per_block. Why use thread_per_block instead of virtual_thread_count?  The number of blocks can be higher than physical maximum and cuda will automatically put these blocks into queue. On the other hand, limiting the number of blocks to physical limit would make the computation incomplete when the number of threads is large.
See: http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png
The the kernel launch config computed is not optimal. I would suggest using the API provided by cuda >=6.5
See: https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/
Can anyone confirm what I said? If my suggestion make sense, I would like to reimplement these functions using cuda's occupancy api while writing my GetCuda3DLaunchConfig.