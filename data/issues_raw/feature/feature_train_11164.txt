Expose reader.read_up_to in slim parallel reader

When reading small records (in my case, one example is a a floatlist of about 150 floats), the DataSetProvider from slim is very slow. I found out that things get much faster if I write a custom input pipeline that leverages reader.read_up_to
def ReadTFRecord(filename_queue):
  num_tfrecords_at_once = 1024 
  reader = tf.TFRecordReader()
  _, queue_batch = reader.read_up_to(filename_queue, num_tfrecords_at_once)
  return [queue_batch]
the returned value is then fed to tf.train.shuffle_batch with enqueue_many set to true.
As far as I understand, this behavior is currently not exposed in slim.ParallelReader, see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/parallel_reader.py#L132. Are there any plans for adding it?