[Feature Request] Python - like behavior of tf.string_split. Consider whole multi-byte delimiter.

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS, Ubuntu
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):1.7
Python version: 3.6,3.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:No
GPU model and memory:
Exact command to reproduce:

Describe the problem
tf.string_split is documented as If delimiter contains multiple bytes, it is treated as a set of delimiters with each considered a potential split point. Hence, each character in a multi-byte string delimiter will become a separate delimiter, which is very non-intuitive and difficult to implement.
so for example, I want to split by to the token <eos>
tf.string_split(["hello world <eos> tensorflow"], delimiter="<eos>").values
# equal:     ['h', 'll', ' w', 'rld ', ' t', 'n', 'rfl', 'w']  
# should be ['hello world ', ' tensorflow]  

Can tf.string_split be reimplemented so that it consider the whole delimiter string, whether single or multiple character, to be its true delimiter instead making every single character in it delimiter?
Thank you