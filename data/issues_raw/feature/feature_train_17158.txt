Feature Request: Better error reporting for tflite conversion

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.5.0
Python version: 2.7.10
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: Not using
GPU model and memory: Radeon Pro 455
Exact command to reproduce: In macOS terminal: bazel-bin/tensorflow/contrib/lite/toco/toco 
--input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb 
--input_format=TENSORFLOW_GRAPHDEF 
--output_format=TFLITE 
--output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.lite 
--inference_type=FLOAT 
--input_arrays=input 
--output_arrays=output 
--input_shapes=1 
--output_shapes=1

You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the problem
Overview
I have been trying to convert a word2vec custom model into tensorflow lite. I am using the code in the tensorflow documentation that was provided as a simple word2vec example.The conversion process up to the freeze_graph.py works fine and I can use that file to run inference no problem in a python script on my desktop. My problem is when I try to convert to lite.
Model conversation seems to work
When I run the command I listed above I get a .tflite file. I can even visualize this file using bazel-bin/tensorflow/contrib/lite/tools/visualize /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec_model_viz.html. The model looks fine in the visualizer too.
Use on mobile doesn't work
When I implement my tflite model into an iOS app and try to grab a node with
float* out = interpreter->typed_tensor<float>();

I sometimes get a valid pointer back but other times I get a null pointer depending on the node.
After retraining my model with different shapes of inputs and outputs I finally noticed documentation at the end of the page https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md that states some operations that are present but not ready for custom models. I am using a couple of these operations and I'm guessing this is the reason my lite model isn't working.
My reqeust
Please update the errors available for tflite conversion. I don't understand why it would have been difficult to throw an error on tflite conversion that specifies a node and states that tflite doesn't support its operation. Videos posted by google employees and the introduction to tflite make it sound like this product is ready to make life easier when converting for mobile which is what encouraged me to give it a shot. Now I see that this isn't the case yet. If nothing else I would ask for the sanity of your users that tensorflow documentation more clearly warns about the limitations on mobile.
Tensorflow is a cool concept and the parts that are ready are awesome. Thanks for the great work!
My graph code:
valid_dataset = tf.constant(valid_examples, dtype=tf.int32,name="input")

embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name="embeddings")

norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True),name="norm")
normalized_embeddings = tf.div(embeddings, norm, 'normalized_embeddings')
valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset,name="valid_embeddings")
similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings), transpose_b=False,name="similarity")
output = tf.reshape(similarity, [-1], name="output")

Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Output from tflite conversion:
(adventures-in-ml-code) miperry-macOS:tensorflow miperry$ bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite   --inference_type=FLOAT   --input_arrays=input   --output_arrays=output   --input_shapes=1   --output_shapes=1000
2018-02-20 13:06:59.037245: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 21 arrays (0 quantized)
2018-02-20 13:06:59.038319: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 21 arrays (0 quantized)
2018-02-20 13:06:59.072631: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 5 operators, 11 arrays (0 quantized)
2018-02-20 13:06:59.072687: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)
2018-02-20 13:06:59.072731: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 2401216 bytes, theoretical optimal value: 240000