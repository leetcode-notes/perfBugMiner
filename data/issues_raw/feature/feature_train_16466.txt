[Feature request] Adding a PR curves to canned estimators for (binary) classifiers

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.4
Python version: 3.6.3
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Describe the problem
This is a feature request, and I'd be happy to contribute if you think it's a valuable addition.
I have been using estimators both pre-made and custom for classification tasks. I like that sharing the use of a head  as defined in tensorflow/python/estimator/canned/head.py allows both the canned models and the custom ones to share prediction and evaluation metrics for comparison, however currently it feels that some key metrics are missing, mainly PR curves which are fully supported by tensorboard.  Currently, the head constructor allows a list of thresholds, although they are not used by default. The problem is that when used, it creates scalar summaries for the precision and recall at each threshold, which is not that useful as in general one wants to compare how different models compare precision wise while fixing recall and the other way round.
Adding the PR summary op from here would make the eval metrics more informative IMO.
Thanks for taking the time to read this!
Source code / logs
Any code that uses pre-made estimator classifiers relies on the same head. This is one example.