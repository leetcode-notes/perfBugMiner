dilated convoluton uses a lot of memory

What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

#5083

Environment info
Operating System: Linux hpclogin2 2.6.32-642.15.1.el6.x86_64 #1 SMP Thu Feb 23 11:19:57 CST 2017 x86_64 x86_64 x86_64 GNU/Linux
Installed version of CUDA and cuDNN: 8.0 and 5.1

(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so -> libcublas.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so.8.0 -> libcublas.so.8.0.27
-rwxr-xr-x 1 sebo root  38838688 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so.8.0.27
-rw-r--r-- 1 sebo root  49345532 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas_device.a
-rw-r--r-- 1 sebo root  45050574 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas_static.a
-rw-r--r-- 1 sebo root    560184 Sep  1  2016 /appl/cuda/8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 sebo root    394472 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 sebo root    737516 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 sebo root        15 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so -> libcufft.so.8.0
lrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so.8.0 -> libcufft.so.8.0.27
-rwxr-xr-x 1 sebo root 146745600 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so.8.0.27
-rw-r--r-- 1 sebo root 129655446 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft_static.a
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so -> libcufftw.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so.8.0 -> libcufftw.so.8.0.27
-rwxr-xr-x 1 sebo root    456424 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so.8.0.27
-rw-r--r-- 1 sebo root     42134 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw_static.a
lrwxrwxrwx 1 sebo root        17 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so -> libcuinj64.so.8.0
lrwxrwxrwx 1 sebo root        20 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so.8.0 -> libcuinj64.so.8.0.27
-rwxr-xr-x 1 sebo root   6459464 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so.8.0.27
-rw-r--r-- 1 sebo root   1649302 Sep  1  2016 /appl/cuda/8.0/lib64/libculibos.a
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so -> libcurand.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so.8.0 -> libcurand.so.8.0.27
-rwxr-xr-x 1 sebo root  59057024 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so.8.0.27
-rw-r--r-- 1 sebo root  59273876 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand_static.a
lrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so -> libcusolver.so.8.0
lrwxrwxrwx 1 sebo root        21 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so.8.0 -> libcusolver.so.8.0.27
-rwxr-xr-x 1 sebo root  52380368 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so.8.0.27
-rw-r--r-- 1 sebo root  22313722 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver_static.a
lrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so -> libcusparse.so.8.0
lrwxrwxrwx 1 sebo root        21 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so.8.0 -> libcusparse.so.8.0.27
-rwxr-xr-x 1 sebo root  42976296 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so.8.0.27
-rw-r--r-- 1 sebo root  51604078 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse_static.a
```


If installed from source, provide

The commit hash (git rev-parse HEAD): 168d188168b30b204099f21e456151752d7fb718
The output of bazel version: 0.4.3

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import numpy as np
import sugartensor as stf
import tensorflow as tf


def get_variable(name, in_dim, out_dim, size=None):
    if size is None:
        size = 1
        shape = (in_dim, out_dim)
    else:
        shape = (size, in_dim, out_dim)

    w = tf.get_variable(name, shape, dtype=tf.float32,
                        initializer=tf.random_uniform_initializer(
                            minval=-np.sqrt(1 / (in_dim * size)),
                            maxval=np.sqrt(1 / (in_dim * size))
                        ))
    return w

# build forward pass
embedding = get_variable('embed', 128, 892)
embedding_inv = get_variable('embed-inv', 892, 128)

data = tf.placeholder(name='x', shape=(160, 200), dtype=tf.int32)
output = tf.nn.embedding_lookup(embedding, data)

for i in range(60):
    Wi = get_variable(f'W{i}', 892, 892, 5)
    output = tf.nn.convolution(input=output, filter=Wi,
                               padding='SAME', dilation_rate=[16],
                               name='aconv1d')
    output = tf.nn.relu(output)

logits = tf.reshape(tf.matmul(tf.reshape(output, [-1, 892]), embedding_inv),
                    [160, 200, 128])

# optimize for the idendity function
loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=data, logits=logits
))

# create update ops
optimizer = tf.train.AdamOptimizer()
grad_and_vars = optimizer.compute_gradients(loss, tf.trainable_variables())
update_ops = optimizer.apply_gradients(grad_and_vars)

config = tf.ConfigProto(allow_soft_placement=True)
with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(1000):
        loss_result = sess.run([loss, update_ops], feed_dict={
            data: np.random.randint(0, 128, size=(160, 200))
        })[0]
        print(f'iteration {i} complete: {loss_result}')
This example is perhaps too theoretical to be discussed from a practical application perspective. The actual application is the ByteNet model, the implementation is very similar to https://github.com/buriburisuri/ByteNet. The ByteNet model stacks multiple one-dimensional-dilated-convolutions (30), because each of them uses space_to_batch they use a lot of memory.
What other attempted solutions have you tried?
I've implemented one-dimensional-masked-dilated-convolutions using tf.scan, this uses much less memory but is also slower.
Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

The error log from the actual application: https://gist.github.com/AndreasMadsen/91e49e13f0085ececbef0f80c830c5af (note that this happens after 21334 iterations/14 hours, so there may also be a garbage collection issue)
The error log from the simplified example: https://gist.github.com/AndreasMadsen/94f5100aff697cdf5ff6c26f90a6dad7