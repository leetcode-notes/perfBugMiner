`tf.foldl` should have more robust input handling (like `tf.scan`)

System information

Windows 10 x64
Installed from binary
TensorFlow 1.4.0 (Cpu version)
Python 3.6.1

Bug Description
tf.foldl (and tf.foldr) are conceptually very very similar to tf.scan. Therefore the implementations are also very similar. However, tf.scan accepts initializer lists or tuples with varying type arguments, while tf.foldl does not. I think this is a simple oversight, and it seems that cutting and pasting some code from tf.scan to tf.foldl fixes this problem. Specifically. the master tf.foldlcode is (after removing the docstring):
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
  if not callable(fn):
    raise TypeError("fn must be callable.")

  with ops.name_scope(name, "foldl", [elems]):
    # Any get_variable calls in fn will cache the first call locally
    # and not issue repeated network I/O requests for each iteration.
    varscope = vs.get_variable_scope()
    varscope_caching_device_was_none = False
    if varscope.caching_device is None:
      # TODO(ebrevdo): Change to using colocate_with here and in other methods.
      varscope.set_caching_device(lambda op: op.device)
      varscope_caching_device_was_none = True

    # Convert elems to tensor array.
    elems = ops.convert_to_tensor(elems, name="elems")
    n = array_ops.shape(elems)[0]
    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                            dynamic_size=False,
                                            infer_shape=True)
    elems_ta = elems_ta.unstack(elems)

    if initializer is None:
      a = elems_ta.read(0)
      i = constant_op.constant(1)
    else:
      a = ops.convert_to_tensor(initializer)
      i = constant_op.constant(0)

    def compute(i, a):
      a = fn(a, elems_ta.read(i))
      return [i + 1, a]
    _, r_a = control_flow_ops.while_loop(
        lambda i, a: i < n, compute, [i, a],
        parallel_iterations=parallel_iterations,
        back_prop=back_prop,
        swap_memory=swap_memory)

    if varscope_caching_device_was_none:
      varscope.set_caching_device(None)
    return r_a


Modifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the "useModifications" flag:
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
    if not callable(fn):
        raise TypeError("fn must be callable.")

    with ops.name_scope(name, "foldl", [elems]):
        # Any get_variable calls in fn will cache the first call locally
        # and not issue repeated network I/O requests for each iteration.
        varscope = vs.get_variable_scope()
        varscope_caching_device_was_none = False
        if varscope.caching_device is None:
            # TODO(ebrevdo): Change to using colocate_with here and in other methods.
            varscope.set_caching_device(lambda op: op.device)
            varscope_caching_device_was_none = True

        # Convert elems to tensor array.
        elems = ops.convert_to_tensor(elems, name="elems")
        n = array_ops.shape(elems)[0]
        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                                dynamic_size=False,
                                                infer_shape=True)
        elems_ta = elems_ta.unstack(elems)

        if initializer is None:
            a = elems_ta.read(0)
            i = constant_op.constant(1)
        else:
            useModifications = True
            if useModifications:
                output_is_sequence = nest.is_sequence(initializer)
                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]
                initializer_flat = output_flatten(initializer)
                a = [ops.convert_to_tensor(init) for init in initializer_flat]
            else:
                a = ops.convert_to_tensor(initializer)

            i = constant_op.constant(0)

        def compute(i, a):
            a = fn(a, elems_ta.read(i))
            return [i + 1, a]

        _, r_a = control_flow_ops.while_loop(
            lambda i, a: i < n, compute, (i, a),
            parallel_iterations=parallel_iterations,
            back_prop=back_prop,
            swap_memory=swap_memory)

        if varscope_caching_device_was_none:
            varscope.set_caching_device(None)
        return r_a

Here is a MWE:
import tensorflow as tf

a = tf.constant( 1, dtype = tf.float32 )
b = tf.constant( 2, dtype = tf.int64   )

useTuple = False

def body( ab, i ):
    a = ab[0]
    b = ab[1]
    if useTuple:
        return (a,b)
    else:
        return [a,b]

N = 3
with tf.Session() as sess:
    if useTuple:
        ab = (a,b)
    else:
        ab = [a,b]
    print( "new foldl :", sess.run(   foldl(  body, tf.range(N), ab ) ) )  
    print( "tf.scan   :", sess.run( tf.scan(  body, tf.range(N), ab ) ) )
    print( "tf.foldl  :", sess.run( tf.foldl( body, tf.range(N), ab ) ) )

with useTuple = False, this returns
new foldl : [1.0, 2]
tf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]
# Crash for tf.foldl with error: 
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)