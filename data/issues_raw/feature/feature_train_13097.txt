Support 64bit float point gradient

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):1.1.0
Python version:  3.5.3
CUDA/cuDNN version:Cuda 8.0 cudnn 5.1
GPU model and memory:GTX 1080Ti 11GB

Describe the problem
I use Tensorflow to train a multilayer Convolutional network.
Since my network has too many parameters and my GTX 1080Ti has limited memory(11GB), so the batch size cannot exceed 16 otherwise it would cause OutOfMemory exception.
I want to update parameters using bigger batch size, so I follow the answer that is, accumulate and average gradients over multiple batches.
Scenario 1
If I use batch size=16, and update parameters after each batch, my network can converge to loss=0.01.
Scenario 2
If I use batch size=1, and update parameters after accumulating and averaging gradients after every 16 batches, my network can only converge to loss=0.04.
Theoreticall the two scenarios should converge to the same loss, but the problem is when the network converge close to the extrema, the magnitude of the gradients is about 1e-5.
And guess how precise is float32 in Tensorflow? I compute gradients and don't update parameters, they differ after 6 significant digits.
I want my network continues to converge even the magnitude of the gradients is about 1e-5, the float32 cannot satisfy my needs.
The obvious solution is to use float64 as the data type of the parameters, but Tensorflow tells me float64 is not supported in Conv2D.