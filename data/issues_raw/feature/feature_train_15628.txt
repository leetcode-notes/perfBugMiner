Support truly pluggable protocol/transport for distributed mode

At this moment Distributed TensorFlow supports only one type of transport/protocol which is gRPC, however, it does seem to be configurable (cluster, server, session).
So, there are at least three things that need to be covered:

Document the intercommunication protocol (session to a server), i.e. what session sends to a server, what server should respond, etc. The good example here would some kind of swagger spec.
Make protocol configurable from Python.
Give all a Python interface to implement new types of protocols.

The whole idea is to let developers an ability to implement the protocol that would let distributed TensorFlow spin up and talk a number compute units (serverless, containers, VMs, etc.) instead of having a bunch of processes (cluster servers) running during computations within the session.
Please note, I wasn't able to find any corresponding issues related to given topic.