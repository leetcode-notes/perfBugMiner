DC-ASGD(Delay Compensated Asynchronous Stochastic Gradient Descent)?

DC-ASGD is Microsoft's very useful algorithm for distributed asynchronous training. Compared with the ordinary ASGD algorithm, DC-ASGD has no significant loss in speed, but can get almost the same effect as Sequential SGD. As far as I know, other mainstream deep learning open source tools have implemented this algorithm, such as: CNTK, Mxnet, Paddle and so on. But in Tensorflow I have not found similar modules.
What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Microsoft/CNTK#1295
PaddlePaddle/Paddle#185
dmlc/mxnet#3614
What other attempted solutions have you tried?
I tried to implement this algorithm in Tensorflow by myself.  I do not have enough ability to do this now.
The link address of the paper
Asynchronous Stochastic Gradient Descent with Delay Compensation for Distributed Deep Learning