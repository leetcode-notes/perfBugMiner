Feature Request: Embedding Lookup Gradient

Hi,
Since embedding_lookup is just a fast way of doing a matrix multiplication between the embedding weights and a tensor of one-hot vectors, it would be nice to have an option to get the gradient of the embedding_lookup as if it were such a matrix multiplication.  There are situations where this is actually useful - for example, the one-hots could be sampled from a high-dimensional multinomial distribution computed by a first neural network and passed as input to a second neural network, where using embedding_lookup would be much faster because of the high dimensionality.  There are some recent papers with various methods for back-propagating through the sampling procedure that I would like to implement, but I don't see any way to do it efficiently for high-dimensional distributions without a feature like this.
Thanks,
Shawn