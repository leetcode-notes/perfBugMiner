Support for native half-float computation (float16/fp16)

#1300 added support for fp16 storage, but there is currently no support for native fp16 computation, which is available on some hardware such as Pascal GPUs.
In particular, the conv2d and matmul ops could take a new parameter along the lines of "compute_dtype", which would be plumbed through to CUDNN (convolution descriptor) and CUBLAS (Hgemm) in the backend, with the potential for up to a 2x speedup.
Related issues:
#1300
#4314
#851 (comment)