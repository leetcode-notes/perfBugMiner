Mixed precision not enabled with TF1.4 on Tesla V100

Hi there,
I was interested in testing my neural net (an Autoencoder that serves as a generator + a CNN as a discriminator)  that uses 3dconv/deconv layers with the new Volta architecture and benefit from the Mixed-Precision training. I compiled the most recent source code of Tensorflow 1.4 with CUDA 9 and CudNN 7.0 and cast all the trainable variables used by my conv/deconv layers to tf.float16. Also, all my input and output tensors have sizes that are multiple of 8.
I have two issues so far:

I do not see any substantial speed improvement, the training time is roughly similar to when using tf.float32. My understanding is that with the Volta and cuDNN 7.0, Mixed Precision should be  automatically detected by TF and hence should use Tensor Core math. Am I wrong, or is there anything I should do to enable it? FYI, I also tried the TF1.5 nighlty build, and it seems that it is even slower than my custom 1.4.
I also noticed that the Maxpool3D layers are not supporting TF.float16 yet and need a float32 input. Any plan to change that anytime soon ?

Thanks for your support!