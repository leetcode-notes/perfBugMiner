New Feature:  Pascal, Cuda 8, Unified memory

Hi,
As Cuda 8 enables unified memory for Pascal GPU, combining CPU and GPU on the same address level, and enhancing the memory size available for GPU (with limited latency), using CPU RAM.

Is there a possibility to have larger than GPU ram NN+data (lower than CPU ram) for training in tensor flow ? (it would help reducing distributed computing/network latency) ?

ie, using the idea of  Oversubscribe GPU memory for large dataset/models.
here, CUDA API :
http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-simplifying
Example of 64GB allocation on GPU:
void foo() {
// Allocate 64 GB on GPU, using CPU RAM
char *data;
size_t size = 64*1024*1024*1024;
cudaMallocManaged(&data, size);
}