xorshift128+ version of (stateless) random ops

Currently, TensorFlow's random numbers use the Philox counter mode generator, which is extremely easy to parallelize on both CPU and GPU.  This applies to both the normal stateful ops and the new tf.contrib.stateless versions with custom seeding.
xorshift128+ is a simpler generator that could conceivably speed up random number generation.  Unfortunately, it is not a counter mode generator, and is thus difficult to parallelize or use safely in a random access setting.
Until now!  Commit girving/tensorflow@60abb26 on branch https://github.com/girving/tensorflow/tree/xorshift implements random access into the xorshift128+ generator in a reasonably efficient manner, using some finite field machinery.  Specifically, jumps in xorshift128+ are represented as elements of the finite field GF(2^128), composed to produce other jumps, then mapped through linear maps to produce xorshift128+ values.
However, the code is a proof of concept.  A decent amount of further work would have to be done to get committed to TensorFlow.  In particular, the parallelism code on both CPU and GPU would have to be written, by computing one jump per thread of execution (many jumps can be computed more cheaply vs. one at a time).  The current code is also nonportable: it assumes special  instructions for carryless multiplication of polynomials over GF(2).  These instructions are available on recent Intel and AMD CPUs, but a slow path would need to be written to handle everything else.
Also, whether the result would actually be faster is an open question.
I don't have time to do the remaining work, so I am leaving this here as a project in case someone wants to take it on with my help.