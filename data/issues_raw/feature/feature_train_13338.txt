Feature request: add tf.layers.Group to group multiple layers under one name

Example usage (relevant for networks with skip connection i.e. u-net):
encoder1 = tf.layers.Group([
  tf.layers.Conv2d(...),
  tf.layers.BatchNorm(),
  ActivationLayer(),
], name='encoder1')

encoder2 = tf.layers.Group([
  tf.layers.Conv2d(...),
  tf.layers.BatchNorm(),
  ActivationLayer(),
], name='encoder1')

inputs = outputs = tf.layers.Input(tensor=x)

for enc in [encoder1, encoder2]:
  outputs = enc(outputs)

encoders = tf.layers.Network(inputs, outputs)

# same for decoders
for i, dec in enumerate(decoders):
  outputs = dec(tf.concat([encoders.get_layer(f'encoder{i}').output, outputs], 3))
Currently you could either

use network.get_output_at() however this requires to track node indices or parse network.layers if encoders are not symmetric (i.e. some encoder have no batch norm / dropout)
inherit from tf.layers.Layer however documentation is not clear on how to "forward" variables for example