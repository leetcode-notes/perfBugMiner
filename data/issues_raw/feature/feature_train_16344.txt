Variance of weights initialized with tf.variance_scaling_initializer is somewhat surprising

If tf.variance_scaling_initializer is called with distribution='uniform', then the weights have variance scale / n. However, if tf.variance_scaling_initializer is called with distribution='normal', the weights have variance scale / n * (1 - (4 * norm.pdf(2))/(2 * norm.cdf(2) - 1)), because the weights are drawn from a normal distribution truncated at +/- 2 std and the scale is not adjusted for this truncation.
While I understand that there are reasons to use a truncated normal distribution, I would argue that the distribution should be scaled to adjust for the truncation, or the fact that the scale is not adjusted for truncation should feature more prominently in the documentation. The current statement that:

With distribution="normal", samples are drawn from a truncated normal distribution centered on zero, with stddev = sqrt(scale / n)

is not totally clear, since it's not obvious that stddev is referring to the standard deviation of the normal distribution before truncation and not after.
To make matters more confusing, unlike tf.variance_scaling_initializer, tf.contrib.layers.variance_scaling_initializer performs the appropriate correction so that the weights have variance scale / n.