Support optional activation function to produce attentional hidden states

This is a feature request.
When using an AttentionWrapper with the attention_layer_size argument set, the cell output and the context are combined and fed through a dense layer to produce an attentional hidden state.
However, it is also common to apply an activation function on this layer output (typically tanh). This is for example described in:

Minh-Thang Luong, Hieu Pham, Christopher D. Manning. "Effective Approaches to Attention-based Neural Machine Translation." EMNLP 2015.  https://arxiv.org/abs/1508.04025

I would be happy to contribute. Do we want to add a new argument attention_layer_activation=None to the AttentionWrapper constructor?