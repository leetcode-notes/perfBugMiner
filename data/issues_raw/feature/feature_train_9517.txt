Placing Variables on the cpu using `tf.contrib.layers` functions

Dear tensorflow team,
After constructing my model using the functionality provided by tf.contrib.layers I now want to extend my model over several GPUs. I learned that it might be beneficary to place Variables on the CPU when doing that, to reduce data transfer overhead. After not seeing an easy way to do this I found a workaround I described on stackoverflow. My solution is to generate Variable-nodes in the graph where the Variable-getter of the fully_connected layer for example would expect the variables to be.
As this is not a very nice solution, I messed with the fully_connected layer and the _build_variable_getter function to basically allow me to specify, where I want to place the variabels. Thus after

adding the kwarg variable_device to tf.contrib.layer.fully_connected
adding the kwarg variable_device to tf.contrib.layer._build_variable_getter
adding the kwarg device to tf.contrib.layer._model_variable_getter
and passing this as kwarg to model_variable defined in tensorflow.contrib.framework.python.ops.variables

I get the desired functionality when using the fully_connected layer.
Below you find the modified version of layers.py
In my eyes this would be a very useful feature for all layers that contain trainable variables, which is why I would like to make a request for this feature.
If you think the supplied modification is good enough, I can also try to make a pull request, after updating the other layers.
Best, Johannes

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): As described above, I did
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version: ('v1.0.0-65-g4763edf-dirty', '1.0.1')
CUDA/cuDNN version: 8, 5.1
GPU model and memory: Titan X Pascal, 12GB

Source code / logs
See

stackoverflow
modified layers.py