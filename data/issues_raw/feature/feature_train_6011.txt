Is it possible to implement spatial pyramid pooling layer with tensorflow?

I would like to implement the spatial pyramid pooling layer as introduced in this paper.
As the paper setting, the keypoint is to define variant kernel size and stride size of max_pooling layer, which is:
kernel_size = ceil(a/n)
stride_size = floor(a/n)

where a is the input tensor spatial size, and n is the pyramid level, i.e. spatial bins of the pooling output.
I try to implement this layer with tensorflow, the code is:
def spp_layer(input_, name = 'SPP_layer'):
    '''4 level spp layer 
    spatial bins: [6_6, 3_3, 2_2, 1_1] '''

    shape = input_.get_shape().as_list()

    with tf.variable_scope(name):

        spp_6_6_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/6).astype(np.int32), np.ceil(shape[1]/6).astype(np.int32), 1], 
                                      strides=[1, shape[1]//6, shape[2]//6, 1], 
                                      padding='SAME')
        print('SPP layer level 6:', spp_6_6_pool.get_shape().as_list())

        spp_3_3_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/3).astype(np.int32), np.ceil(shape[2]/3).astype(np.int32), 1], 
                                      strides=[1, shape[1]//3, shape[2]//3, 1], 
                                      padding='SAME')
        print('SPP layer level 3:', spp_3_3_pool.get_shape().as_list())

        spp_2_2_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/2).astype(np.int32), np.ceil(shape[2]/2).astype(np.int32), 1], 
                                      strides=[1, shape[1]//2, shape[2]//2, 1], 
                                      padding='SAME')
        print('SPP layer level 2:', spp_2_2_pool.get_shape().as_list())

        spp_1_1_pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1]/1).astype(np.int32), np.ceil(shape[2]/1).astype(np.int32), 1], 
                                      strides=[1, shape[1]//1, shape[2]//1, 1], 
                                      padding='SAME')
        print('SPP layer level 1:', spp_1_1_pool.get_shape().as_list())


        spp_6_6_pool_flat = tf.reshape(spp_6_6_pool, [shape[0], -1])
        spp_3_3_pool_flat = tf.reshape(spp_3_3_pool, [shape[0], -1])
        spp_2_2_pool_flat = tf.reshape(spp_2_2_pool, [shape[0], -1])
        spp_1_1_pool_flat = tf.reshape(spp_1_1_pool, [shape[0], -1])

        spp_pool = tf.concat(1, [spp_6_6_pool_flat, spp_3_3_pool_flat, spp_2_2_pool_flat, spp_1_1_pool_flat])

    return spp_pool

But it cannot gurantee the same length pooling output, when the input sizes are different.
Does tensorflow support the fixed length spatial pyramid pooling layer?
I also post the question on the stackoverflow.