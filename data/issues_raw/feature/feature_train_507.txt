New type of decays, esp. for learning rate

After playing around TF for multiple CNN models with different optimizers (SGD and Adam) I found it difficult to find the best time to decay from only steps number.

It would be nice if you add decay based on changes in a loss, as it was common, if loss doesn't change for a threshold of a step and amount, then decay it.
The other feature may be useful is to increase the decay steps as it goes further, for example decay it each 200 steps for the first 2,000 steps and then decay it for each 1,000 steps to the rest or another decay level which can be decay it only each 5,000 after step 100,000.