Feature Request: Dynamic Convolution Kernels

I was wondering if it is possible to allow the kernel in conv2d and conv3d to have an additional batch dimension, e.g. to allow the filter shape to be:
[batch, filter_depth, filter_height, filter_width, in_channels, out_channels]
Hence, the convolution kernel can depend on the input data.
In doing so, the convolution kernels can be 'dynamic' and use prior information.
Currently, the kernels are 'static' and therefore always look for the same patterns in the input data.
However, it would be helpful for the kernel to be a function of some input.
That way a local transformation (learned through back propagation) can be applied to the kernels, in order to look for patterns unique for that image/ event/ data sample.
I implemented a 3d version of this in python, however, all the indexing and slicing make it rather slow. For the conv2d, I guess one could use tf.extract_image_patches to speed things up, but something equivalent does not exist for the 3d case (unless I'm missing something?).
I tried looking into the code of conv3d and conv2d to see how much effort it would be to implement this. Unfortunately, I'm neither a cuda expert, nor familiar with the way ops and kernels are implemented in tensorflow.
A feature like this would be greatly appreciated.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
TensorFlow installed from (source or binary): N/A
TensorFlow version (use command below): N/A
Python version:  N/A
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A