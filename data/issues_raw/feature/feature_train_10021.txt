Class weighting in tf.losses.softmax_cross_entropy

Feature request
In tf.losses.softmax_cross_entropy there's an optional field weights. I assumed this field was used for assigning a different weight to each class, but it actually is used to assign a weight to each sample in the batch. In my use case i have a batch_size of 128 and 30 classes, so I was passing a [1, 30] tensor and got this error:
InvalidArgumentError (see above for traceback): Incompatible shapes: [128] vs. [30]
	 [[Node: optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device="/job:localhost/replica:0/task:0/gpu:0"](optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape, optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape_1)]]

I looked at the implementation, confirmed that the function expects batch_size as the dimension of thensor and realized that my expected behavior cannot be achieved easily as tf.nn.softmax_cross_entropy_with_logits doesn't have a weight parameter.
My current workaround solution is to re-implement this function calculating the loss for each class and then multiplying for the weight vector that I pass, but that's inefficient compared to the optimized implementation of tf.nn.softmax_cross_entropy_with_logits.
So my request is:

provide an optimized tf.nn.softmax_cross_entropy_with_logits that also accepts weights for each class as a parameter
use it inside tf.losses.softmax_cross_entropy so that one can pass weights as a scalar, a [batch_size, 1] tensor, a [1, num_classes] tensor or a [batch_size, num_classes] tensor (the same dimension of  onehot_labels)