Implement components for "Self-normalizing networks"

Hochreiter's group has recently come up with a new dropout technique and activation function in a recent paper (https://arxiv.org/abs/1706.02515). Presented experiments demonstrate that these lead to better learning in standard feed forward networks.
I would like to implement these components in TensorFlow. Creating this issue to gauge the community's interest level in such components. Feedback will be extremely helpful.