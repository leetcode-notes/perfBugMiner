Inaccuracies in tf.reduce_sum

I've been trying to normalize some vectors by dividing by their sum, but have noticed that this doesn't always result in vectors that sum to one.
For example:
foo = np.random.rand(100)*100 #random floats between 0-100
print(sess.run(tf.reduce_sum(foo/tf.reduce_sum(foo)))) #should return 1
print((foo/foo.sum()).sum()) #does return 1

While I realize that this is possible with finite precision floating point operations, I find it odd how readily it occurs, especially when the same operation in numpy doesn't encounter this issue. Is this a known issue? I've also noticed that tf.nn.softmax doesn't appear to suffer from this problem, is there a different op I need to use to ensure proper normalization?