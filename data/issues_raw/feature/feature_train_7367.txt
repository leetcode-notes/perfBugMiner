TF Slim batch_norm does not expose beta_regularizer or gamma_regularizer

batch_normalization takes in a beta_regularizer and gamma_regularizer but the TF slim batch_norm layer does not expose this.
Compare this to other TF slim layers, such as convolution which do expose the regularizers: 
  
    
      tensorflow/tensorflow/contrib/layers/python/layers/layers.py
    
    
        Lines 789 to 791
      in
      bd4a1c5
    
    
    
    

        
          
           weights_regularizer=None, 
        

        
          
           biases_initializer=init_ops.zeros_initializer(), 
        

        
          
           biases_regularizer=None,