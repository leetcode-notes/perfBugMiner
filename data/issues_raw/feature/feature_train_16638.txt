Feature request: use padded_batch with tf.estimator.export.build_parsing_serving_input_receiver_fn

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12.6
TensorFlow installed from (source or binary): I forget
TensorFlow version (use command below): 1.4
Python version: 2.7.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Describe the problem
As far as I can tell, tf.estimator.export.build_parsing_serving_input_receiver_fn() doesn't allow you to control the way examples are batched. So if I have an Estimator that I've trained by using Dataset.padded_batch() in the input_fn, there doesn't seem to be a way for me to use that model with TensorFlow Serving.
Source code / logs
N/A?