tf.contrib.seq2seq.prepare_attention doesn't allow decoder states and attention states to be different lengths

I'm using the new tf.contrib.seq2seq.prepare_attention with tf.contrib.seq2seq.attention_decoder_fn_train and tf.contrib.seq2seq.dynamic_rnn_decoder to do dynamic decoding with attention.
If we are using e.g. attention_option="bahdanau", then this line implements the standard attention equation:
scorei = vT tanh(Wh hi + Wq q)
where

hi is the ith attention state, a vector length num_units
Wh is a weight matrix shape [num_units, num_units]
q is the query (i.e. current decoder hidden state), a vector length num_units
Wq is a weight matrix shape [num_units, num_units]
v is a weight vector length num_units

In particular, the code assumes that:

Decoder hidden states q and attention states hi are the same size (which isn't true if e.g. you want different size hidden vectors for your encoder and decoder, or you want to use bidirectional RNN for encoder but not decoder)
v, Wh hi and Wq q must also be same length num_units

In particular assumption 1 is very limiting. I think it would be better to allow:

hi is the ith attention state, a vector length attn_size
Wh is a weight matrix shape [num_units, attn_size]
q is the query (i.e. current decoder hidden state), a vector length query_size
Wq is a weight matrix shape [num_units, query_size]
v is a weight vector length num_units

where the user can define num_units, attn_size and query_size. From what I can see this would be fairly uncomplicated.