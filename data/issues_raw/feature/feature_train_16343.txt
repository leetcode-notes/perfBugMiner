tf.data.Dataset doesn't provide a good workflow for generating custom samples from large files

We're given hundreds of data files, each containing many gigabytes worth of sample data in a custom format. As far as I can tell there are only two approaches to extract samples from this using Dataset:

tf.data.Dataset.from_generator(generator=my_custom_reader, ...)

Create a generator which produces samples. This approach is not ideal because this method must be the first dataset in the chain. The generator cannot accept a tensor. Therefore you can't batch and shuffle your list of 100's of filenames (or anything more complex). You also can't make use of interleave(...) because the generator can't accept a tensor, and this use case is begging to use interleave(...).
A solution here might be to provide a method for a generator to accept a tensor, as tf.py_func(...) does for functions.

tf.data.Dataset.map(map_func=tf.py_func(my_custom_reader, ...), ...)

The map function does allow us to shuffle and parallelize the filenames using all of the functionality of the Dataset pipeline, however, with map, the files must be read into memory completely, and these files are large. Reading numerous files into memory is infeasible.
A solution here might be to extend the map function to support generators.
Unless there's an alternative approach, which I didn't glean from the docs or stackoverflow, then this seems to be an inherent limitation and a seemingly reasonable use case on which to base a feature request.