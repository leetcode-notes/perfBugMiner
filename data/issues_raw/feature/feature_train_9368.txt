PreventGradients in SoftmaxCrossEntropyWithLogit ops

SparseSoftmaxCrossEntropyWithLogits cannot take second order gradients (It's not a beautiful hack, and I am not sure how much computational speed up it will bring).
Adding PreventGradient node in the gradient graph seems to contaminate the computation graph structure, e.g. I am doing custom forward-mode automatic differentiation.
Reference: https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/nn_grad.py#L334