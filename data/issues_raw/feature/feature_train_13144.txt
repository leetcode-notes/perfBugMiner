No gradient defined for Relu6Grad

System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes. MWE below is not a stock example.


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04


TensorFlow installed from (source or binary):
Binary (Miniconda)


TensorFlow version (use command below):
print(tf.GIT_VERSION, tf.VERSION)
('v1.2.0-5-g435cdfc', '1.2.1')


Python version:
2.7.13


Bazel version (if compiling from source):
N/A


CUDA/cuDNN version:
CUDA 8.0.61
CuDNN 5.1.10


GPU model and memory:
GeForce GTX TITAN X
Total Memory 11.91GiB


Exact command to reproduce:
See below.


Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
One can not apply the gradient operator to a nn.Relu6 more than once.
Applying it once gives a Relu6Grad node,
applying it a second time (i.e applying it to the Relu6Grad)  throws an error.
Unless I have messed up my math (quiet possible),
Relu6 is infinitely differentiable, except at 2 points,
albeit very boring

f(x) = relu6(x)
df/dx = 1 if 0<x<6 else 0
d2f/dxx = 0
all further derivatives also 0.

How-ever, it is marginally more interesting if one is applying the chain rule to it

f(x1,x2) = relu6(x1*x2)
df/dx1 = x2 if 0<x1*x2<6 else 0
d2f/dx1x2 = 1 if 0<x1*x2<6 else 0

This is a feature request to make the gradient of Relu6Grad defined.
Source code / logs
MWE:
import tensorflow as tf
sess = tf.Session()

x1 = tf.placeholder(tf.float32)
x2 = tf.placeholder(tf.float32)

y = tf.nn.relu6(x1*x2)
d1 = tf.gradients(y,x1)
d2 = tf.gradients(d1, x2)
Errors on the final line  with
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
<ipython-input-7-4250ab355a8b> in <module>()
----> 1 d2 = tf.gradients(d1, x2)

...usr/lib/python2.7/site-packages/te
nsorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops,
 gate_gradients, aggregation_method)
    512               raise LookupError(
    513                   "No gradient defined for operation '%s' (op type: %s)" %
--> 514                   (op.name, op.type))
    515         if loop_state:
    516           loop_state.EnterGradWhileContext(op, before=False)

LookupError: No gradient defined for operation 'gradients/Relu6_grad/Relu6Grad' (op type: Relu6Grad)