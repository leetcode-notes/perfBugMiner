complex gradient update in optimization

Is there a plan to allow complex optimization in Tensorflow in the future?
When you try to do it with version 1.3, you can calculate and evaluate gradients, but you cannot apply them with opt.apply_gradients(grds_and_vars). The error message you get is:
InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'ApplyAdadelta' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
device='GPU'; T in [DT_DOUBLE]
device='GPU'; T in [DT_FLOAT]
device='GPU'; T in [DT_HALF]
device='CPU'; T in [DT_DOUBLE]
device='CPU'; T in [DT_FLOAT]
device='CPU'; T in [DT_HALF]

System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No


**OS Platform and Distribution
Distributor ID: Debian
Description:    Debian GNU/Linux 8.9 (jessie)
Release:        8.9


TensorFlow installed from (source or binary):
using pip install in a virtual conda environment


TensorFlow version (use command below):
1.3


Python version:
Python 3.5.2 |Anaconda 4.3.0 (64-bit)| (default, Jul  2 2016, 17:53:06)


Bazel version (if compiling from source):
not applicable


CUDA/cuDNN version:
8.0/6.0


GPU model and memory:
Tesla K40c, 11439MiB


Exact command to reproduce:
not necessary, since feature request/question


Describe the problem
problem description above
Source code / logs
not applicable