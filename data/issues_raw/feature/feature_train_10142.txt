tf.nn.softmax overflow due to exp(x)

From reading the doc and code of tf.nn.softmax, it looks like this function naively call a _softmax function which may have the overflow problem if the x in exp(x) is too big. (underflow can happen, too)
This is a typical problem of machine learning, and googling 'log sum exp' gives some information. TF even has a function reduce_logsumexp to calculate the value in a robust way.
I think it makes sense that nn.softmax also use similar method of reduce_logsumexp. I ran into a loss  = NaN today and suspect this is the cause.
Thanks,