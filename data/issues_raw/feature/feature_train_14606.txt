Converting unsupported operation: Dequantize

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.4.0
Python version:  2.7
Bazel version (if compiling from source): 0.5.4
GCC/Compiler version (if compiling from source): 4.8.5
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Describe the problem
I use the command:
bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/guoxiang/workspace/tensorflow/sms_model/optimized_sms.pb' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--output_file=/home/guoxiang/workspace/tensorflow/sms_model/sms_char_cnn.lite' '--inference_type=QUANTIZED_UINT8' '--input_type=QUANTIZED_UINT8' '--input_arrays=embedding_matrix' '--output_arrays=final_scores' '--input_shapes=1,5000,64,1' --logtostderr '--v=2'
to convert a quantized model to a  lite model, got the following error:
Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Dequantize) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
so , now qutianzed model is still not supported ?