Feature request: easier access to tensor de-allocation information

TLDR; to debug TensorFlow out-of-memory situations one needs to see tensor de-allocation info.
You can see allocation stats in timeline, but without de-allocation info you can't calculate peak memory. Currently getting peak memory is possible by:


Hacking TensorFlow to print deallocation messages with timestamps as here


A bunch of regular expression to parse __LOG_MEMORY__ messages as in here


Since this needs building your own version of tensorflow, this is not accessible to most people. Perhaps this can be remedied by adding deallocation events to session run timeline? @michaelisard
Some recent places this issue came up:
http://stackoverflow.com/questions/41517145/outer-product-based-conv-filters-consume-disproportionately-high-memory
http://stackoverflow.com/questions/41496251/fitting-large-matrix-calculations-into-memory-when-using-tensorflow
http://stackoverflow.com/questions/41451273/tensorflow-specifying-storage-of-layer-activations
http://stackoverflow.com/questions/40190510/tensorflow-how-to-log-gpu-memory-vram-utilization/40197094#comment70145571_40197094
#6019 (comment)