Fast Layer Normalization GPU kernel

Hi, I wrote a custom CUDA op for layer normalization, which is about 5-10x faster than the current tf.contrib.layers.layer_norm.
After I posted the link on reddit, some people suggested that I should try to merge this kernel into the trunk, but I have never done this before, so I need some guidance on how this custom kernel could be integrated into TensorFlow's current code.
With my current understanding on how TensorFlow's code is organized, here are some of the problems I think should be solved before the custom op can be integrated into the trunk:

It doesn't have CPU kernels.
Since the kernel uses shuffle instructions, it only supports cards newer than Kepler.
Current implementation has a restriction on the size of last dimension (cannot be larger than 5102)

Without solving these problems, the fastest way to merge this op into the trunk is probably by putting it under the tf.contrib folder as it is right now. I will fill out the CLA and make a pull request if you guys think this custom kernel is ready to be merged into the tf.contrib folder.