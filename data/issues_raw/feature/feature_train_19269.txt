tf.contrib.data.parallel_interleave with tf.data.Dataset.from_generator

I think this is a feature request (or maybe I miss something). When data is coming from multiple remote sources multiple tf.data.Dataset.from_generator should run in parallel. Therefore tf.contrib.data.parallel_interleave is already available, see this small code example.
def generator(n):
  # returns n-th generator function

def dataset(n):
  return tf.data.Dataset.from_generator(generator(n))

ds = tf.data.Dataset.range(N).apply(tf.contrib.data.parallel_interleave(dataset, cycle_lenght=N))

# where N is the number of generators you use


Describe the problem
This problem with the code above is that generator cannot look like this:
def generator(n):
  return consumers[n] # A python list with n items

Because then the error 'TypeError: list indices must be integers or slices, not Tensor' will occur. However it would be really nice to have support for multiple from_generator pipelines in parallel.
@mrry Do you have an idea whether this is indeed a feature request or that it is possible with the current codebase?