Feature Request: AdaDelta optimizer

AdaDelta (http://arxiv.org/abs/1212.5701) is a popular training algorithm for Neural Network. It is available in most other libraries I have used, like Torch or chainer.
In my small personal experience (and at least for the Neural Networks I have used), AdaDelta is often the optimizer that works best out of the box (RMSProp working quite well as well, while Adam, Adagrad and SGD typically being not as good)
Therefore, it would be nice if AdaDelta could be added to the set of available optimizers.