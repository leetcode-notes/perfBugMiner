Automatically merge identical ops

When an op constructing function is called for multiple times on the same set of inputs, each of these calls adds a new op to the graph, which leads to extra computation. I understand that this could be avoided in most cases by reusing the output tensor. However, this can't be easily done in some situations, e.g. when using different optimizers for different parts of a neural network. Therefore, IMHO, it would be beneficial to have the framework automatically merge identical ops.