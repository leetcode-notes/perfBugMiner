Can we run Dataset API on GPU?

I am running binary TF 1.3.0 on Ubuntu 16.04.
Python Version: 3.5.3
I have Nvidia TITAN Xp installed.
I use Dataset API to build input pipeline. What I want is to extract image features using CNN layers
in the input pipeline. The code looks like this:
    def extract_feats(image):
      with tf.device("/gpu:0"):
        _, end_points = vgg.vgg_16(tf.expand_dims(image, 0),
                                   is_training=(mode == ModeKeys.TRAIN),
                                   spatial_squeeze=False)
        final_conv_layer = end_points['vgg_16/conv5/conv5_3']
        feats = spatial_pyramid_pooling(final_conv_layer, [bin_size], mode='avg')
      return tf.reshape(feats, shape=(bin_size * bin_size, tf.shape(final_conv_layer)[-1]))

    features = features.map(extract_feats)

When running the code, my CPU usage is more than 1000% (I have an 6 cores/12 threads CPU), while the GPU usage is 0%. I suspect that the input pipeline built from Dataset API are forced to run on CPU. I tried to set log_device_placement=True and I can see that the operation is placed on GPU.
Since I want to extract vectors with same length from variable-sized images using SPP pooling, I have to process these images one by one using Dataset.map before calling Dataset.batch. So I hope the operations inside Dataset could be run on GPU.