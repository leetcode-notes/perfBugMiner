Feature request: RMSPropOptimizer support for sparse gradients

I use RMSPropOptimizer to optimize parameters, I get NotImplementedError. But when I change to use AdamOptimizer, it works fine.
So I try to fix the problem and I find some key point, may be it can help.
Here is my code:
self.x = tf.placeholder(tf.int32, [None, sequence_length])  
point = tf.get_variable([len(embedding_matrix)])
............
............
outputs, states = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)
index     =    self.x[:, 0]
index  = tf.reshape(index, [-1,1])
index_point  =    tf.gather(pointt, index)
output   =  tf.mul(outputs[-1] ,   index_point)

scores = tf.nn.xw_plus_b(output, self.W, b)
losses = tf.nn.softmax_cross_entropy_with_logits(scores, self.input_y)
self.loss = tf.reduce_mean(losses) 
optimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.9)
grads_and_vars = optimizer.compute_gradients(self.loss)
self.train_op = optimizer.apply_gradients(grads_and_vars)

When I try to change the line output = tf.mul(outputs[-1] , index_point) to some others such as output = tf.mul(outputs[-1] , 2), the error disappear. And I try to change to use bidirectional_rnn, the error also disappear.