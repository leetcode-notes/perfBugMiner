Make `tf.contrib.seq2seq._BaseAttentionMechanism` public

Currently, if users want to write their own attention mechanisms, they have to do so from scratch by extending the tf.contrib.seq2seq.AttentionMechanism class (which has nothing) or import the _BaseAttentionMechanism from attention_wrapper.py with the following, inconvenient import statement:
from tensorflow.contrib.seq2seq.python.ops.attention_wrapper import _BaseAttentionMechanism

It's worth including this class directly accessible from tf.contrib.seq2seq, considering that it adds good defaults and that both forms of attention currently available, Bahdanau and Luong, inherit this class.
My proposal is to simply rename the class to BaseAttentionMechanism without an underscore (or perhaps a more meaningful name such as BasicAttentionMechanismto differentiate it from the parent class AttentionMechanism), and add the class to the __all__ array in attention_wrapper.py.